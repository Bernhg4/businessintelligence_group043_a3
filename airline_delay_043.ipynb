{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First, create a new conda environment named BI2025 and install the required packages from requirements.txt\n",
   "id": "a6e4ed63c042fd89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!conda create -n BI2025 python=3.11 -y\n",
    "!conda activate BI2025\n",
    "!pip install -r requirements.txt"
   ],
   "id": "6944938728c387d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# DO NOT MODIFY OR COPY THIS CELL!! \n",
    "# Note: The only imports allowed are Python's standard library, pandas, numpy, scipy, matplotlib, seaborn and scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "import typing\n",
    "import requests\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "from starvers.starvers import TripleStoreEngine"
   ],
   "id": "7fe771ac7019d993"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import seaborn",
   "id": "b1166d7482c10a90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Graph-based documentation preliminaries",
   "id": "58ed3386a2e8496a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**!!!IMPORTANT!!!**\n",
    "\n",
    "Everytime you work on this notebook, enter your student ID in the `executed_by` variable so that the cell executions are accredited to you."
   ],
   "id": "f545888e39d999b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "executed_by = 'stud-id_12019873'  # Replace the digits after \"id_\" with your own student ID\n",
    "#executed_by = 'stud-id_12120509'  # Replace the digits after \"id_\" with your own student ID"
   ],
   "id": "608a4b62db1eaeab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set your group and student IDs. Do this only once.",
   "id": "9da491420eddf5c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# group id for this project\n",
    "group_id = '43'  # Replace the digits with your group id\n",
    "\n",
    "# Students working on this notebook\n",
    "student_a = 'stud-id_12019873'  # Replace the digits after \"id_\" with student A's student ID\n",
    "student_b = 'stud-id_12120509'  # Replace the digits after \"id_\" with student B's student ID"
   ],
   "id": "9fccc8a08d122d46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Roles. Don't change these values.\n",
    "code_writer_role = 'code_writer'\n",
    "code_executor_role = 'code_executor'"
   ],
   "id": "e3879addf86e6617"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Setup the starvers API for logging your steps into our server-sided graph database.",
   "id": "170b446470defb5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "get_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025\"\n",
    "post_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025/statements\"\n",
    "engine = TripleStoreEngine(get_endpoint, post_endpoint, skip_connection_test=True)"
   ],
   "id": "bed15a90ad4f7d0b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use these prefixes in your notebooks. You can extend this dict with your prefixes of additional ontologies that you use in this notebook. Replace 00 with your group id",
   "id": "b6f03aaaeba91af3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prefixes = {\n",
    "    'xsd': 'http://www.w3.org/2001/XMLSchema#',\n",
    "    'rdfs': 'http://www.w3.org/2000/01/rdf-schema#',\n",
    "    'foaf': 'http://xmlns.com/foaf/0.1/',\n",
    "    'prov': 'http://www.w3.org/ns/prov#',\n",
    "    'sc': 'https://schema.org/',\n",
    "    'cr': 'http://mlcommons.org/croissant/',\n",
    "    'mls': 'http://www.w3.org/ns/mls#',\n",
    "    'mlso': 'http://w3id.org/mlso',\n",
    "    'siu': 'https://si-digital-framework.org/SI/units/',\n",
    "    'siq': 'https://si-digital-framework.org/SI/quantities/',\n",
    "    'qudt': 'http://qudt.org/schema/qudt/',\n",
    "    '': f'https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/',\n",
    "}\n",
    "\n",
    "prefix_header = '\\n'.join([f'PREFIX {k}: <{v}>' for k, v in prefixes.items()]) + '\\n\\n'"
   ],
   "id": "7c8319f76eda1bb6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Ontologies to use\n",
    "* Provenance of the experiment process\n",
    "    * PROV-O: \n",
    "        * doc: https://www.w3.org/TR/prov-o/\n",
    "        * serialization: https://www.w3.org/ns/prov-o\n",
    "* Data used and created\n",
    "    * schema.org - Dataset: \n",
    "        * doc: https://schema.org/Dataset\n",
    "        * serialization: https://schema.org/version/latest/schemaorg-current-https.ttl\n",
    "    * Crossaint\n",
    "        * doc: https://docs.mlcommons.org/croissant/docs/croissant-spec.html\n",
    "        * serialization: https://github.com/mlcommons/croissant/blob/main/docs/croissant.ttl\n",
    "* ML experiments performed\n",
    "    * MLSO: \n",
    "        * doc: https://github.com/dtai-kg/MLSO\n",
    "        * doc: https://dtai-kg.github.io/MLSO/#http://w3id.org/\n",
    "        * serialization: https://dtai-kg.github.io/MLSO/ontology.ttl\n",
    "* Measurements, Metrics, Units\n",
    "    * QUDT\n",
    "        * doc:https://qudt.org/\n",
    "        * doc: https://github.com/qudt/qudt-public-repo\n",
    "        * serialization: https://github.com/qudt/qudt-public-repo/blob/main/src/main/rdf/schema/SCHEMA_QUDT.ttl\n",
    "    * SI Digital Framework\n",
    "        * doc: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/docs/README.md\n",
    "        * doc: https://si-digital-framework.org/\n",
    "        * doc: https://si-digital-framework.org/SI\n",
    "        * serialization: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/TTL/si.ttl\n",
    "    * Quantities and Units\n",
    "        * doc: https://www.omg.org/spec/Commons\n",
    "        * serialization: https://www.omg.org/spec/Commons/QuantitiesAndUnits.ttl"
   ],
   "id": "a1f6444a923fae92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use this function to record execution times.",
   "id": "fcebedaecede3b94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "def now() -> str:\n",
    "    \"\"\"\n",
    "    Returns the current time in ISO 8601 format with UTC timezone in the following format:\n",
    "    YYYY-MM-DDTHH:MM:SS.sssZ\n",
    "    \"\"\"\n",
    "    time.tzname = ('Europe/Vienna', 'Europe/Vienna')\n",
    "    timestamp = datetime.datetime.now(datetime.timezone.utc)\n",
    "    timestamp_formated = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3] + \"Z\"\n",
    "\n",
    "    return timestamp_formated"
   ],
   "id": "e38a16f7dd11f3d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Register yourself in the Knowledge Graph using ProvO. Change the given name, family name and immatriculation number to reflect your own data.",
   "id": "b25b252f8947418d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Ontologies used: foaf, prov, IAO\n",
    "reigstration_triples_a = [\n",
    "    f':{student_a} rdf:type foaf:Person .',\n",
    "    f':{student_a} rdf:type prov:Agent .',\n",
    "    f':{student_a} foaf:givenName \"Johanna\" .',\n",
    "    f':{student_a} foaf:familyName \"Six\" .',\n",
    "    f':{student_a} <http://vivoweb.org/ontology/core#identifier> :{student_a} .',\n",
    "    f':{student_a} rdf:type <http://purl.obolibrary.org/obo/IAO_0000578> .',\n",
    "    f':{student_a} <http://www.w3.org/2000/01/rdf-schema#label> \"Immatriculation number\" .',\n",
    "    f':{student_a} <http://purl.obolibrary.org/obo/IAO_0000219> \"12019873\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "reigstration_triples_b = [\n",
    "    f':{student_b} rdf:type foaf:Person .',\n",
    "    f':{student_b} rdf:type prov:Agent .',\n",
    "    f':{student_b} foaf:givenName \"Bernhard\" .',\n",
    "    f':{student_b} foaf:familyName \"Siegl\" .',\n",
    "    f':{student_b} <http://vivoweb.org/ontology/core#identifier> :{student_b} .',\n",
    "    f':{student_b} rdf:type <http://purl.obolibrary.org/obo/IAO_0000578> .',\n",
    "    f':{student_b} <http://www.w3.org/2000/01/rdf-schema#label> \"Immatriculation number\" .',\n",
    "    f':{student_b} <http://purl.obolibrary.org/obo/IAO_0000219> \"12120509\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "role_triples = [\n",
    "    f':{code_writer_role} rdf:type prov:Role .',\n",
    "    f':{code_executor_role} rdf:type prov:Role .',\n",
    "]\n",
    "\n",
    "engine.insert(reigstration_triples_a, prefixes=prefixes)\n",
    "engine.insert(reigstration_triples_b, prefixes=prefixes)\n",
    "engine.insert(role_triples, prefixes=prefixes)"
   ],
   "id": "4423323e4cea8d11"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**What not do do**\n",
    "\n",
    "Do not use [blank nodes](https://www.w3.org/wiki/BlankNodes).\n",
    "\n",
    "PROV-O uses blank nodes to connect multiple elements with each other.\n",
    "Such blank nodes (such as _:association) should not be used.\n",
    "Instead, assign a fixed node ID such as\n",
    ":5119fcd7-b571-41e0-9464-a37c7be0f574 by generating them outside of the\n",
    "notebook.\n",
    "We suggest that, for each setting where such a blank node is needed to\n",
    "connect multiple elements, you create a unique hash (using uuid.uuid4())\n",
    "and keep this as hard-coded identifier for the blank node. The template\n",
    "notebook contains examples of this. Do *not* use these provided values,\n",
    "as otherwise, your provenance documentations will all be connected via\n",
    "these identifiers!\n",
    "Also, do not generate them dynamically in every cell execution, e.g. by\n",
    "using uuid.uuid4() in a cell. This would generate many new linking nodes\n",
    "for connecting the same elements.\n",
    "Compute one for each node (cell) where you need them and make sure to\n",
    "use the same one on each re-execution of the notebook."
   ],
   "id": "70e67913a9f7edb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "airline_data_path = os.path.join(\"data\", \"airline\")",
   "id": "9d73265b536a92d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Business Understanding ",
   "id": "a29ef1f0087d8577"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Each Activity that follows is part of the Business Understanding Phase\n",
    "\n",
    "business_understanding_phase_executor = [\n",
    "    f':business_understanding_phase rdf:type prov:Activity .',\n",
    "    f':business_understanding_phase rdfs:label \"Business Understanding Phase\" .',  ## Phase 1: Business Understanding\n",
    "]\n",
    "engine.insert(business_understanding_phase_executor, prefixes=prefixes)\n"
   ],
   "id": "2a64bea0a8a9ccc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "data_src_and_scenario_comment = \"\"\"\n",
    "The dataset consists of 21 attributes and about 3351 rows. It contains data from December 2019 and 2020, including the number of delayed, cancelled or diverted flights, as well as the cause of delays per airline and airport. The data can be used by airlines to analyze and reduce flight delays. Furthermore primary contributors to delays and high-risk airports or carriers can be identified.\n",
    "\"\"\"\n",
    "\n",
    "business_objectives_comment = \"\"\"\n",
    "Airlines want to identify main causes of flight delays.\n",
    "Airlines and airports can be compared regarding the delay frequency.\n",
    "The airlines wants to improve punctuality of their flights and therefore reduce the number of delayed, cancelled flights.\n",
    "\"\"\"\n",
    "\n",
    "business_success_criteria_comment = \"\"\"\n",
    "The success criteria include measurable improvements in on-time performance. Delayed flights are reduced by at least 5 percent in the next year. Most common causes for delays are identified.\n",
    "Top 10 airports are identified with the most delays, cancelled flights and monitored.\n",
    "\"\"\"\n",
    "\n",
    "data_mining_goals_comment = \"\"\"\n",
    "We need to identify the main drivers of delays and cancellations by analyzing delay causes, airport, airline and temporal factors;\n",
    "Quantify the distribution of delay categories (weather, carrier, security, NAS, ..);\n",
    "Identify airports and airlines with significant higher delays/cancellations;\n",
    "Build models to estimate the likelihood of delays for a given airport, airline and time.\n",
    "\"\"\"\n",
    "\n",
    "data_mining_success_criteria_comment = \"\"\"\n",
    "Clustering results to show interpretable groupings of airports based on their delay and cancellation behaviour,\n",
    "Achieve a accuraccy of at least 70% with our classification model\n",
    "\"\"\"\n",
    "\n",
    "ai_risk_aspects_comment = \"\"\"\n",
    "The system should be low-risk, as it supports operational analysis and decision-making rather than automated enforcement or individual-level decisions\n",
    "\"\"\"\n",
    "\n",
    "bu_ass_uuid_executor = \"bb6a40f9-9d92-4f9f-bbd2-b65ef6a82da2\"  # Generate once\n",
    "business_understanding_executor = [\n",
    "    f':business_understanding rdf:type prov:Activity .',\n",
    "    f':business_understanding sc:isPartOf :business_understanding_phase .',\n",
    "    # Connect Activity to Parent Business Understanding Phase Activity\n",
    "    f':business_understanding prov:qualifiedAssociation :{bu_ass_uuid_executor} .',\n",
    "    f':{bu_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{bu_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{bu_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(business_understanding_executor, prefixes=prefixes)\n",
    "\n",
    "business_understanding_data_executor = [\n",
    "    # 1a\n",
    "    f':bu_data_source_and_scenario rdf:type prov:Entity .',\n",
    "    f':bu_data_source_and_scenario prov:wasGeneratedBy :business_understanding .',\n",
    "    f':bu_data_source_and_scenario rdfs:label \"1a Data Source and Scenario\" .',\n",
    "    f':bu_data_source_and_scenario rdfs:comment \"\"\"{data_src_and_scenario_comment}\"\"\" .',\n",
    "    # 1b\n",
    "    f':bu_business_objectives rdf:type prov:Entity .',\n",
    "    f':bu_business_objectives prov:wasGeneratedBy :business_understanding .',\n",
    "    f':bu_business_objectives rdfs:label \"1b Business Objectives\" .',\n",
    "    f':bu_business_objectives rdfs:comment \"\"\"{business_objectives_comment}\"\"\" .',\n",
    "    # 1c\n",
    "    f':bu_business_success_criteria rdf:type prov:Entity .',\n",
    "    f':bu_business_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "    f':bu_business_success_criteria rdfs:label \"1c Business Success Criteria\" .',\n",
    "    f':bu_business_success_criteria rdfs:comment \"\"\"{business_success_criteria_comment}\"\"\" .',\n",
    "    # 1d\n",
    "    f':bu_data_mining_goals rdf:type prov:Entity .',\n",
    "    f':bu_data_mining_goals prov:wasGeneratedBy :business_understanding .',\n",
    "    f':bu_data_mining_goals rdfs:label \"1d Data Mining Goals\" .',\n",
    "    f':bu_data_mining_goals rdfs:comment \"\"\"{data_mining_goals_comment}\"\"\" .',\n",
    "    # 1e\n",
    "    f':bu_data_mining_success_criteria rdf:type prov:Entity .',\n",
    "    f':bu_data_mining_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "    f':bu_data_mining_success_criteria rdfs:label \"1e Data Mining Success Criteria\" .',\n",
    "    f':bu_data_mining_success_criteria rdfs:comment \"\"\"{data_mining_success_criteria_comment}\"\"\" .',\n",
    "    # 1f\n",
    "    f':bu_ai_risk_aspects rdf:type prov:Entity .',\n",
    "    f':bu_ai_risk_aspects prov:wasGeneratedBy :business_understanding .',\n",
    "    f':bu_ai_risk_aspects rdfs:label \"1f AI risk aspects\" .',\n",
    "    f':bu_ai_risk_aspects rdfs:comment \"\"\"{ai_risk_aspects_comment}\"\"\" .',\n",
    "\n",
    "]\n",
    "engine.insert(business_understanding_data_executor, prefixes=prefixes)"
   ],
   "id": "4c4d89ed3e1bb3be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Understanding",
   "id": "da35c75d7c3aee7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The following pseudo-code & pseudo-documentation may be used as a hint.",
   "id": "86163d7c65af6aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Each Activity that follows is part of the Data Understanding Phase\n",
    "\n",
    "business_understanding_phase_executor = [\n",
    "    f':data_understanding_phase rdf:type prov:Activity .',\n",
    "    f':data_understanding_phase rdfs:label \"Data Understanding Phase\" .',\n",
    "]\n",
    "engine.insert(business_understanding_phase_executor, prefixes=prefixes)\n"
   ],
   "id": "df9c8af1a9ff5701"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "airline_data_path = os.path.join(\"data\", \"airline\")\n",
    "load_airline_data_code_writer = student_a\n",
    "\n",
    "\n",
    "def load_airline_data() -> pd.DataFrame:\n",
    "    ### Load your data\n",
    "    input_file = os.path.join(airline_data_path, 'airline_delay.csv')\n",
    "    raw_data = pd.read_csv(input_file, sep=',', header=0)\n",
    "\n",
    "    raw_data.sort_values(['year', 'month'], inplace=True)\n",
    "    raw_data.set_index(['year', 'month', 'carrier', 'airport'], inplace=True)\n",
    "\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "start_time_ld = now()\n",
    "data = load_airline_data()\n",
    "numeric_cols = ['arr_flights', 'arr_del15', 'carrier_ct', 'weather_ct', 'nas_ct', 'security_ct', 'late_aircraft_ct', 'arr_cancelled', 'arr_diverted', 'arr_delay', 'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay' ]\n",
    "end_time_ld = now()\n",
    "\n",
    "display(data.head())\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "# Now document the raw data and the loaded data using appropriate ontologies.\n",
    "\n",
    "# Always add these triples for every activity to define the executor!\n",
    "ld_ass_uuid_executor = \"b8bac193-c4e6-4e31-9134-b23e001e279c\"  # Generate once\n",
    "load_airline_data_executor = [\n",
    "    f':load_airline_data prov:qualifiedAssociation :{ld_ass_uuid_executor} .',\n",
    "    f':{ld_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ld_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ld_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(load_airline_data_executor, prefixes=prefixes)\n",
    "\n",
    "ld_ass_uuid_writer = \"c600e15c-87a9-4e2a-be85-b6c2a3014210\"  # Generate once\n",
    "ld_report = \"\"\"\n",
    "Load all airline data and create a hierarchical index (year, month).\n",
    "\"\"\"\n",
    "load_airline_data_activity = [\n",
    "    ':load_airline_data rdf:type prov:Activity .',\n",
    "    ':load_airline_data sc:isPartOf :data_understanding_phase .',\n",
    "    ':load_airline_data rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':load_airline_data rdfs:comment \"\"\"{ld_report}\"\"\" .',\n",
    "    f':load_airline_data prov:startedAtTime \"{start_time_ld}\"^^xsd:dateTime .',\n",
    "    f':load_airline_data prov:endedAtTime \"{end_time_ld}\"^^xsd:dateTime .',\n",
    "    f':load_airline_data prov:qualifiedAssociation :{ld_ass_uuid_writer} .',\n",
    "    f':{ld_ass_uuid_writer} prov:agent :{load_airline_data_code_writer} .',\n",
    "    f':{ld_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{ld_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    # INPUT of activity\n",
    "    ':load_airline_data prov:used :raw_data .',\n",
    "    ':load_airline_data prov:used :raw_data_path .',\n",
    "    ':raw_data rdf:type prov:Entity .',\n",
    "    ':raw_data_path rdf:type prov:Entity .',\n",
    "    ':raw_data prov:wasDerivedFrom :raw_data_path .',\n",
    "    # OUTPUT of activity\n",
    "    ':data rdf:type prov:Entity .',\n",
    "    ':data prov:wasGeneratedBy :load_airline_data .',\n",
    "    ':data prov:wasDerivedFrom :raw_data .',\n",
    "]\n",
    "engine.insert(load_airline_data_activity, prefixes=prefixes)\n",
    "\n",
    "# Further descibe the raw data using Croissant\n",
    "raw_data_triples = [\n",
    "    ':raw_data rdf:type sc:Dataset .',\n",
    "    ':raw_data sc:name \\'Airline Delay Dataset (Dec 2019 and Dec 2020)\\' .',\n",
    "    ':raw_data sc:description \\'Airline delay counts per carrier per airport for December 2019 and December 2020.\\' .',\n",
    "\n",
    "    # Continue with futher information about the dataset...\n",
    "    ':airline_delay_csv rdf:type cr:FileObject .',\n",
    "    ':airline_delay_csv sc:name \\'airline_delay.csv\\' .',\n",
    "    ':airline_delay_csv sc:encodingFormat \\'text/csv\\' .',\n",
    "    ':raw_data sc:distribution :airline_delay .',\n",
    "    # Continue with further information about the distribution...\n",
    "    ':raw_recordset rdf:type cr:RecordSet .',\n",
    "    ':raw_recordset sc:name \\'Airline data for december for 2019 and december 2020\\' .',\n",
    "    ':raw_recordset cr:source :airline_delay_csv .',\n",
    "    ':raw_data cr:recordSet :raw_recordset .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_year .',\n",
    "    ':field_year rdf:type cr:Field .',\n",
    "    ':field_year sc:name \\'year\\' .',\n",
    "    ':field_year sc:description \\'Year in which flight data was collected.\\' .',\n",
    "    ':field_year cr:dataType xsd:gYear .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_month .',\n",
    "    ':field_month rdf:type cr:Field .',\n",
    "    ':field_month sc:name \\'month\\' .',\n",
    "    ':field_month sc:description \\'Month (1–12) of data collection.\\' .',\n",
    "    ':field_month cr:dataType xsd:integer .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_carrier .',\n",
    "    ':field_carrier rdf:type cr:Field .',\n",
    "    ':field_carrier sc:name \\'carrier\\' .',\n",
    "    ':field_carrier sc:description \\'Two-letter airline carrier code.\\' .',\n",
    "    ':field_carrier cr:dataType xsd:string .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_carrier_name .',\n",
    "    ':field_carrier_name rdf:type cr:Field .',\n",
    "    ':field_carrier_name sc:name \\'carrier_name\\' .',\n",
    "    ':field_carrier_name sc:description \\'Full airline carrier name.\\' .',\n",
    "    ':field_carrier_name cr:dataType xsd:string .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_airport .',\n",
    "    ':field_airport rdf:type cr:Field .',\n",
    "    ':field_airport sc:name \\'airport\\' .',\n",
    "    ':field_airport sc:description \\'Three-letter airport code for arrival airport.\\' .',\n",
    "    ':field_airport cr:dataType xsd:string .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_airport_name .',\n",
    "    ':field_airport_name rdf:type cr:Field .',\n",
    "    ':field_airport_name sc:name \\'airport_name\\' .',\n",
    "    ':field_airport_name sc:description \\'Full name of the airport.\\' .',\n",
    "    ':field_airport_name cr:dataType xsd:string .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_arr_flights .',\n",
    "    ':field_arr_flights rdf:type cr:Field .',\n",
    "    ':field_arr_flights sc:name \\'arr_flights\\' .',\n",
    "    ':field_arr_flights sc:description \\'Number of flights arriving at airport.\\' .',\n",
    "    ':field_arr_flights cr:dataType xsd:integer .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_arr_del15 .',\n",
    "    ':field_arr_del15 rdf:type cr:Field .',\n",
    "    ':field_arr_del15 sc:name \\'arr_del15\\' .',\n",
    "    ':field_arr_del15 sc:description \\'Flights arriving more than 15 minutes late.\\' .',\n",
    "    ':field_arr_del15 cr:dataType xsd:integer .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_carrier_ct .',\n",
    "    ':field_carrier_ct rdf:type cr:Field .',\n",
    "    ':field_carrier_ct sc:name \\'carrier_ct\\' .',\n",
    "    ':field_carrier_ct sc:description \\'Flights delayed due to air carrier (e.g. no crew).\\' .',\n",
    "    ':field_carrier_ct cr:dataType xsd:decimal .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_weather_ct .',\n",
    "    ':field_weather_ct rdf:type cr:Field .',\n",
    "    ':field_weather_ct sc:name \\'weather_ct\\' .',\n",
    "    ':field_weather_ct sc:description \\'Flights delayed due to weather.\\' .',\n",
    "    ':field_weather_ct cr:dataType xsd:decimal .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_nas_ct .',\n",
    "    ':field_nas_ct rdf:type cr:Field .',\n",
    "    ':field_nas_ct sc:name \\'nas_ct\\' .',\n",
    "    ':field_nas_ct sc:description \\'Flights delayed due to National Aviation System.\\' .',\n",
    "    ':field_nas_ct cr:dataType xsd:decimal .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_security_ct .',\n",
    "    ':field_security_ct rdf:type cr:Field .',\n",
    "    ':field_security_ct sc:name \\'security_ct\\' .',\n",
    "    ':field_security_ct sc:description \\'Flights canceled due to security issues.\\' .',\n",
    "    ':field_security_ct cr:dataType xsd:decimal .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_late_aircraft_ct .',\n",
    "    ':field_late_aircraft_ct rdf:type cr:Field .',\n",
    "    ':field_late_aircraft_ct sc:name \\'late_aircraft_ct\\' .',\n",
    "    ':field_late_aircraft_ct sc:description \\'Flights delayed due to a previous late aircraft.\\' .',\n",
    "    ':field_late_aircraft_ct cr:dataType xsd:decimal .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_arr_cancelled .',\n",
    "    ':field_arr_cancelled rdf:type cr:Field .',\n",
    "    ':field_arr_cancelled sc:name \\'arr_cancelled\\' .',\n",
    "    ':field_arr_cancelled sc:description \\'Number of cancelled flights.\\' .',\n",
    "    ':field_arr_cancelled cr:dataType xsd:integer .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_arr_diverted .',\n",
    "    ':field_arr_diverted rdf:type cr:Field .',\n",
    "    ':field_arr_diverted sc:name \\'arr_diverted\\' .',\n",
    "    ':field_arr_diverted sc:description \\'Number of diverted flights.\\' .',\n",
    "    ':field_arr_diverted cr:dataType xsd:integer .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_arr_delay .',\n",
    "    ':field_arr_delay rdf:type cr:Field .',\n",
    "    ':field_arr_delay sc:name \\'arr_delay\\' .',\n",
    "    ':field_arr_delay sc:description \\'Total delay time in minutes.\\' .',\n",
    "    ':field_arr_delay cr:dataType xsd:integer .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_carrier_delay .',\n",
    "    ':field_carrier_delay rdf:type cr:Field .',\n",
    "    ':field_carrier_delay sc:name \\'carrier_delay\\' .',\n",
    "    ':field_carrier_delay sc:description \\'Delay minutes due to air carrier issues.\\' .',\n",
    "    ':field_carrier_delay cr:dataType xsd:integer .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_weather_delay .',\n",
    "    ':field_weather_delay rdf:type cr:Field .',\n",
    "    ':field_weather_delay sc:name \\'weather_delay\\' .',\n",
    "    ':field_weather_delay sc:description \\'Delay minutes due to weather.\\' .',\n",
    "    ':field_weather_delay cr:dataType xsd:integer .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_nas_delay .',\n",
    "    ':field_nas_delay rdf:type cr:Field .',\n",
    "    ':field_nas_delay sc:name \\'nas_delay\\' .',\n",
    "    ':field_nas_delay sc:description \\'Delay minutes due to National Aviation System.\\' .',\n",
    "    ':field_nas_delay cr:dataType xsd:integer .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_security_delay .',\n",
    "    ':field_security_delay rdf:type cr:Field .',\n",
    "    ':field_security_delay sc:name \\'security_delay\\' .',\n",
    "    ':field_security_delay sc:description \\'Delay minutes due to security issues.\\' .',\n",
    "    ':field_security_delay cr:dataType xsd:integer .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_late_aircraft_delay .',\n",
    "    ':field_late_aircraft_delay rdf:type cr:Field .',\n",
    "    ':field_late_aircraft_delay sc:name \\'late_aircraft_delay\\' .',\n",
    "    ':field_late_aircraft_delay sc:description \\'Delay minutes due to previous late aircraft.\\' .',\n",
    "    ':field_late_aircraft_delay cr:dataType xsd:integer .',\n",
    "]\n",
    "engine.insert(raw_data_triples, prefixes=prefixes)\n",
    "\n",
    "# Also the output of the load activity is a dataset that can be described with Croissant\n",
    "data_triples = [\n",
    "    ':data rdf:type sc:Dataset .',\n",
    "\n",
    "    ':recordset rdf:type cr:RecordSet .',\n",
    "\n",
    "    ':data cr:recordSet :recordset .',\n",
    "\n",
    "    # Fields can also be reused\n",
    "    ':recordset cr:field :field_year .',\n",
    "    ':recordset cr:field :field_month .',\n",
    "    ':recordset cr:field :field_carrier .',\n",
    "    ':recordset cr:field :field_carrier_name .',\n",
    "    ':recordset cr:field :field_airport .',\n",
    "    ':recordset cr:field :field_airport_name .',\n",
    "\n",
    "    ':recordset cr:field :field_arr_flights .',\n",
    "    ':recordset cr:field :field_arr_del15 .',\n",
    "    ':recordset cr:field :field_carrier_ct .',\n",
    "    ':recordset cr:field :field_weather_ct .',\n",
    "    ':recordset cr:field :field_nas_ct .',\n",
    "    ':recordset cr:field :field_security_ct .',\n",
    "    ':recordset cr:field :field_late_aircraft_ct .',\n",
    "    ':recordset cr:field :field_arr_cancelled .',\n",
    "    ':recordset cr:field :field_arr_diverted .',\n",
    "\n",
    "    ':recordset cr:field :field_arr_delay .',\n",
    "    ':recordset cr:field :field_carrier_delay .',\n",
    "    ':recordset cr:field :field_weather_delay .',\n",
    "    ':recordset cr:field :field_nas_delay .',\n",
    "    ':recordset cr:field :field_security_delay .',\n",
    "    ':recordset cr:field :field_late_aircraft_delay .',\n",
    "    # This is not actually a field in the dataframe but below demonstrates how units may be used\n",
    "\n",
    "    ':recordset cr:field :field_year .',\n",
    "    ':field_year rdf:type cr:Field .',\n",
    "    ':field_year sc:name \\'year\\' .',\n",
    "    ':field_year sc:description \\'Year in which flight data was collected.\\' .',\n",
    "    ':field_year cr:dataType xsd:gYear .',\n",
    "\n",
    "    ':recordset cr:field :field_month .',\n",
    "    ':field_month rdf:type cr:Field .',\n",
    "    ':field_month sc:name \\'month\\' .',\n",
    "    ':field_month sc:description \\'Month (1–12) of data collection.\\' .',\n",
    "    ':field_month cr:dataType xsd:integer .',\n",
    "\n",
    "    ':recordset cr:field :field_carrier .',\n",
    "    ':field_carrier rdf:type cr:Field .',\n",
    "    ':field_carrier sc:name \\'carrier\\' .',\n",
    "    ':field_carrier sc:description \\'Two-letter airline carrier code.\\' .',\n",
    "    ':field_carrier cr:dataType xsd:string .',\n",
    "\n",
    "    ':recordset cr:field :field_carrier_name .',\n",
    "    ':field_carrier_name rdf:type cr:Field .',\n",
    "    ':field_carrier_name sc:name \\'carrier_name\\' .',\n",
    "    ':field_carrier_name sc:description \\'Full airline carrier name.\\' .',\n",
    "    ':field_carrier_name cr:dataType xsd:string .',\n",
    "\n",
    "    ':recordset cr:field :field_airport .',\n",
    "    ':field_airport rdf:type cr:Field .',\n",
    "    ':field_airport sc:name \\'airport\\' .',\n",
    "    ':field_airport sc:description \\'Three-letter airport code for arrival airport.\\' .',\n",
    "    ':field_airport cr:dataType xsd:string .',\n",
    "\n",
    "    ':recordset cr:field :field_airport_name .',\n",
    "    ':field_airport_name rdf:type cr:Field .',\n",
    "    ':field_airport_name sc:name \\'airport_name\\' .',\n",
    "    ':field_airport_name sc:description \\'Full name of the airport.\\' .',\n",
    "    ':field_airport_name cr:dataType xsd:string .',\n",
    "\n",
    "    ':recordset cr:field :field_arr_flights .',\n",
    "    ':field_arr_flights rdf:type cr:Field .',\n",
    "    ':field_arr_flights sc:name \\'arr_flights\\' .',\n",
    "    ':field_arr_flights sc:description \\'Number of flights arriving at airport.\\' .',\n",
    "    ':field_arr_flights cr:dataType xsd:integer .',\n",
    "\n",
    "    ':recordset cr:field :field_arr_del15 .',\n",
    "    ':field_arr_del15 rdf:type cr:Field .',\n",
    "    ':field_arr_del15 sc:name \\'arr_del15\\' .',\n",
    "    ':field_arr_del15 sc:description \\'Flights arriving more than 15 minutes late.\\' .',\n",
    "    ':field_arr_del15 cr:dataType xsd:integer .',\n",
    "\n",
    "    ':recordset cr:field :field_carrier_ct .',\n",
    "    ':field_carrier_ct rdf:type cr:Field .',\n",
    "    ':field_carrier_ct sc:name \\'carrier_ct\\' .',\n",
    "    ':field_carrier_ct sc:description \\'Flights delayed due to air carrier (e.g. no crew).\\' .',\n",
    "    ':field_carrier_ct cr:dataType xsd:decimal .',\n",
    "\n",
    "    ':recordset cr:field :field_weather_ct .',\n",
    "    ':field_weather_ct rdf:type cr:Field .',\n",
    "    ':field_weather_ct sc:name \\'weather_ct\\' .',\n",
    "    ':field_weather_ct sc:description \\'Flights delayed due to weather.\\' .',\n",
    "    ':field_weather_ct cr:dataType xsd:decimal .',\n",
    "\n",
    "    ':recordset cr:field :field_nas_ct .',\n",
    "    ':field_nas_ct rdf:type cr:Field .',\n",
    "    ':field_nas_ct sc:name \\'nas_ct\\' .',\n",
    "    ':field_nas_ct sc:description \\'Flights delayed due to National Aviation System.\\' .',\n",
    "    ':field_nas_ct cr:dataType xsd:decimal .',\n",
    "\n",
    "    ':recordset cr:field :field_security_ct .',\n",
    "    ':field_security_ct rdf:type cr:Field .',\n",
    "    ':field_security_ct sc:name \\'security_ct\\' .',\n",
    "    ':field_security_ct sc:description \\'Flights canceled due to security issues.\\' .',\n",
    "    ':field_security_ct cr:dataType xsd:decimal .',\n",
    "\n",
    "    ':recordset cr:field :field_late_aircraft_ct .',\n",
    "    ':field_late_aircraft_ct rdf:type cr:Field .',\n",
    "    ':field_late_aircraft_ct sc:name \\'late_aircraft_ct\\' .',\n",
    "    ':field_late_aircraft_ct sc:description \\'Flights delayed due to a previous late aircraft.\\' .',\n",
    "    ':field_late_aircraft_ct cr:dataType xsd:decimal .',\n",
    "\n",
    "    ':recordset cr:field :field_arr_cancelled .',\n",
    "    ':field_arr_cancelled rdf:type cr:Field .',\n",
    "    ':field_arr_cancelled sc:name \\'arr_cancelled\\' .',\n",
    "    ':field_arr_cancelled sc:description \\'Number of cancelled flights.\\' .',\n",
    "    ':field_arr_cancelled cr:dataType xsd:integer .',\n",
    "\n",
    "    ':recordset cr:field :field_arr_diverted .',\n",
    "    ':field_arr_diverted rdf:type cr:Field .',\n",
    "    ':field_arr_diverted sc:name \\'arr_diverted\\' .',\n",
    "    ':field_arr_diverted sc:description \\'Number of diverted flights.\\' .',\n",
    "    ':field_arr_diverted cr:dataType xsd:integer .',\n",
    "\n",
    "    ':recordset cr:field :field_arr_delay .',\n",
    "    ':field_arr_delay rdf:type cr:Field .',\n",
    "    ':field_arr_delay sc:name \\'arr_delay\\' .',\n",
    "    ':field_arr_delay sc:description \\'Total delay time in minutes.\\' .',\n",
    "    ':field_arr_delay cr:dataType xsd:integer .',\n",
    "\n",
    "    ':recordset cr:field :field_carrier_delay .',\n",
    "    ':field_carrier_delay rdf:type cr:Field .',\n",
    "    ':field_carrier_delay sc:name \\'carrier_delay\\' .',\n",
    "    ':field_carrier_delay sc:description \\'Delay minutes due to air carrier issues.\\' .',\n",
    "    ':field_carrier_delay cr:dataType xsd:integer .',\n",
    "\n",
    "    ':recordset cr:field :field_weather_delay .',\n",
    "    ':field_weather_delay rdf:type cr:Field .',\n",
    "    ':field_weather_delay sc:name \\'weather_delay\\' .',\n",
    "    ':field_weather_delay sc:description \\'Delay minutes due to weather.\\' .',\n",
    "    ':field_weather_delay cr:dataType xsd:integer .',\n",
    "\n",
    "    ':recordset cr:field :field_nas_delay .',\n",
    "    ':field_nas_delay rdf:type cr:Field .',\n",
    "    ':field_nas_delay sc:name \\'nas_delay\\' .',\n",
    "    ':field_nas_delay sc:description \\'Delay minutes due to National Aviation System.\\' .',\n",
    "    ':field_nas_delay cr:dataType xsd:integer .',\n",
    "\n",
    "    ':recordset cr:field :field_security_delay .',\n",
    "    ':field_security_delay rdf:type cr:Field .',\n",
    "    ':field_security_delay sc:name \\'security_delay\\' .',\n",
    "    ':field_security_delay sc:description \\'Delay minutes due to security issues.\\' .',\n",
    "    ':field_security_delay cr:dataType xsd:integer .',\n",
    "\n",
    "    ':recordset cr:field :field_late_aircraft_delay .',\n",
    "    ':field_late_aircraft_delay rdf:type cr:Field .',\n",
    "    ':field_late_aircraft_delay sc:name \\'late_aircraft_delay\\' .',\n",
    "    ':field_late_aircraft_delay sc:description \\'Delay minutes due to previous late aircraft.\\' .',\n",
    "    ':field_late_aircraft_delay cr:dataType xsd:integer .',\n",
    "]\n",
    "engine.insert(data_triples, prefixes=prefixes)\n",
    "\n",
    "# Also add the units to the fields\n",
    "units_triples = [\n",
    "    ':field_arr_flights qudt:unit qudt:CountingUnit .',\n",
    "    ':field_arr_del15 qudt:unit qudt:CountingUnit .',\n",
    "    ':field_carrier_ct qudt:unit qudt:CountingUnit .',\n",
    "    ':field_weather_ct qudt:unit qudt:CountingUnit .',\n",
    "    ':field_nas_ct qudt:unit qudt:CountingUnit .',\n",
    "    ':field_security_ct qudt:unit qudt:CountingUnit .',\n",
    "    ':field_late_aircraft_ct qudt:unit qudt:CountingUnit .',\n",
    "    ':field_arr_cancelled qudt:unit qudt:CountingUnit .',\n",
    "    ':field_arr_diverted qudt:unit qudt:CountingUnit .',\n",
    "\n",
    "    ':field_arr_delay qudt:unit qudt:Minute .',\n",
    "    ':field_carrier_delay qudt:unit qudt:Minute .',\n",
    "    ':field_weather_delay qudt:unit qudt:Minute .',\n",
    "    ':field_nas_delay qudt:unit qudt:Minute .',\n",
    "    ':field_security_delay qudt:unit qudt:Minute .',\n",
    "    ':field_late_aircraft_delay qudt:unit qudt:Minute .',\n",
    "]\n",
    "engine.insert(units_triples, prefixes=prefixes)"
   ],
   "id": "ae14dd2cc686e863"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataset_description_code_writer = student_a\n",
    "\n",
    "def dataset_description(df: pd.DataFrame):\n",
    "    return df.describe().T\n",
    "\n",
    "start_time_co = now()\n",
    "dataset_stats = dataset_description(data)\n",
    "end_time_co = now()\n",
    "\n",
    "start_time_ho = now()\n",
    "print(dataset_stats)\n",
    "end_time_ho = now()\n",
    "\n",
    "# 1. Activty: Checking for outliers and creating the report\n",
    "co_ass_uuid_executor = \"293cd5a0-5335-4b10-b156-c22d9fa27454\"\n",
    "dataset_description_executor = [\n",
    "    f':dataset_description prov:qualifiedAssociation :{co_ass_uuid_executor} .',\n",
    "    f':{co_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{co_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(dataset_description_executor, prefixes=prefixes)\n",
    "\n",
    "co_ass_uuid_writer = \"cd4970df-9f40-4bb1-8fad-e4dc4fcdd284\"\n",
    "co_comment = \"\"\"\n",
    "For the numerical columns of the dataset the descriptive statistics are computed. The description includes count, mean, standard deviation,  minimum, maximum, and quartile values (25%, 50%, 75%).\n",
    "The summary provides an initial overview of data distributions and potential anomalies.\n",
    "\"\"\"\n",
    "dataset_description_activity = [\n",
    "    ':dataset_description rdf:type prov:Activity .',\n",
    "    ':dataset_description sc:isPartOf :data_understanding_phase .',\n",
    "    ':dataset_description rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':dataset_description rdfs:comment \"\"\"{co_comment}\"\"\" .',\n",
    "    f':dataset_description prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':dataset_description prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':dataset_description prov:qualifiedAssociation :{co_ass_uuid_writer} .',\n",
    "    f':{co_ass_uuid_writer} prov:agent :{dataset_description_code_writer} .',\n",
    "    f':{co_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':dataset_description prov:used :data .',\n",
    "    ':description_report rdf:type prov:Entity .',\n",
    "    f':description_report rdfs:comment \"\"\"{json.dumps(dataset_stats, indent=2)}\"\"\" .',\n",
    "    ':description_report prov:wasGeneratedBy :dataset_description .',\n",
    "    # ...\n",
    "]\n",
    "engine.insert(dataset_description_activity, prefixes=prefixes)\n",
    "\n",
    "# 2. Activity: Inspecting the report and taking a decision on what to do\n",
    "ior_ass_uuid_executor = \"64563522-7dcb-47bb-9be8-3f43b110531c\"\n",
    "ior_comment = \"\"\"\n",
    "The report shows that the count is 3343. This indicates missing values. Arriving flights ('arr_flights') show strong skewness, a median of 83 with over 19000 arriving flights. A reason could be a small number of large airports.\n",
    "Delayed flights ('arr_del15') have a total count of 2289 with a median of 12.\n",
    "The total arrival delay ('arr_delay'), as well as the causes of delay as carrier, NAS and late aircraft show a high standard deviation and maxima. This indicates extreme outliers.\n",
    "Wheres security and weather related delay have low medians.\n",
    "\"\"\"\n",
    "inspect_dataset_description_executor = student_a\n",
    "inspect_dataset_description_activity = [\n",
    "    ':inspect_dataset_description rdf:type prov:Activity .',\n",
    "    ':inspect_dataset_description rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':inspect_dataset_description rdfs:comment \"\"\"{co_comment}\"\"\" .',\n",
    "    f':inspect_dataset_description prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_dataset_description prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_dataset_description prov:qualifiedAssociation :{ior_ass_uuid_executor} .',\n",
    "    f':{ior_ass_uuid_executor} prov:agent :{inspect_dataset_description_executor} .',\n",
    "    f':{ior_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ior_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':inspect_dataset_description prov:used :description_report .',\n",
    "    ':description_decision rdf:type prov:Entity .',\n",
    "    f':description_decision rdfs:comment \"\"\"Further analysis checks have to be performed, to check for missing values, skewness and outliers.\"\"\" .',\n",
    "    ':description_decision prov:wasGeneratedBy :inspect_dataset_description .',\n",
    "    # ...\n",
    "]\n",
    "engine.insert(inspect_dataset_description_activity, prefixes=prefixes)"
   ],
   "id": "59f76287a66c4b76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "skewness_check_code_writer = student_a\n",
    "\n",
    "def check_skewness(df: pd.DataFrame, columns: list):\n",
    "    skewness_dict = {}\n",
    "\n",
    "    for column in columns:\n",
    "        s = df[column].dropna()\n",
    "        sk = s.skew()\n",
    "        skewness_dict[column] = sk\n",
    "    return skewness_dict\n",
    "\n",
    "start_time_ho = now()\n",
    "skewness_report = check_skewness(data, numeric_cols)\n",
    "end_time_co = now()\n",
    "\n",
    "start_time_co = now()\n",
    "for col, count in skewness_report.items():\n",
    "    print(f\"{col}: {count}\")\n",
    "end_time_ho = now()\n",
    "\n",
    "# 1. Activty: Checking for skewness and creating the report\n",
    "co_ass_uuid_executor = \"1790023e-315b-48f7-b39c-ba55c0341253\"\n",
    "skewness_check_executor = [\n",
    "    f':skewness_check prov:qualifiedAssociation :{co_ass_uuid_executor} .',\n",
    "    f':{co_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{co_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(skewness_check_executor, prefixes=prefixes)\n",
    "\n",
    "co_ass_uuid_writer = \"3a1bc808-4f3c-42a5-bcef-906363cba4ca\"\n",
    "co_comment = \"\"\"\n",
    "The dataset is checked for skewness in each numerical column. High absolute skewness\n",
    "values may indicate that the data is not normally distributed and\n",
    "may need transformation during the data preparation phase.\n",
    "\"\"\"\n",
    "skewness_check_activity = [\n",
    "    ':skewness_check rdf:type prov:Activity .',\n",
    "    ':skewness_check sc:isPartOf :data_understanding_phase .',\n",
    "    ':skewness_check rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':skewness_check rdfs:comment \"\"\"{co_comment}\"\"\" .',\n",
    "    f':skewness_check prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':skewness_check prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':skewness_check prov:qualifiedAssociation :{co_ass_uuid_writer} .',\n",
    "    f':{co_ass_uuid_writer} prov:agent :{skewness_check_code_writer} .',\n",
    "    f':{co_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':skewness_check prov:used :data .',\n",
    "    ':skewness_report rdf:type prov:Entity .',\n",
    "    f':skewness_report rdfs:comment \"\"\"{json.dumps(skewness_report, indent=2)}\"\"\" .',\n",
    "    ':skewness_report prov:wasGeneratedBy :skewness_check .',\n",
    "]\n",
    "engine.insert(skewness_check_activity, prefixes=prefixes)\n",
    "\n",
    "# 2. Activity: Inspecting the report and taking a decision on what to do\n",
    "ior_ass_uuid_executor = \"a6d6c34f-425d-4f76-871e-c234e51caeaa\"\n",
    "ior_comment = \"\"\"\n",
    "The report shows strong right skewness across all numeric columns. This indicates that most observations have low values while a small number of observations have very large values. The skewness is an indicator of the presence of outliers and the attributes not being normally distributed. Therefore needing transformation in the data preparation stage. NAS Delay shows the highest asymmetry.\n",
    "\"\"\"\n",
    "inspect_skewness_check_executor = student_a\n",
    "inspect_skewness_check_activity = [\n",
    "    ':inspect_skewness_check rdf:type prov:Activity .',\n",
    "    ':inspect_skewness_check rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':inspect_skewness_check rdfs:comment \"\"\"{co_comment}\"\"\" .',\n",
    "    f':inspect_skewness_check prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_skewness_check prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_skewness_check prov:qualifiedAssociation :{ior_ass_uuid_executor} .',\n",
    "    f':{ior_ass_uuid_executor} prov:agent :{inspect_skewness_check_executor} .',\n",
    "    f':{ior_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ior_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':inspect_skewness_check prov:used :skewness_report .',\n",
    "    ':skewness_decision rdf:type prov:Entity .',\n",
    "    f':skewness_decision rdfs:comment \"\"\"Records with high skewness require transformation\" .\"\"\" .',\n",
    "    ':skewness_decision prov:wasGeneratedBy :inspect_skewness_check .',\n",
    "]\n",
    "engine.insert(inspect_skewness_check_activity, prefixes=prefixes)\n"
   ],
   "id": "8e94c8198a24ecde"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "correlation_check_code_writer = student_a\n",
    "\n",
    "def correlation_check(df: pd.DataFrame, columns: list):\n",
    "    return df[columns].corr()\n",
    "\n",
    "start_time_co = now()\n",
    "correlation_report = correlation_check(data, numeric_cols)\n",
    "end_time_co = now()\n",
    "\n",
    "start_time_ho = now()\n",
    "print(correlation_report)\n",
    "end_time_ho = now()\n",
    "\n",
    "# 1. Activty: Checking for skewness and creating the report\n",
    "co_ass_uuid_executor = \"01c07e7a-6df4-4f5c-822f-b7ee19709a28\"\n",
    "correlation_check_executor = [\n",
    "    f':correlation_check prov:qualifiedAssociation :{co_ass_uuid_executor} .',\n",
    "    f':{co_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{co_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(correlation_check_executor, prefixes=prefixes)\n",
    "\n",
    "co_ass_uuid_writer = \"07ea56ce-5e16-4f63-a0ae-d90c6e8de83d\"\n",
    "co_comment = \"\"\"\n",
    "The dataset is checked for correlations between flight volume, delay counts, causes and durations.\n",
    "\"\"\"\n",
    "correlation_check_activity = [\n",
    "    ':correlation_check rdf:type prov:Activity .',\n",
    "    ':correlation_check sc:isPartOf :data_understanding_phase .',\n",
    "    ':correlation_check rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':correlation_check rdfs:comment \"\"\"{co_comment}\"\"\" .',\n",
    "    f':correlation_check prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':correlation_check prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':correlation_check prov:qualifiedAssociation :{co_ass_uuid_writer} .',\n",
    "    f':{co_ass_uuid_writer} prov:agent :{correlation_check_code_writer} .',\n",
    "    f':{co_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':correlation_check prov:used :data .',\n",
    "    ':correlation_report rdf:type prov:Entity .',\n",
    "    f':correlation_report rdfs:comment \"\"\"{json.dumps(correlation_report, indent=2)}\"\"\" .',\n",
    "    ':correlation_report prov:wasGeneratedBy :correlation_check .',\n",
    "]\n",
    "engine.insert(correlation_check_activity, prefixes=prefixes)\n",
    "\n",
    "# 2. Activity: Inspecting the report and taking a decision on what to do\n",
    "ior_ass_uuid_executor = \"a6d6c34f-425d-4f76-871e-c234e51caeaa\"\n",
    "ior_comment = \"\"\"\n",
    "The report shows a high correlation between arriving flights and flight delays. Airports with  higher traffic volume have a stronger correlation with total delay time. The three delay causes with the highest correlation with flight delays are late aircraft, carrier and NAS. In contrary weather and security show weaker correlations, indicating that these factors impact delays independently.\n",
    "\"\"\"\n",
    "inspect_correlation_check_executor = student_a\n",
    "inspect_correlation_check_activity = [\n",
    "    ':inspect_correlation_check rdf:type prov:Activity .',\n",
    "    ':inspect_correlation_check rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':inspect_correlation_check rdfs:comment \"\"\"{co_comment}\"\"\" .',\n",
    "    f':inspect_correlation_check prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_correlation_check prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_correlation_check prov:qualifiedAssociation :{ior_ass_uuid_executor} .',\n",
    "    f':{ior_ass_uuid_executor} prov:agent :{inspect_correlation_check_executor} .',\n",
    "    f':{ior_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ior_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':inspect_correlation_check prov:used :correlation_report .',\n",
    "    ':correlation_decision rdf:type prov:Entity .',\n",
    "    f':correlation_decision rdfs:comment \"\"\"No further actions are to be taken.\" .\"\"\" .',\n",
    "    ':correlation_decision prov:wasGeneratedBy :inspect_correlation_check .',\n",
    "    # ...\n",
    "]\n",
    "engine.insert(inspect_correlation_check_activity, prefixes=prefixes)"
   ],
   "id": "d5a0bcb720322d3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "kurtosis_check_code_writer = student_a\n",
    "\n",
    "def kurtosis_check(df: pd.DataFrame):\n",
    "    return df.kurtosis(numeric_only=True)\n",
    "\n",
    "start_time_co = now()\n",
    "kurtosis_report = kurtosis_check(data)\n",
    "end_time_co = now()\n",
    "\n",
    "start_time_ho = now()\n",
    "print(kurtosis_report)\n",
    "end_time_ho = now()\n",
    "\n",
    "# 1. Activty: Checking for skewness and creating the report\n",
    "co_ass_uuid_executor = \"cd43c98d-3b98-47b6-a987-028ffa2c2c21\"\n",
    "kurtosis_check_executor = [\n",
    "    f':kurtosis_check prov:qualifiedAssociation :{co_ass_uuid_executor} .',\n",
    "    f':{co_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{co_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(kurtosis_check_executor, prefixes=prefixes)\n",
    "\n",
    "co_ass_uuid_writer = \"23a4a301-437a-4302-9fd2-6e88d38d13b1\"\n",
    "co_comment = \"\"\"\n",
    "The dataset is analyzed for kurtosis across numerical columns.\n",
    "High kurtosis values indicate that the distributions have heavy tails and may contain extreme outliers.\n",
    "\"\"\"\n",
    "kurtosis_check_activity = [\n",
    "    ':kurtosis_check rdf:type prov:Activity .',\n",
    "    ':kurtosis_check sc:isPartOf :data_understanding_phase .',\n",
    "    ':kurtosis_check rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':kurtosis_check rdfs:comment \"\"\"{co_comment}\"\"\" .',\n",
    "    f':kurtosis_check prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':kurtosis_check prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':kurtosis_check prov:qualifiedAssociation :{co_ass_uuid_writer} .',\n",
    "    f':{co_ass_uuid_writer} prov:agent :{kurtosis_check_code_writer} .',\n",
    "    f':{co_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':kurtosis_check prov:used :data .',\n",
    "    ':kurtosis_report rdf:type prov:Entity .',\n",
    "    f':kurtosis_report rdfs:comment \"\"\"{json.dumps(kurtosis_report, indent=2)}\"\"\" .',\n",
    "    ':kurtosis_report prov:wasGeneratedBy :kurtosis_check .',\n",
    "]\n",
    "engine.insert(kurtosis_check_activity, prefixes=prefixes)\n",
    "\n",
    "# 2. Activity: Inspecting the report and taking a decision on what to do\n",
    "ior_ass_uuid_executor = \"a6d6c34f-425d-4f76-871e-c234e51caeaa\"\n",
    "ior_comment = \"\"\"\n",
    "The report shows a high kurtosis for number of flights canceled due to a security breach, and total time of delay due to National Aviation System, suggesting extreme outliers. Visualization of outliers is performed in a later step.\n",
    "\"\"\"\n",
    "inspect_kurtosis_check_executor = student_a\n",
    "inspect_kurtosis_check_activity = [\n",
    "    ':inspect_kurtosis_check rdf:type prov:Activity .',\n",
    "    ':inspect_kurtosis_check rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':inspect_kurtosis_check rdfs:comment \"\"\"{co_comment}\"\"\" .',\n",
    "    f':inspect_kurtosis_check prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_kurtosis_check prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_kurtosis_check prov:qualifiedAssociation :{ior_ass_uuid_executor} .',\n",
    "    f':{ior_ass_uuid_executor} prov:agent :{inspect_kurtosis_check_executor} .',\n",
    "    f':{ior_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ior_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':inspect_kurtosis_check prov:used :kurtosis_report .',\n",
    "    ':kurtosis_decision rdf:type prov:Entity .',\n",
    "    f':kurtosis_decision rdfs:comment \"\"\"Visualization of outliers is performed in a later step.\" .\"\"\" .',\n",
    "    ':kurtosis_decision prov:wasGeneratedBy :inspect_kurtosis_check .',\n",
    "    # ...\n",
    "]\n",
    "engine.insert(inspect_kurtosis_check_activity, prefixes=prefixes)"
   ],
   "id": "f5031443d433afbe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "missing_values_check_code_writer = student_a\n",
    "\n",
    "def missing_values_check(df: pd.DataFrame):\n",
    "    return {\n",
    "        column: {\n",
    "            \"missing\": int(df[column].isna().sum()),\n",
    "            \"total\": int(df[column].shape[0])\n",
    "        }\n",
    "        for column in df.columns\n",
    "    }\n",
    "\n",
    "start_time_ho = now()\n",
    "missing_values_report = missing_values_check(data)\n",
    "end_time_co = now()\n",
    "\n",
    "start_time_co = now()\n",
    "for col, count in missing_values_report.items():\n",
    "    print(f\"{col}: {count}\")\n",
    "end_time_ho = now()\n",
    "\n",
    "# 1. Activty: Checking for missing values and creating the report\n",
    "co_ass_uuid_executor = \"d4b9fc6b-b503-430c-afc5-936974356e3b\"\n",
    "missing_values_check_executor = [\n",
    "    f':missing_values_check prov:qualifiedAssociation :{co_ass_uuid_executor} .',\n",
    "    f':{co_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{co_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(missing_values_check_executor, prefixes=prefixes)\n",
    "\n",
    "co_ass_uuid_writer = \"cd4970df-9f40-4bb1-8fad-e4dc4fcdd284\"\n",
    "co_comment = \"\"\"\n",
    "The dataset is checked for missing values (NA). For each column that\n",
    "contains missing values, the report lists the indices of the rows in which\n",
    "those missing values occur.\n",
    "\"\"\"\n",
    "missing_values_check_activity = [\n",
    "    ':missing_values_check rdf:type prov:Activity .',\n",
    "    ':missing_values_check sc:isPartOf :data_understanding_phase .',\n",
    "    ':missing_values_check rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':missing_values_check rdfs:comment \"\"\"{co_comment}\"\"\" .',\n",
    "    f':missing_values_check prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':missing_values_check prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':missing_values_check prov:qualifiedAssociation :{co_ass_uuid_writer} .',\n",
    "    f':{co_ass_uuid_writer} prov:agent :{missing_values_check_code_writer} .',\n",
    "    f':{co_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':missing_values_check prov:used :data .',\n",
    "    ':missing_values_report rdf:type prov:Entity .',\n",
    "    f':missing_values_report rdfs:comment \"\"\"{json.dumps(missing_values_report, indent=2)}\"\"\" .',\n",
    "    ':missing_values_report prov:wasGeneratedBy :missing_values_check .',\n",
    "]\n",
    "engine.insert(missing_values_check_activity, prefixes=prefixes)\n",
    "\n",
    "# 2. Activity: Inspecting the report and taking a decision on what to do\n",
    "ior_ass_uuid_executor = \"a6d6c34f-425d-4f76-871e-c234e51caeaa\"\n",
    "ior_comment = \"\"\"\n",
    "The report shows 8 undefined values in all numerical columns. These will be handled in the data preparation phase. Reasons for missing values could be airlines or airports reporting incomplete data, or problems with the data integration. The amount of missing values is rather small with 8 rows containing missing data in comparison to 3351 rows in total. Given this low proportion, the potential impact on the analysis is expected to be minimal.\n",
    "\"\"\"\n",
    "inspect_missing_values_check_executor = student_a\n",
    "inspect_missing_values_check_activity = [\n",
    "    ':inspect_missing_values_check rdf:type prov:Activity .',\n",
    "    ':inspect_missing_values_check rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':inspect_missing_values_check rdfs:comment \"\"\"{co_comment}\"\"\" .',\n",
    "    f':inspect_missing_values_check prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_missing_values_check prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_missing_values_check prov:qualifiedAssociation :{ior_ass_uuid_executor} .',\n",
    "    f':{ior_ass_uuid_executor} prov:agent :{inspect_missing_values_check_executor} .',\n",
    "    f':{ior_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ior_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':inspect_missing_values_check prov:used :missing_values_report .',\n",
    "    ':missing_values_decision rdf:type prov:Entity .',\n",
    "    f':missing_values_decision rdfs:comment \"\"\"Records with missing values have to be removed in the data preparation phase.\" .\"\"\" .',\n",
    "    ':missing_values_decision prov:wasGeneratedBy :inspect_missing_values_check .',\n",
    "]\n",
    "engine.insert(inspect_missing_values_check_activity, prefixes=prefixes)\n"
   ],
   "id": "e7de732f3a438794"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plausible_values_code_writer = student_a\n",
    "\n",
    "def negative_values_check(df: pd.DataFrame, columns: list):\n",
    "    for col in columns:\n",
    "        if (df[col] < 0).any():\n",
    "            return f\"Column {col} has negative values!\"\n",
    "\n",
    "def check_delayed_flights_valid(df: pd.DataFrame):\n",
    "    invalid_del15 = df[df['arr_del15'] > df['arr_flights']]\n",
    "    if not invalid_del15.empty:\n",
    "        return \"Rows with arr_del15 > arr_flights:\", len(invalid_del15)\n",
    "    return None\n",
    "\n",
    "\n",
    "def check_cancelled_flights_valid(df: pd.DataFrame):\n",
    "    invalid_cancel = df[df['arr_cancelled'] > df['arr_flights']]\n",
    "    if not invalid_cancel.empty:\n",
    "        return \"Rows with arr_cancelled > arr_flights:\", len(invalid_cancel)\n",
    "    return None\n",
    "\n",
    "\n",
    "start_time_co = now()\n",
    "validation_results = {\n",
    "    \"negative_values\": negative_values_check(data, numeric_cols),\n",
    "    \"delayed_flights\": check_delayed_flights_valid(data),\n",
    "    \"cancelled_flights\": check_cancelled_flights_valid(data)\n",
    "}\n",
    "end_time_co = now()\n",
    "\n",
    "start_time_ho = now()\n",
    "for entry in validation_results:\n",
    "    print(entry, validation_results[entry])\n",
    "end_time_ho = now()\n",
    "\n",
    "# 1. Activty: Checking for outliers and creating the report\n",
    "co_ass_uuid_executor = \"c66bcb78-db91-441b-a926-60457b667cd3\"\n",
    "plausible_values_executor = [\n",
    "    f':plausible_values prov:qualifiedAssociation :{co_ass_uuid_executor} .',\n",
    "    f':{co_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{co_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(plausible_values_executor, prefixes=prefixes)\n",
    "\n",
    "co_ass_uuid_writer = \"8f6aeed4-2d23-4ede-a632-a5f58157d157\"\n",
    "co_comment = \"\"\"\n",
    "The dataset is checked for the plausibility of the values. Data validation checks include verification of no negative values, as well as cancelled and delayed flights not exceeding arriving flights.\n",
    "\"\"\"\n",
    "plausible_values_activity = [\n",
    "    ':plausible_values rdf:type prov:Activity .',\n",
    "    ':plausible_values sc:isPartOf :data_understanding_phase .',\n",
    "    ':plausible_values rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':plausible_values rdfs:comment \"\"\"{co_comment}\"\"\" .',\n",
    "    f':plausible_values prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':plausible_values prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':plausible_values prov:qualifiedAssociation :{co_ass_uuid_writer} .',\n",
    "    f':{co_ass_uuid_writer} prov:agent :{plausible_values_code_writer} .',\n",
    "    f':{co_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':plausible_values prov:used :data .',\n",
    "    ':plausible_values_report rdf:type prov:Entity .',\n",
    "    f':plausible_values_report rdfs:comment \"\"\"{json.dumps(validation_results, indent=2)}\"\"\" .',\n",
    "    ':plausible_values_report prov:wasGeneratedBy :plausible_values .',\n",
    "]\n",
    "engine.insert(plausible_values_activity, prefixes=prefixes)\n",
    "\n",
    "# 2. Activity: Inspecting the report and taking a decision on what to do\n",
    "ior_ass_uuid_executor = \"6eaa2c0a-e592-4d85-b37f-d695844910cf\"\n",
    "ior_comment = \"\"\"\n",
    "The report shows no negative values and both delayed and canceled flights not exceeding arriving flights.\n",
    "\"\"\"\n",
    "plausible_values_report_executor = student_a\n",
    "plausible_values_report_activity = [\n",
    "    ':inspect_plausible_values_report rdf:type prov:Activity .',\n",
    "    ':inspect_plausible_values_report rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':inspect_plausible_values_report rdfs:comment \"\"\"{co_comment}\"\"\" .',\n",
    "    f':inspect_plausible_values_report prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_plausible_values_report prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_plausible_values_report prov:qualifiedAssociation :{ior_ass_uuid_executor} .',\n",
    "    f':{ior_ass_uuid_executor} prov:agent :{plausible_values_report_executor} .',\n",
    "    f':{ior_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ior_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':inspect_plausible_values_report prov:used :plausible_values_report .',\n",
    "    ':plausible_values_decision rdf:type prov:Entity .',\n",
    "    f':plausible_values_decision rdfs:comment \"\"\"No actions have to be taken.\"\"\" .',\n",
    "    ':plausible_values_decision prov:wasGeneratedBy :inspect_plausible_values_report .',\n",
    "    # ...\n",
    "]\n",
    "engine.insert(plausible_values_report_activity, prefixes=prefixes)"
   ],
   "id": "8f320036d8e7dea7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "iqr_check_code_writer = student_a\n",
    "\n",
    "def iqr_outliers(df: pd.DataFrame, columns, multiplier=1.5):\n",
    "    results = {}\n",
    "\n",
    "    for col in columns:\n",
    "        values = df[col]\n",
    "        q1 = values.quantile(0.25)\n",
    "        q3 = values.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "\n",
    "        lower_bound = q1 - multiplier * iqr\n",
    "        upper_bound = q3 + multiplier * iqr\n",
    "\n",
    "        outliers_above = df[df[col] > upper_bound].index.tolist()\n",
    "        outliers_below = df[df[col] < lower_bound].index.tolist()\n",
    "\n",
    "        results[col] = {\n",
    "            \"lower_bound\": lower_bound,\n",
    "            \"upper_bound\": upper_bound,\n",
    "            \"n_outliers_above\": len(outliers_above),\n",
    "            \"n_outliers_below\": len(outliers_below),\n",
    "            \"indices_above\": outliers_above[:5],\n",
    "            \"indices_below\": outliers_below[:5],\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "start_time_co = now()\n",
    "outliers = iqr_outliers(data, numeric_cols, multiplier = 3)\n",
    "end_time_co = now()\n",
    "\n",
    "start_time_ho = now()\n",
    "print(outliers)\n",
    "end_time_ho = now()\n",
    "\n",
    "# 1. Activty: Checking for skewness and creating the report\n",
    "co_ass_uuid_executor = \"4dbb7e2e-7db7-43b2-b022-6f9cbfd2abf8\"\n",
    "iqr_check_executor = [\n",
    "    f':iqr_check prov:qualifiedAssociation :{co_ass_uuid_executor} .',\n",
    "    f':{co_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{co_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(missing_values_check_executor, prefixes=prefixes)\n",
    "\n",
    "co_ass_uuid_writer = \"38d424c1-039f-4cb7-a0f3-2f0d922110ff\"\n",
    "co_comment = \"\"\"\n",
    "In this report potential outliers are identified using the Interquartile Range (IQR) method.\n",
    "A multiplier of 3 is used instead of the standard 1.5 to flag only extreme deviations. This is done because of the highly skewed count and delay data.\n",
    "To keep the report concise and readable, only the first five outlier indices per variable are recorded.\n",
    "\"\"\"\n",
    "iqr_check_activity = [\n",
    "    ':iqr_check rdf:type prov:Activity .',\n",
    "    ':iqr_check sc:isPartOf :data_understanding_phase .',\n",
    "    ':iqr_check rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':iqr_check rdfs:comment \"\"\"{co_comment}\"\"\" .',\n",
    "    f':iqr_check prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':iqr_check prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':iqr_check prov:qualifiedAssociation :{co_ass_uuid_writer} .',\n",
    "    f':{co_ass_uuid_writer} prov:agent :{iqr_check_code_writer} .',\n",
    "    f':{co_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':iqr_check prov:used :data .',\n",
    "    ':iqr_report rdf:type prov:Entity .',\n",
    "    f':iqr_report rdfs:comment \"\"\"{json.dumps(outliers, indent=2)}\"\"\" .',\n",
    "    ':iqr_report prov:wasGeneratedBy :iqr_check .',\n",
    "]\n",
    "engine.insert(iqr_check_activity, prefixes=prefixes)\n",
    "\n",
    "# 2. Activity: Inspecting the report and taking a decision on what to do\n",
    "ior_ass_uuid_executor = \"a6d6c34f-425d-4f76-871e-c234e51caeaa\"\n",
    "ior_comment = \"\"\"\n",
    "The dataset contains numerous high-value outliers, primarily in total arriving flights, flights delayed over 15 minutes, carrier-related delays, NAS delays, and late aircraft delays. This is expected because of the highly data.\"\"\"\n",
    "inspect_iqr_check_executor = student_a\n",
    "inspect_iqr_check_activity = [\n",
    "    ':inspect_iqr_check rdf:type prov:Activity .',\n",
    "    ':inspect_iqr_check rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':inspect_iqr_check rdfs:comment \"\"\"{co_comment}\"\"\" .',\n",
    "    f':inspect_iqr_check prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_iqr_check prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_iqr_check prov:qualifiedAssociation :{ior_ass_uuid_executor} .',\n",
    "    f':{ior_ass_uuid_executor} prov:agent :{inspect_iqr_check_executor} .',\n",
    "    f':{ior_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ior_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':inspect_iqr_check prov:used :iqr_report .',\n",
    "    ':iqr_decision rdf:type prov:Entity .',\n",
    "    f':iqr_decision rdfs:comment \"\"\"Visualization of outliers is performed in a later step.\" .\"\"\" .',\n",
    "    ':iqr_decision prov:wasGeneratedBy :inspect_iqr_check .',\n",
    "    # ...\n",
    "]\n",
    "engine.insert(inspect_iqr_check_activity, prefixes=prefixes)"
   ],
   "id": "91b1193b616b2e07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "outliers_visualization_code_writer = student_a\n",
    "\n",
    "def outliers_visualization(df: pd.DataFrame, columns: list):\n",
    "    for col in columns:\n",
    "        plt.figure(figsize=(6,2))\n",
    "        seaborn.boxplot(x=df[col])\n",
    "        plt.title(f\"Boxplot of {col}\")\n",
    "        plt.show()\n",
    "\n",
    "start_time_co = now()\n",
    "outlier_report = outliers_visualization(data, numeric_cols)\n",
    "end_time_co = now()\n",
    "\n",
    "# 1. Activty: Checking for skewness and creating the report\n",
    "co_ass_uuid_executor = \"e7726ebf-a948-414c-af0c-5cbb34d8e83c\"\n",
    "outliers_visualization_executor = [\n",
    "    f':outliers_visualization prov:qualifiedAssociation :{co_ass_uuid_executor} .',\n",
    "    f':{co_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{co_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(outliers_visualization_executor, prefixes=prefixes)\n",
    "\n",
    "co_ass_uuid_writer = \"28cca8ce-55b2-410e-a490-e8fede217511\"\n",
    "co_comment = \"\"\"\n",
    "The dataset is visualized using boxplots for numeric columns to identify potential outliers. There have been indication about outliers in the kurtosis check before.\n",
    "\"\"\"\n",
    "outliers_visualization_activity = [\n",
    "    ':outliers_visualization rdf:type prov:Activity .',\n",
    "    ':outliers_visualization sc:isPartOf :data_understanding_phase .',\n",
    "    ':outliers_visualization rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':outliers_visualization rdfs:comment \"\"\"{co_comment}\"\"\" .',\n",
    "    f':outliers_visualization prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':outliers_visualization prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':outliers_visualization prov:qualifiedAssociation :{co_ass_uuid_writer} .',\n",
    "    f':{co_ass_uuid_writer} prov:agent :{outliers_visualization_code_writer} .',\n",
    "    f':{co_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':outliers_visualization prov:used :data .',\n",
    "    ':outliers_visualization_report rdf:type prov:Entity .',\n",
    "    f':outliers_visualization_report rdfs:comment \"\"\"{json.dumps(outlier_report, indent=2)}\"\"\" .',\n",
    "    ':outliers_visualization_report prov:wasGeneratedBy :outliers_visualization .',\n",
    "]\n",
    "engine.insert(outliers_visualization_activity, prefixes=prefixes)\n",
    "\n",
    "# 2. Activity: Inspecting the report and taking a decision on what to do\n",
    "ior_ass_uuid_executor = \"a6d6c34f-425d-4f76-871e-c234e51caeaa\"\n",
    "ior_comment = \"\"\"\n",
    "As indicated in the kurtosis check before number of delays due to security ans NAS show extreme outliers. Furthermore the columns of arriving and canceled flights show high outliers.\n",
    "\"\"\"\n",
    "inspect_outliers_visualization_executor = student_a\n",
    "inspect_outliers_visualization_activity = [\n",
    "    ':inspect_outliers_visualization rdf:type prov:Activity .',\n",
    "    ':inspect_outliers_visualization rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':inspect_outliers_visualization rdfs:comment \"\"\"{co_comment}\"\"\" .',\n",
    "    f':inspect_outliers_visualization prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_outliers_visualization prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_outliers_visualization prov:qualifiedAssociation :{ior_ass_uuid_executor} .',\n",
    "    f':{ior_ass_uuid_executor} prov:agent :{inspect_outliers_visualization_executor} .',\n",
    "    f':{ior_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ior_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':inspect_outliers_visualization prov:used :outliers_visualization_report .',\n",
    "    ':outliers_decision rdf:type prov:Entity .',\n",
    "    f':outliers_decision rdfs:comment \"\"\"Outliers will not be removed, as they have significant value to the airport and airline data.\" .\"\"\" .',\n",
    "    ':outliers_decision prov:wasGeneratedBy :inspect_outliers_visualization .',\n",
    "]\n",
    "engine.insert(inspect_outliers_visualization_activity, prefixes=prefixes)"
   ],
   "id": "c926c0263567898"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "histogram_visualization_code_writer = student_a\n",
    "\n",
    "def histogram(df: pd.DataFrame, columns: list):\n",
    "    for col in columns:\n",
    "        plt.figure(figsize=(6,3))\n",
    "        seaborn.histplot(df[col], bins=100, kde=False)\n",
    "        plt.title(f\"Histogram of {col}\")\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.yscale('log')\n",
    "        plt.show()\n",
    "\n",
    "start_time_co = now()\n",
    "histogram_chart = histogram(data, numeric_cols)\n",
    "end_time_co = now()\n",
    "\n",
    "# 1. Activty: Checking for skewness and creating the report\n",
    "co_ass_uuid_executor = \"a6b7e087-2ebe-4c76-ad2d-e0d49d6930dd\"\n",
    "histogram_executor = [\n",
    "    f':histogram prov:qualifiedAssociation :{co_ass_uuid_executor} .',\n",
    "    f':{co_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{co_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(histogram_executor, prefixes=prefixes)\n",
    "\n",
    "co_ass_uuid_writer = \"3631abe4-0d22-4f50-a3a1-57cc4ef29072\"\n",
    "co_comment = \"\"\"\n",
    "The dataset is visualized using histograms for numeric columns to display the distribution.\n",
    "\"\"\"\n",
    "check_outliers_activity = [\n",
    "    ':histogram rdf:type prov:Activity .',\n",
    "    ':histogram sc:isPartOf :data_understanding_phase .',\n",
    "    ':histogram rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':histogram rdfs:comment \"\"\"{co_comment}\"\"\" .',\n",
    "    f':histogram prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':histogram prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':histogram prov:qualifiedAssociation :{co_ass_uuid_writer} .',\n",
    "    f':{co_ass_uuid_writer} prov:agent :{histogram_visualization_code_writer} .',\n",
    "    f':{co_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':histogram prov:used :data .',\n",
    "    ':histogram_report rdf:type prov:Entity .',\n",
    "    f':histogram_report rdfs:comment \"\"\"{json.dumps(histogram_chart, indent=2)}\"\"\" .',\n",
    "    'histogram_report prov:wasGeneratedBy :histogram .',\n",
    "]\n",
    "engine.insert(check_outliers_activity, prefixes=prefixes)\n",
    "\n",
    "# 2. Activity: Inspecting the report and taking a decision on what to do\n",
    "ior_ass_uuid_executor = \"a6d6c34f-425d-4f76-871e-c234e51caeaa\"\n",
    "ior_comment = \"\"\"\n",
    "The histograms show a right skewed distribution, with a concentration of low values. The outliers can be detected as well.\n",
    "\"\"\"\n",
    "inspect_histogram_executor = student_a\n",
    "inspect_histogram_activity = [\n",
    "    ':inspect_histogram rdf:type prov:Activity .',\n",
    "    ':inspect_histogram rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':inspect_histogram rdfs:comment \"\"\"{co_comment}\"\"\" .',\n",
    "    f':inspect_histogram prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_histogram prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_histogram prov:qualifiedAssociation :{ior_ass_uuid_executor} .',\n",
    "    f':{ior_ass_uuid_executor} prov:agent :{inspect_histogram_executor} .',\n",
    "    f':{ior_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ior_ass_uuid_executor} prov:hadRole :{code_executor_role} .'\n",
    "]\n",
    "engine.insert(inspect_histogram_activity, prefixes=prefixes)"
   ],
   "id": "511b2dd6fb66d166"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ethically_sensitive_data_evaluation = \"\"\"\n",
    "The dataset does not contain any personal or demographic data, making it low risk of ethical sensitivity. Smaller airlines such as Hawaiian Airlines Inc. show significantly lower amount of flights, than big airlines e.g. JetBlue Airways. Therefore the model will perform well for large airlines, but not for smaller airlines. Due to class imbalance between large and small airlines, macro-averaged evaluation metrics would be necessary to ensure equal performance. Oversampling of the smaller airlines could be used to reduce the bias caused by the imbalance.\n",
    "\"\"\"\n",
    "\n",
    "ethically_sensitive_data_evaluation_activity = [\n",
    "    ':ethically_sensitive_data_evaluation rdf:type prov:Activity .',\n",
    "    ':ethically_sensitive_data_evaluation sc:isPartOf :data_understanding_phase .',\n",
    "    ':ethically_sensitive_data_evaluation rdfs:label \"Data Understanding – Evaluation of ethically sensitive data\" .',\n",
    "    f':ethically_sensitive_data_evaluation rdfs:comment \"\"\"{ethically_sensitive_data_evaluation}\"\"\" .'\n",
    "]\n",
    "\n",
    "engine.insert(ethically_sensitive_data_evaluation_activity, prefixes=prefixes)"
   ],
   "id": "6dad00a5a067c6bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "bias_evaluation = \"\"\"\n",
    "As the dataset only covers the months of December, the other months are not represented. Therefore seasonal patterns can not be analyzed. As the dataset contains data of the year 2019 and 2020, comparisons might be skewed, because of the pandemic during December 2020. Large airlines with more frequent flights than smaller airlines dominate the dataset, leading to representation bias towards the bigger airlines. A risk could be missing data, or inconsistent reporting. An expert could give insights on how the pandemic influenced the number of flights, operational change and data quality, as most of the missing data values were recorded in the year 2020.\n",
    "\"\"\"\n",
    "\n",
    "bias_evaluation_activity = [\n",
    "    ':bias_evaluation rdf:type prov:Activity .',\n",
    "    ':bias_evaluation sc:isPartOf :data_understanding_phase .',\n",
    "    ':bias_evaluation rdfs:label \"Data Understanding – Risks and additional types of bias\" .',\n",
    "    f':bias_evaluation rdfs:comment \"\"\"{bias_evaluation}\"\"\" .'\n",
    "]\n",
    "\n",
    "engine.insert(bias_evaluation_activity, prefixes=prefixes)"
   ],
   "id": "77e58eea0c434d0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "actions_evaluation = \"\"\"\n",
    "The following actions are likely required based on the analysis.\n",
    "Removal of missing values. Removal of outliers.\n",
    "Potential removal of attributes with strong correlation.\n",
    "Scaling and log Transformation.\n",
    "\"\"\"\n",
    "\n",
    "actions_evaluation_activity = [\n",
    "    ':actions_evaluation rdf:type prov:Activity .',\n",
    "    ':actions_evaluation sc:isPartOf :data_understanding_phase .',\n",
    "    ':actions_evaluation rdfs:label \"Data Understanding – Potential actions to be taken\" .',\n",
    "    f':actions_evaluation rdfs:comment \"\"\"{actions_evaluation}\"\"\" .'\n",
    "]\n",
    "\n",
    "engine.insert(actions_evaluation_activity, prefixes=prefixes)"
   ],
   "id": "53e95d8dc90ba6f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preparation",
   "id": "b058e71c67b69352"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Each Activity that follows is part of the Data Preparation Phase\n",
    "\n",
    "data_preparation_phase_executor = [\n",
    "    f':data_preparation_phase rdf:type prov:Activity .',\n",
    "    f':data_preparation_phase rdfs:label \"Data Preparation Phase\" .',\n",
    "]\n",
    "engine.insert(data_preparation_phase_executor, prefixes=prefixes)"
   ],
   "id": "cefb5957ba28046c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "handle_outliers_code_writer = student_b\n",
    "\n",
    "\n",
    "def handle_outliers(df: pd.DataFrame, outliers_report: dict) -> pd.DataFrame:\n",
    "    df_clean = df.copy()\n",
    "    n_start = len(df_clean)\n",
    "\n",
    "    def log(rule_name, before, after):\n",
    "        print(f\"[{rule_name}] removed {before - after} rows (remaining: {after})\")\n",
    "\n",
    "\n",
    "    numeric_cols = [\n",
    "    'arr_flights', 'arr_del15', 'carrier_ct', 'weather_ct', 'nas_ct',\n",
    "    'security_ct', 'late_aircraft_ct', 'arr_cancelled', 'arr_diverted',\n",
    "    'arr_delay', 'carrier_delay', 'weather_delay', 'nas_delay',\n",
    "    'security_delay', 'late_aircraft_delay'\n",
    "    ]\n",
    "\n",
    "    # Drop rows where we have NA values in some columns\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean.dropna(subset=numeric_cols)\n",
    "    after = len(df_clean)\n",
    "    log(\"Drop NAs\", before, after)\n",
    "\n",
    "    # Drop rows with negative values where not meaningful\n",
    "    before = len(df_clean)\n",
    "    for col in numeric_cols:\n",
    "        df_clean = df_clean[df_clean[col] >= 0]\n",
    "    after = len(df_clean)\n",
    "    log(\"Negative values\", before, after)\n",
    "\n",
    "    # Delayed flights cannot exceed arriving flights\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean[\"arr_del15\"] <= df_clean[\"arr_flights\"]]\n",
    "    after = len(df_clean)\n",
    "    log(\"Delayed > arriving flights\", before, after)\n",
    "\n",
    "    # Cancelled flights cannot exceed arriving flights\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean[\"arr_cancelled\"] <= df_clean[\"arr_flights\"]]\n",
    "    after = len(df_clean)\n",
    "    log(\"Cancelled > arriving flights\", before, after)\n",
    "\n",
    "    # Delay minutes must be zero if no delayed flights occurred\n",
    "    before = len(df_clean)\n",
    "    delay_min_cols = [\n",
    "        \"arr_delay\", \"carrier_delay\", \"weather_delay\",\n",
    "        \"nas_delay\", \"security_delay\", \"late_aircraft_delay\"\n",
    "    ]\n",
    "    no_delays = df_clean[\"arr_del15\"] == 0\n",
    "    for col in delay_min_cols:\n",
    "        df_clean = df_clean[~(no_delays & (df_clean[col] > 0))]\n",
    "    after = len(df_clean)\n",
    "    log(\"Delay minutes with zero delayed flights\", before, after)\n",
    "\n",
    "    # Division-by-zero cases: arrivals == 0 but delays recorded\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[~((df_clean[\"arr_flights\"] == 0) & (df_clean[\"arr_delay\"] > 0))]\n",
    "    after = len(df_clean)\n",
    "    log(\"Arrivals = 0 with delay minutes\", before, after)\n",
    "\n",
    "    print(f\"[TOTAL] removed {n_start - len(df_clean)} rows (final size: {len(df_clean)})\")\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "start_time_td = now()\n",
    "cleaned_data = handle_outliers(data, data)\n",
    "end_time_td = now()\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "# This is the continuation of the example from the Data Understanding phase above.\n",
    "# There are three steps involved in this process:\n",
    "# 1. activity creates a figure, report etc. => already done in data understanding phase\n",
    "# 2. activity inspects the outcome and derives decisions => already done in data understanding phase\n",
    "# 3. activity follows up on the decision by changing the data => in this case by removing the the outliers that were found\n",
    "\n",
    "ro_ass_uuid_executor = \"ec7e81e1-86ea-475a-a8d4-c7d8ee535488\"\n",
    "handle_outliers_executor = [\n",
    "    f':handle_outliers prov:qualifiedAssociation :{ro_ass_uuid_executor} .',\n",
    "    f':{ro_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ro_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ro_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(handle_outliers_executor, prefixes=prefixes)\n",
    "\n",
    "td_ass_uuid_writer = \"1405f15a-3545-4014-a962-637f3c10a137\"\n",
    "td_comment = \"\"\"\n",
    "Removing all outliers or unusual data points that cannot be used further in our model. We dropp rows with NAs, negative values (where it does not make sense) and remove also rows where we made a sanity check (e.g. delayed flights count cannot exceed total flights arrived).\n",
    "\"\"\"\n",
    "handle_outliers_activity = [\n",
    "    ':handle_outliers rdf:type prov:Activity .',\n",
    "    ':handle_outliers sc:isPartOf :data_preparation_phase .',\n",
    "    #':handle_outliers rdfs:comment \\'Data Preparation\\' .',\n",
    "    f':handle_outliers rdfs:comment \"\"\"{td_comment}\"\"\" .',\n",
    "    f':handle_outliers prov:startedAtTime \"{start_time_td}\"^^xsd:dateTime .',\n",
    "    f':handle_outliers prov:endedAtTime \"{end_time_td}\"^^xsd:dateTime .',\n",
    "    f':handle_outliers prov:qualifiedAssociation :{td_ass_uuid_writer} .',\n",
    "    f':{td_ass_uuid_writer} prov:agent :{handle_outliers_code_writer} .',\n",
    "    f':{td_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{td_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':handle_outliers prov:used :data .',\n",
    "    ':handle_outliers prov:used :outlier_decision .',\n",
    "    ':cleaned_data rdf:type prov:Entity .',\n",
    "    ':cleaned_data prov:wasGeneratedBy :handle_outliers .',\n",
    "    ':cleaned_data prov:wasDerivedFrom :data .',\n",
    "]\n",
    "engine.insert(handle_outliers_activity, prefixes=prefixes)"
   ],
   "id": "28e906083b831520"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Continue with other tasks of the Data Preparation phase such as binning, scaling etc...**",
   "id": "e611e7686c7af7fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "task3b_comment = \"\"\"\n",
    "Several preprocessing steps were considered but deliberately not applied:\n",
    "\n",
    "- Outlier removal: The numerical variables are extreme right-skewed, so those values correspond to large airports, carriers or extreme delay events. If we would remove those, we would distort the business objective and underrepresent the major datapoints. We want to reserve the real world variability.\n",
    "\n",
    "- Scaling/Log transformations: We decided to not scale some columns yet, since me may do that in the modeling phase if it is required by an algorithm\n",
    "\n",
    "- Removal of attributes due to correlation: We have a strong correlation between arr_delay and carrier_delay, nas_delay, .. and also between  arr_del15 and other delay counts. We believe the different delay values encode different operational causes so we would remove some information which could be necessary for our business objective.\n",
    "\n",
    "- Encoding of categorical attributes: We will do that in the modeling phase since it depends on which algorithm we use.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "task3b_activity = [\n",
    "    ':task3b rdf:type prov:Activity .',\n",
    "    ':task3b sc:isPartOf :data_preparation_phase .',\n",
    "    ':task3b rdfs:label \"Data Preparation – Considered but Not Applied Steps\" .',\n",
    "    f':task3b rdfs:comment \"\"\"{task3b_comment}\"\"\" .'\n",
    "]\n",
    "\n",
    "engine.insert(task3b_activity, prefixes=prefixes)"
   ],
   "id": "2c4abd39344fb90f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "task3c_comment = \"\"\"\n",
    "Several options for derived attributes were analyzed.\n",
    "\n",
    "- Relative delay: arr_delay/arr_flights\n",
    "with that we can easily compare delay across airports and carriers of different sizes, for small flight counts it could be unstable\n",
    "- High potential\n",
    "\n",
    "- Proportion of total delay attributable to each cause: e.g. weather_Delay/ arr_delay or carrier_delay / arr_delay\n",
    "We can capture the structure of delays rather than absolute volume\n",
    "- high potential\n",
    "\n",
    "- Aggregation statistics: e.g. average delay per airport, cancellationrate per carrier\n",
    "Capture historical performance, improve predictive performance (data leakage danger!)\n",
    "- High potential\n",
    "\"\"\"\n",
    "\n",
    "task3c_activity = [\n",
    "    ':task3c rdf:type prov:Activity .',\n",
    "    ':task3c sc:isPartOf :data_preparation_phase .',\n",
    "    ':task3c rdfs:label \"Data Preparation – Derived Attribute Analysis\" .',\n",
    "    f':task3c rdfs:comment \"\"\"{task3c_comment}\"\"\" .'\n",
    "]\n",
    "\n",
    "engine.insert(task3c_activity, prefixes=prefixes)\n"
   ],
   "id": "5110222ea05e857e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "task3d_comment = \"\"\"\n",
    "Several external data sources were identified as potentially beneficial.\n",
    "\n",
    "- Weather data: Observations linked to airport and date\n",
    "- Airport infrastructure data: Characteristics like number of runways, passenger volume, runway utilization\n",
    "- Airline operational characteristics: fleet size and age, network complexity\n",
    "- Public holiday and event calendar: Flag major travel periods or high demand spikes caused by events\n",
    "- Air traffic control: Information on airspace restrictions or other temporary regulations\n",
    "\"\"\"\n",
    "\n",
    "task3d_activity = [\n",
    "    ':task3d rdf:type prov:Activity .',\n",
    "    ':task3d sc:isPartOf :data_preparation_phase .',\n",
    "    ':task3d rdfs:label \"Data Preparation – External Data Source Analysis\" .',\n",
    "    f':task3d rdfs:comment \"\"\"{task3d_comment}\"\"\" .'\n",
    "]\n",
    "\n",
    "engine.insert(task3d_activity, prefixes=prefixes)"
   ],
   "id": "3f5accf91eb1c034"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Your final transformed dataset should also be documented appropriately using Croissant, SI, etc.\n",
    "\n",
    "prepared_data_triples = [\n",
    "    ':prepared_data rdf:type prov:Entity .',\n",
    "    ':prepared_data rdf:type sc:Dataset .',\n",
    "    ':prepared_data rdfs:label \"Prepared Airline Delay Dataset\" .',\n",
    "    ':prepared_data prov:wasDerivedFrom :cleaned_data .',\n",
    "    ':prepared_data prov:wasDerivedFrom :data .',\n",
    "    ':prepared_data rdfs:comment \"\"\"Final dataset after data preparation. Contains only valid, non-negative, and semantically consistent airline delay records. Used as input for the modeling phase.\"\"\" .'\n",
    "]\n",
    "engine.insert(prepared_data_triples, prefixes=prefixes)"
   ],
   "id": "33fb88cb8a1d9bdb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Modeling",
   "id": "40037e1008bccd85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Each Activity that follows is part of the Modeling Phase\n",
    "\n",
    "modeling_phase_executor = [\n",
    "    f':modeling_phase rdf:type prov:Activity .',\n",
    "    f':modeling rdfs:label \"Modeling Phase\" .',\n",
    "]\n",
    "engine.insert(modeling_phase_executor, prefixes=prefixes)\n"
   ],
   "id": "b3d5a341602bc796"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_data_code_writer = student_a\n",
    "\n",
    "#############################################\n",
    "# Documentation 4a\n",
    "#############################################\n",
    "\n",
    "dma_ass_uuid_writer = \"b3e840ab-ac23-415e-bd9c-6d00bb79c37a\"\n",
    "dma_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "identify_data_mining_algorithm_activity = [\n",
    "    f':define_algorithm rdf:type prov:Activity .',\n",
    "    f':define_algorithm sc:isPartOf :modeling_phase .',\n",
    "    f':define_algorithm rdfs:comment \"\"\"{dma_comment}\"\"\" .',\n",
    "    f':define_algorithm prov:qualifiedAssociation :{dma_ass_uuid_writer} .',\n",
    "    f':{dma_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{dma_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{dma_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # example algorithm definition\n",
    "    f':random_forest_algorithm rdf:type mls:Algorithm .',\n",
    "    f':random_forest_algorithm rdfs:label \"Random Forest Algorithm\" .',\n",
    "\n",
    "    # example implementation\n",
    "    f':random_forrest_classifier_implementation rdf:type mls:Implementation .',\n",
    "    f':random_forrest_classifier_implementation rdfs:label \"Scikit-learn RandomForestClassifier\" .',\n",
    "    f':random_forrest_classifier_implementation mls:implements :random_forest_algorithm .',\n",
    "    f':random_forrest_classifier_implementation prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "    # you can also define your Evaluation Measures here\n",
    "\n",
    "    # example evaluation \n",
    "    f':r2_score_measure rdf:type mls:EvaluationMeasure .',\n",
    "    f':r2_score_measure rdfs:label \"R-squared Score\" .',\n",
    "    f':r2_score_measure rdfs:comment \"xxx\" .',\n",
    "    f':r2_score_measure prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "]\n",
    "engine.insert(identify_data_mining_algorithm_activity, prefixes=prefixes)"
   ],
   "id": "32a88d401405f2a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#############################################\n",
    "# Documentation 4b\n",
    "#############################################\n",
    "\n",
    "hp_ass_uuid_writer = \"fff582a8-c5cd-4030-978b-9f56b603167c\"\n",
    "hp_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "identify_hp_activity = [\n",
    "    f':identify_hyperparameters rdf:type prov:Activity .',\n",
    "    f':identify_hyperparameters sc:isPartOf :modeling_phase .',\n",
    "    f':identify_hyperparameters rdfs:comment \"\"\"{hp_comment}\"\"\" .',\n",
    "    f':identify_hyperparameters prov:qualifiedAssociation :{hp_ass_uuid_writer} .',\n",
    "    f':{hp_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{hp_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{hp_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # example parameter\n",
    "    f':hp_learning_rate rdf:type mls:HyperParameter .',\n",
    "    f':hp_learning_rate rdfs:label \"Learning Rate\" .',\n",
    "    f':hp_learning_rate rdfs:comment \"...\" .',\n",
    "    f':random_forrest_classifier_implementation mls:hasHyperParameter :hp_learning_rate .',\n",
    "    f':hp_learning_rate prov:wasGeneratedBy :identify_hyperparameters .',\n",
    "\n",
    "    # continue with your identified hyperparameters\n",
    "\n",
    "]\n",
    "engine.insert(identify_hp_activity, prefixes=prefixes)"
   ],
   "id": "cb8b4b0eacfdfa7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def split_data(df: pd.DataFrame):\n",
    "    #do something\n",
    "    return 'train_set', 'validation_set', 'test_set'\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Documentation 4c\n",
    "#############################################\n",
    "\n",
    "### Define Train/Validation/Test splits\n",
    "split_ass_uuid_writer = \"fb58ae6c-9d58-44c9-ac7e-529111bdf7fc\"\n",
    "split_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "## Use your prepared dataset\n",
    "input_dataset = \":prepared_data\"\n",
    "\n",
    "define_split_activity = [\n",
    "    f':define_data_split rdf:type prov:Activity .',\n",
    "    f':define_data_split sc:isPartOf :modeling_phase .',\n",
    "    f':define_data_split rdfs:comment \"Train/Validation/Test Split Definition\" .',\n",
    "    f':define_data_split rdfs:comment \"\"\"{split_comment}\"\"\" .',\n",
    "    f':define_data_split prov:qualifiedAssociation :{split_ass_uuid_writer} .',\n",
    "    f':{split_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{split_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{split_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':define_data_split prov:used {input_dataset} .',\n",
    "\n",
    "    # Training Set\n",
    "    f':training_set rdf:type sc:Dataset .',\n",
    "    f':training_set rdfs:label \"Training Set\" .',\n",
    "    f':training_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':training_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':training_set rdfs:comment \"Contains xx samples\" .',\n",
    "\n",
    "    # Validation Set\n",
    "    f':validation_set rdf:type sc:Dataset .',\n",
    "    f':validation_set rdfs:label \"Validation Set\" .',\n",
    "    f':validation_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':validation_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':validation_set rdfs:comment \"Contains xx samples\" .',\n",
    "\n",
    "    # Test Set\n",
    "    f':test_set rdf:type sc:Dataset .',\n",
    "    f':test_set rdfs:label \"Test Set\" .',\n",
    "    f':test_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':test_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':test_set rdfs:comment \"Contains xx samples\" .',\n",
    "\n",
    "]\n",
    "engine.insert(define_split_activity, prefixes=prefixes)"
   ],
   "id": "3dae0e371cfc9037"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_and_finetune_model(training_set, validation_set):\n",
    "    # do something here\n",
    "\n",
    "    # Try to automate as much documentation work as possible.\n",
    "    # Define your training runs with their respective hyperparameter settings, etc.\n",
    "    # Document each time a training run, model, its hp_settings, evaluations, ...  \n",
    "    # Create performance figures/graphs\n",
    "\n",
    "    return 'Find most suitable model'\n",
    "\n",
    "\n",
    "start_time_tafm = now()\n",
    "# train_and_finetune_model()\n",
    "end_time_tafm = now()\n",
    "\n",
    "#############################################\n",
    "# Documentation 4d & e & f\n",
    "#############################################\n",
    "\n",
    "tafm_ass_uuid_writer = \"21d60fe3-c9ab-4a0a-bae7-b9fe9653c755\"\n",
    "tafm_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "# EXAMPLE output from your training\n",
    "training_run1 = \"run_1\"\n",
    "model_run1 = \"model_run1\"\n",
    "hp1_setting_run1 = \"hp_setting_run1\"\n",
    "eval_train_run1 = \"metric_train_run1\"\n",
    "eval_validation_run1 = \"metric_validation_run1\"\n",
    "\n",
    "train_model_activity = [\n",
    "    # Activity \n",
    "    f':train_and_finetune_model rdf:type prov:Activity .',\n",
    "    f':train_and_finetune_model sc:isPartOf :modeling_phase .',\n",
    "    f':train_and_finetune_model rdfs:comment \"\"\"{tafm_comment}\"\"\" .',\n",
    "    f':train_and_finetune_model prov:startedAtTime \"{start_time_tafm}\"^^xsd:dateTime .',\n",
    "    f':train_and_finetune_model prov:endedAtTime \"{end_time_tafm}\"^^xsd:dateTime .',\n",
    "    f':train_and_finetune_model prov:qualifiedAssociation :{tafm_ass_uuid_writer} .',\n",
    "    f':{tafm_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{tafm_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{tafm_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    ########################################\n",
    "    # ONE model run - automate everything below!\n",
    "\n",
    "    # Parameter settings\n",
    "    f':{hp1_setting_run1} rdf:type mls:HyperParameterSetting .',\n",
    "    f':{hp1_setting_run1} mls:specifiedBy :hp_learning_rate .',\n",
    "    f':{hp1_setting_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{hp1_setting_run1} prov:wasGeneratedBy :train_and_finetune_model .',\n",
    "    # add your further parameters\n",
    "\n",
    "    # Describe your Run\n",
    "    f':{training_run1} rdf:type mls:Run .',\n",
    "    f':{training_run1} sc:isPartOf :train_and_finetune_model .',\n",
    "    f':{training_run1} mls:realizes :random_forest_algorithm .',\n",
    "    f':{training_run1} rdf:label \"Training Run 1 with...\" .',\n",
    "    f':{training_run1} mls:executes :your_implementation .',\n",
    "    f':{training_run1} mls:hasInput :training_set .',\n",
    "    f':{training_run1} mls:hasInput :validation_set .',\n",
    "    f':{training_run1} mls:hasInput :{hp1_setting_run1} .',\n",
    "    # list all your used parameters here\n",
    "    f':{training_run1} mls:hasOutput :{model_run1} .',\n",
    "    f':{training_run1} mls:hasOutput :{eval_train_run1} .',\n",
    "    f':{training_run1} mls:hasOutput :{eval_validation_run1} .',\n",
    "\n",
    "    # Describe your Model\n",
    "    f':{model_run1} rdf:type mls:Model .',\n",
    "    f':{model_run1} prov:label \"xxx\" .',\n",
    "    f':{model_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{model_run1} mlso:trainedOn :training_set .',\n",
    "    f':{model_run1} mlso:hasAlgorithmType :random_forest_algorithm .',\n",
    "\n",
    "    # Describe your evaluations\n",
    "    # You can have multiple evaluations per model \n",
    "    f':{eval_train_run1} rdf:type mls:ModelEvaluation .',\n",
    "    f':{eval_train_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{eval_train_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{eval_train_run1} mls:specifiedBy :r2_score_measure .',\n",
    "    f':{eval_train_run1} prov:used :training_set .',\n",
    "\n",
    "    f':{eval_validation_run1} rdf:type mls:ModelEvaluation .',\n",
    "    f':{eval_validation_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{eval_validation_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{eval_validation_run1} mls:specifiedBy :r2_score_measure .',\n",
    "    f':{eval_validation_run1} prov:used :validation_set .',\n",
    "\n",
    "    # Dont forget to document any visualizations\n",
    "\n",
    "]\n",
    "engine.insert(train_model_activity, prefixes=prefixes)\n"
   ],
   "id": "cb3d77010f0162ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def retrain_model_full_data(training_set, validation_set):\n",
    "    # create your\n",
    "    return \"Final Trained Model\"\n",
    "\n",
    "\n",
    "start_time_tafm = now()\n",
    "# train_and_finetune_model()\n",
    "end_time_tafm = now()\n",
    "\n",
    "#############################################\n",
    "# Documentation 4g\n",
    "#############################################\n",
    "\n",
    "retrain_ass_uuid_writer = \"96815ee0-524c-437b-b5fa-2e15b945c993\"  # Generate once\n",
    "\n",
    "final_training_activity = \":retrain_final_model\"\n",
    "final_model = \":final_model_entity\"\n",
    "\n",
    "# Document the retraining activity.\n",
    "# Hint: This activity is still part of the :modeling_phase\n",
    "\n",
    "retrain_documentation = [\n",
    "    # your documentation here    \n",
    "]\n",
    "engine.insert(retrain_documentation, prefixes=prefixes)\n"
   ],
   "id": "6f550518282c252c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\n",
   "id": "dbea70f9aaccd0f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5e5efb7868439a03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation",
   "id": "4a51a33701c44d4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Each Activity that follows is part of the Evaluation Phase\n",
    "\n",
    "evaluation_phase_executor = [\n",
    "    f':evaluation_phase rdf:type prov:Activity .',\n",
    "    f':evaluation_phase rdfs:label \"Evaluation Phase\" .',\n",
    "]\n",
    "engine.insert(evaluation_phase_executor, prefixes=prefixes)"
   ],
   "id": "88dfe8047f20368f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "eval_code_writer = student_b\n",
    "\n",
    "\n",
    "def evaluate_on_test_data(final_model, test_set):\n",
    "    # Predict and evaluation on test data\n",
    "\n",
    "    return 'Performance'\n",
    "\n",
    "\n",
    "start_time_eval = now()\n",
    "#evaluate_on_test_data()\n",
    "end_time_eval = now()\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "eval_ass_uuid = \"7f1431e9-feed-429a-92ed-c131b23cbe79\"  # Generate once\n",
    "final_model = \":final_model_entity\"\n",
    "test_set = \":test_set\"\n",
    "\n",
    "eval_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "evaluate_activity = [\n",
    "    f':evaluate_final_model rdf:type prov:Activity .',\n",
    "    f':evaluate_final_model sc:isPartOf :evaluation_phase .',\n",
    "    f':evaluate_final_model rdfs:label \"Final Model Evaluation on Test Set\" .',\n",
    "    f':evaluate_final_model rdfs:comment \"\"\"{eval_comment}\"\"\" .',\n",
    "    f':evaluate_final_model prov:startedAtTime \"{start_time_eval}\"^^xsd:dateTime .',\n",
    "    f':evaluate_final_model prov:endedAtTime \"{end_time_eval}\"^^xsd:dateTime .',\n",
    "    f':evaluate_final_model prov:qualifiedAssociation :{eval_ass_uuid} .',\n",
    "\n",
    "    f':{eval_ass_uuid} prov:agent :{eval_code_writer} .',\n",
    "    f':{eval_ass_uuid} rdf:type prov:Association .',\n",
    "    f':{eval_ass_uuid} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Inputs\n",
    "    f':evaluate_final_model prov:used {final_model} .',\n",
    "    f':evaluate_final_model prov:used {test_set} .',\n",
    "\n",
    "    # Reference to Data Mining Success Criteria from Phase 1\n",
    "    f':evaluate_final_model prov:used :bu_data_mining_success_criteria .',\n",
    "\n",
    "    # Document you final model performance\n",
    "\n",
    "    # Hint: you evaluate bias in this way:\n",
    "    f':bias_evaluation_result rdf:type mls:ModelEvaluation .',\n",
    "    f':bias_evaluation_result prov:wasGeneratedBy :evaluate_final_model .',\n",
    "    f':bias_evaluation_result rdfs:label \"Bias Analysis\" .',\n",
    "    f':bias_evaluation_result rdfs:comment \"...\" .',\n",
    "\n",
    "]\n",
    "engine.insert(evaluate_activity, prefixes=prefixes)"
   ],
   "id": "8770607709ca9c1e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Deployment",
   "id": "68a354186625c336"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Each Activity that follows is part of the Deployment Phase\n",
    "\n",
    "deployment_phase_executor = [\n",
    "    f':deployment_phase rdf:type prov:Activity .',\n",
    "    f':deployment_phase rdfs:label \"Deployment Phase\" .',\n",
    "]\n",
    "engine.insert(deployment_phase_executor, prefixes=prefixes)"
   ],
   "id": "1c0c9377a27bfdb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "comparison_and_recommendations_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "ethical_aspects_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "monitoring_plan_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "reproducibility_reflection_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "dep_ass_uuid_executor = \"72a921e0-1234-4567-89ab-cdef01234567\"  # Generate once\n",
    "deployment_executor = [\n",
    "    f':plan_deployment rdf:type prov:Activity .',\n",
    "    f':plan_deployment sc:isPartOf :deployment_phase .',  # Connect to Parent Phase\n",
    "    f':plan_deployment rdfs:label \"Plan Deployment\"@en .',\n",
    "\n",
    "    f':plan_deployment prov:qualifiedAssociation :{dep_ass_uuid_executor} .',\n",
    "    f':{dep_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{dep_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{dep_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(deployment_executor, prefixes=prefixes)\n",
    "\n",
    "deployment_data_executor = [\n",
    "    #6a\n",
    "    f':dep_recommendations rdf:type prov:Entity .',\n",
    "    f':dep_recommendations prov:wasGeneratedBy :plan_deployment .',\n",
    "    f':dep_recommendations rdfs:label \"6a Business Objectives Reflection and Deployment Recommendations\" .',\n",
    "    f':dep_recommendations rdfs:comment \"\"\"{comparison_and_recommendations_comment}\"\"\" .',\n",
    "    #6b\n",
    "    f':dep_ethical_risks rdf:type prov:Entity .',\n",
    "    f':dep_ethical_risks prov:wasGeneratedBy :plan_deployment .',\n",
    "    f':dep_ethical_risks rdfs:label \"6b Ethical Aspects and Risks\" .',\n",
    "    f':dep_ethical_risks rdfs:comment \"\"\"{ethical_aspects_comment}\"\"\" .',\n",
    "    #6c\n",
    "    f':dep_monitoring_plan rdf:type prov:Entity .',\n",
    "    f':dep_monitoring_plan prov:wasGeneratedBy :plan_deployment .',\n",
    "    f':dep_monitoring_plan rdfs:label \"6c Monitoring Plan\" .',\n",
    "    f':dep_monitoring_plan rdfs:comment \"\"\"{monitoring_plan_comment}\"\"\" .',\n",
    "    #6d\n",
    "    f':dep_reproducibility_reflection rdf:type prov:Entity .',\n",
    "    f':dep_reproducibility_reflection prov:wasGeneratedBy :plan_deployment .',\n",
    "    f':dep_reproducibility_reflection rdfs:label \"6d Reproducibility Reflection\" .',\n",
    "    f':dep_reproducibility_reflection rdfs:comment \"\"\"{reproducibility_reflection_comment}\"\"\" .',\n",
    "\n",
    "]\n",
    "engine.insert(deployment_data_executor, prefixes=prefixes)"
   ],
   "id": "4fad059b918c647c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "905af6fc5bcffc1a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate Latex Report",
   "id": "dba99944dbf6832c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The following cells give you an example of how to automatically create a Latex Report from your provenance documentation.\n",
    "\n",
    "Feel free to use the example provided. If you use it, you should adapt and extend it with relevant sections/tables/plots/... "
   ],
   "id": "c2c9ba36a0a9010a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "base_iri = f\"https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/\"",
   "id": "cebe770e20ee1d1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# This cell includes cleaning functions\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def latex_escape(text: str | None) -> str:\n",
    "    if text is None: return \"\"\n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\\\\", r\"\\textbackslash{}\")\n",
    "    pairs = [\n",
    "        (\"&\", r\"\\&\"), (\"%\", r\"\\%\"), (\"$\", r\"\\$\"), (\"#\", r\"\\#\"),\n",
    "        (\"_\", r\"\\_\"), (\"{\", r\"\\{\"), (\"}\", r\"\\}\"),\n",
    "        (\"~\", r\"\\textasciitilde{}\"), (\"^\", r\"\\textasciicircum{}\")\n",
    "    ]\n",
    "    for k, v in pairs:\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_rdf(x) -> str:\n",
    "    if hasattr(x, \"toPython\"): return str(x.toPython())\n",
    "    if x is None: return \"\"\n",
    "    s = str(x).strip()\n",
    "    s = s.strip('\"').strip(\"'\")\n",
    "    s = s.strip()\n",
    "    if \"^^\" in s:\n",
    "        s = s.split(\"^^\")[0].strip('\"')\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def fmt_iso(ts: str) -> str:\n",
    "    if not ts: return \"\"\n",
    "    try:\n",
    "        clean_ts = ts.split(\"^^\")[0].strip('\"')\n",
    "        clean_ts = clean_ts.replace(\"Z\", \"+00:00\") if clean_ts.endswith(\"Z\") else clean_ts\n",
    "        return datetime.fromisoformat(clean_ts).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    except:\n",
    "        return latex_escape(str(ts))"
   ],
   "id": "47ccf285ff00956b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# This cell includes exemplary queries for different phases\n",
    "\n",
    "\n",
    "### Author Block\n",
    "author_query = f\"\"\"\n",
    "{prefix_header}\n",
    "PREFIX iao: <http://purl.obolibrary.org/obo/>\n",
    "\n",
    "SELECT DISTINCT ?uri ?given ?family ?matr WHERE {{\n",
    "  VALUES ?uri {{ :{student_a} :{student_b} }}\n",
    "  \n",
    "  ?uri a foaf:Person .\n",
    "  ?uri foaf:givenName ?given .\n",
    "  ?uri foaf:familyName ?family .\n",
    "  ?uri iao:IAO_0000219 ?matr .\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "res_authors = engine.query(author_query)\n",
    "author_block_latex = \"\"\n",
    "\n",
    "if not res_authors.empty:  # type:ignore\n",
    "    for _, row in res_authors.iterrows():  # type:ignore\n",
    "\n",
    "        uri_str = str(row['uri'])\n",
    "        given = latex_escape(clean_rdf(row['given']))\n",
    "        family = latex_escape(clean_rdf(row['family']))\n",
    "        matr = latex_escape(clean_rdf(row['matr']))\n",
    "        if student_a in uri_str:\n",
    "            responsibility = \"Student A\"\n",
    "        elif student_b in uri_str:\n",
    "            responsibility = \"Student B\"\n",
    "        else:\n",
    "            responsibility = \"Student\"\n",
    "\n",
    "        author_block_latex += rf\"\"\"\n",
    "          \\author{{{given} {family}}}\n",
    "          \\authornote{{{responsibility}, Matr.Nr.: {matr}}}\n",
    "          \\affiliation{{\n",
    "            \\institution{{TU Wien}}\n",
    "            \\country{{Austria}}\n",
    "          }}\n",
    "          \"\"\"\n",
    "\n",
    "### Business Understanding example\n",
    "bu_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?ds_comment ?bo_comment WHERE {{\n",
    "  OPTIONAL {{ :bu_data_source_and_scenario rdfs:comment ?ds_comment . }}\n",
    "  OPTIONAL {{ :bu_business_objectives rdfs:comment ?bo_comment . }}\n",
    "}} LIMIT 1\n",
    "\"\"\"\n",
    "res_bu = engine.query(bu_query)\n",
    "row_bu = res_bu.iloc[0] if not res_bu.empty else {}  # type:ignore\n",
    "bu_data_source = latex_escape(clean_rdf(row_bu.get(\"ds_comment\", \"\")))\n",
    "bu_objectives = latex_escape(clean_rdf(row_bu.get(\"bo_comment\", \"\")))\n",
    "\n",
    "### Data Understanding examples\n",
    "# Example Dataset Description\n",
    "du_desc_query = f\"\"\"\n",
    "{prefix_header}\n",
    "SELECT ?desc WHERE {{ :raw_data sc:description ?desc . }} LIMIT 1\n",
    "\"\"\"\n",
    "res_du_desc = engine.query(du_desc_query)\n",
    "row_du_desc = res_du_desc.iloc[0] if not res_du_desc.empty else {}  # type:ignore\n",
    "du_description = latex_escape(clean_rdf(row_du_desc.get(\"desc\", \"\")))\n",
    "\n",
    "# Dataset Description\n",
    "dd_desc_query = f\"\"\"\n",
    "{prefix_header}\n",
    "SELECT ?desc WHERE {{ :dataset_description rdfs:comment ?desc . }} LIMIT 1\n",
    "\"\"\"\n",
    "res_dd_desc = engine.query(dd_desc_query)\n",
    "row_dd_desc = res_dd_desc.iloc[0] if not res_dd_desc.empty else {}  # type:ignore\n",
    "dd_description = latex_escape(clean_rdf(row_dd_desc.get(\"desc\", \"\")))\n",
    "\n",
    "# Example Feature Columns Table\n",
    "du_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?name (SAMPLE(?dtypeRaw) as ?dtype) (SAMPLE(?descRaw) as ?desc) WHERE {{\n",
    "  :raw_data cr:recordSet ?rs .\n",
    "  ?rs cr:field ?field .\n",
    "  ?field sc:name ?name .\n",
    "  ?field sc:description ?descRaw .\n",
    "  ?field cr:dataType ?dtypeRaw .\n",
    "}} \n",
    "GROUP BY ?name\n",
    "ORDER BY ?name\n",
    "\"\"\"\n",
    "res_du = engine.query(du_query)\n",
    "du_rows = []\n",
    "if not res_du.empty:  # type:ignore\n",
    "    for _, f in res_du.iterrows():  # type:ignore\n",
    "        dtype_raw = clean_rdf(f.get(\"dtype\", \"\"))\n",
    "        if '#' in dtype_raw:\n",
    "            dtype = dtype_raw.split('#')[-1]\n",
    "        elif '/' in dtype_raw:\n",
    "            dtype = dtype_raw.split('/')[-1]\n",
    "        else:\n",
    "            dtype = dtype_raw\n",
    "\n",
    "        desc = clean_rdf(f.get(\"desc\", \"\"))\n",
    "        row_str = f\"{latex_escape(clean_rdf(f['name']))} & {latex_escape(dtype)} & {latex_escape(desc)} \\\\\\\\\"\n",
    "        du_rows.append(row_str)\n",
    "du_table_rows = \"\\n    \".join(du_rows)\n",
    "\n",
    "### Data Preparation\n",
    "dp_3outliers_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :handle_outliers rdfs:comment ?comment .\n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "res_dp3outliers = engine.query(dp_3outliers_query)\n",
    "row_dp3outliers = res_dp3outliers.iloc[0] if not res_dp3outliers.empty else {}\n",
    "dp3outliers_comment = latex_escape(clean_rdf(row_dp3outliers.get(\"comment\", \"\")))\n",
    "\n",
    "\n",
    "### Task 3b\n",
    "dp_3b_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :task3b rdfs:comment ?comment .\n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "res_dp3b = engine.query(dp_3b_query)\n",
    "row_dp3b = res_dp3b.iloc[0] if not res_dp3b.empty else {}\n",
    "dp3b_comment = latex_escape(clean_rdf(row_dp3b.get(\"comment\", \"\")))\n",
    "\n",
    "dp_3c_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :task3c rdfs:comment ?comment .\n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "res_dp3c = engine.query(dp_3c_query)\n",
    "row_dp3c = res_dp3c.iloc[0] if not res_dp3c.empty else {}\n",
    "dp3c_comment = latex_escape(clean_rdf(row_dp3c.get(\"comment\", \"\")))\n",
    "\n",
    "dp_3d_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :task3d rdfs:comment ?comment .\n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "res_dp3d = engine.query(dp_3d_query)\n",
    "row_dp3d = res_dp3d.iloc[0] if not res_dp3d.empty else {}\n",
    "dp3d_comment = latex_escape(clean_rdf(row_dp3d.get(\"comment\", \"\")))\n",
    "\n",
    "\n",
    "dp_3summary_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :prepared_data rdfs:comment ?comment .\n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "res_dp3summary = engine.query(dp_3summary_query)\n",
    "row_dp3summary = res_dp3summary.iloc[0] if not res_dp3summary.empty else {}\n",
    "dp3summary_comment = latex_escape(clean_rdf(row_dp3summary.get(\"comment\", \"\")))\n",
    "\n",
    "### Modeling example\n",
    "# Hyperparameters\n",
    "hp_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?hpName (SAMPLE(?hpValRaw) as ?hpVal) (MAX(?hpDescRaw) as ?hpDesc) WHERE {{\n",
    "  ?run sc:isPartOf :train_and_finetune_model .\n",
    "  ?run mls:hasInput ?setting .\n",
    "  ?setting a mls:HyperParameterSetting .\n",
    "  ?setting mls:hasValue ?hpValRaw .\n",
    "  ?setting mls:specifiedBy ?hpDef .\n",
    "  ?hpDef rdfs:label ?hpName .\n",
    "  OPTIONAL {{ ?hpDef rdfs:comment ?hpDescRaw . }}\n",
    "}} \n",
    "GROUP BY ?hpName\n",
    "ORDER BY ?hpName\n",
    "\"\"\"\n",
    "res_hp = engine.query(hp_query)\n",
    "hp_rows = []\n",
    "if not res_hp.empty:  #type:ignore\n",
    "    for _, row in res_hp.iterrows():  #type:ignore\n",
    "        name = latex_escape(clean_rdf(row['hpName']))\n",
    "        val = latex_escape(clean_rdf(row['hpVal']))\n",
    "        desc = latex_escape(clean_rdf(row.get('hpDesc', '')))\n",
    "        hp_rows.append(rf\"{name} & {desc} & {val} \\\\\")\n",
    "\n",
    "hp_table_rows = \"\\n    \".join(hp_rows)\n",
    "\n",
    "# Run Info\n",
    "run_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?algoLabel ?start ?end ?metricLabel ?metricVal WHERE {{\n",
    "  OPTIONAL {{ :train_and_finetune_model prov:startedAtTime ?start ; prov:endedAtTime ?end . }}\n",
    "  OPTIONAL {{\n",
    "      ?run sc:isPartOf :train_and_finetune_model .\n",
    "      ?run mls:realizes ?algo .\n",
    "      ?algo rdfs:label ?algoLabel .\n",
    "  }}\n",
    "  OPTIONAL {{\n",
    "    ?run sc:isPartOf :train_and_finetune_model .\n",
    "    ?run mls:hasOutput ?eval .\n",
    "    ?eval a mls:ModelEvaluation ; mls:hasValue ?metricVal .\n",
    "    OPTIONAL {{ ?eval mls:specifiedBy ?m . ?m rdfs:label ?metricLabel . }}\n",
    "  }}\n",
    "}} LIMIT 1\n",
    "\"\"\"\n",
    "res_run = engine.query(run_query)\n",
    "row_run = res_run.iloc[0] if not res_run.empty else {}  #type:ignore\n",
    "mod_algo = latex_escape(clean_rdf(row_run.get(\"algoLabel\", \"\")))\n",
    "mod_start = latex_escape(fmt_iso(clean_rdf(row_run.get(\"start\"))))\n",
    "mod_end = latex_escape(fmt_iso(clean_rdf(row_run.get(\"end\"))))\n",
    "mod_m_lbl = latex_escape(clean_rdf(row_run.get(\"metricLabel\", \"\")))\n",
    "raw_val = clean_rdf(row_run.get('metricVal', ''))\n",
    "mod_m_val = f\"{float(raw_val):.4f}\" if raw_val else \"\"\n",
    "\n",
    "print(\"Data extraction done.\")"
   ],
   "id": "d138af3548a81c0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The following includes the Latex report itself. It fills in the query-results from the cell before. The ACM Template is already filled. \n",
    "Make sure that you update Student A and B accordingly."
   ],
   "id": "9f38084ad9b146e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "latex_content = rf\"\"\"\\documentclass[sigconf]{{acmart}}\n",
    "\n",
    "\\AtBeginDocument{{ \\providecommand\\BibTeX{{ Bib\\TeX }} }}\n",
    "\\setcopyright{{acmlicensed}}\n",
    "\\copyrightyear{{2025}}\n",
    "\\acmYear{{2025}}\n",
    "\\acmDOI{{XXXXXXX.XXXXXXX}}\n",
    "\n",
    "\\acmConference[BI 2025]{{Business Intelligence}}{{-}}{{-}}\n",
    "\n",
    "\\begin{{document}}\n",
    "\n",
    "\\title{{BI2025 Experiment Report - Group {group_id}}}\n",
    "%% ---Authors: Dynamically added ---\n",
    "{author_block_latex}\n",
    "\n",
    "\\begin{{abstract}}\n",
    "  This report documents the machine learning experiment for Group {group_id}, following the CRISP-DM process model.\n",
    "\\end{{abstract}}\n",
    "\n",
    "\\ccsdesc[500]{{Computing methodologies~Machine learning}}\n",
    "\\keywords{{CRISP-DM, Provenance, Knowledge Graph, Machine Learning}}\n",
    "\n",
    "\\maketitle\n",
    "\n",
    "%% --- 1. Business Understanding ---\n",
    "\\section{{Business Understanding}}\n",
    "\n",
    "\\subsection{{Data Source and Scenario}}\n",
    "{bu_data_source}\n",
    "\n",
    "\\subsection{{Business Objectives}}\n",
    "{bu_objectives}\n",
    "\n",
    "%% --- 2. Data Understanding ---\n",
    "\\section{{Data Understanding}}\n",
    "\\textbf{{Dataset Description:}} {du_description}\n",
    "\n",
    "The following features were identified in the dataset:\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Raw Data Features}}\n",
    "  \\label{{tab:features}}\n",
    "  \\begin{{tabular}}{{lp{{0.2\\linewidth}}p{{0.4\\linewidth}}}}\n",
    "    \\toprule\n",
    "    \\textbf{{Feature Name}} & \\textbf{{Data Type}} & \\textbf{{Description}} \\\\\n",
    "    \\midrule\n",
    "    {du_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\\\\\n",
    "\n",
    "\\textbf{{Statistical properties:}} {dd_description}\n",
    "\n",
    "\n",
    "%% --- 3. Data Preparation ---\n",
    "\\section{{Data Preparation}}\n",
    "\\subsection{{Handling outliers}}\n",
    "{dp3outliers_comment}\n",
    "\n",
    "\\subsection{{Considered but Not Applied Pre-processing Steps}}\n",
    "{dp3b_comment}\n",
    "\\subsection{{Analysis of Potential Derived Attributes}}\n",
    "{dp3c_comment}\n",
    "\\subsection{{Analysis of Potential External Data Sources}}\n",
    "{dp3d_comment}\n",
    "\\subsection{{Outcome of the Data Preparation Phase}}\n",
    "{dp3summary_comment}\n",
    "\n",
    "\n",
    "%% --- 4. Modeling ---\n",
    "\\section{{Modeling}}\n",
    "\n",
    "\\subsection{{Hyperparameter Configuration}}\n",
    "The model was trained using the following hyperparameter settings:\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Hyperparameter Settings}}\n",
    "  \\label{{tab:hyperparams}}\n",
    "  \\begin{{tabular}}{{lp{{0.4\\linewidth}}l}}\n",
    "    \\toprule\n",
    "    \\textbf{{Parameter}} & \\textbf{{Description}} & \\textbf{{Value}} \\\\\n",
    "    \\midrule\n",
    "    {hp_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "\\subsection{{Training Run}}\n",
    "A training run was executed with the following characteristics:\n",
    "\\begin{{itemize}}\n",
    "    \\item \\textbf{{Algorithm:}} {mod_algo}\n",
    "    \\item \\textbf{{Start Time:}} {mod_start}\n",
    "    \\item \\textbf{{End Time:}} {mod_end}\n",
    "    \\item \\textbf{{Result:}} {mod_m_lbl} = {mod_m_val}\n",
    "\\end{{itemize}}\n",
    "\n",
    "%% --- 5. Evaluation ---\n",
    "\\section{{Evaluation}}\n",
    "\n",
    "%% --- 6. Deployment ---\n",
    "\\section{{Deployment}}\n",
    "\n",
    "\\section{{Conclusion}}\n",
    "\n",
    "\\end{{document}}\n",
    "\"\"\""
   ],
   "id": "5863469e2c245979"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# This cell stores the Latex report to the data/report directory\n",
    "\n",
    "out_dir = os.path.join(\"data\", \"report\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, \"experiment_report.tex\")\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_content)\n",
    "\n",
    "print(f\"Report written to: {out_path}\")"
   ],
   "id": "42991707ce5f5e55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9bf0fad96ed3ea94"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
