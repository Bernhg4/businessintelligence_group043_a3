{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f7d15c",
   "metadata": {},
   "source": [
    "First, create a new conda environment named BI2025 and install the required packages from requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2329db9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T20:09:09.274578Z",
     "start_time": "2025-12-11T20:08:10.540007Z"
    }
   },
   "outputs": [],
   "source": [
    "!conda create -n BI2025 python=3.11 -y\n",
    "!conda activate BI2025\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5122654",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T20:09:11.203533Z",
     "start_time": "2025-12-11T20:09:11.194976Z"
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY OR COPY THIS CELL!! \n",
    "# Note: The only imports allowed are Python's standard library, pandas, numpy, scipy, matplotlib, seaborn and scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "import typing\n",
    "import requests\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "from starvers.starvers import TripleStoreEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79408d3",
   "metadata": {},
   "source": [
    "## Graph-based documentation preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831a95c",
   "metadata": {},
   "source": [
    "**!!!IMPORTANT!!!**\n",
    "\n",
    "Everytime you work on this notebook, enter your student ID in the `executed_by` variable so that the cell executions are accredited to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a02423",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T20:09:14.914142Z",
     "start_time": "2025-12-11T20:09:14.908777Z"
    }
   },
   "outputs": [],
   "source": [
    "#executed_by = 'stud-id_12019873'  # Replace the digits after \"id_\" with your own student ID\n",
    "executed_by = 'stud-id_12120509'  # Replace the digits after \"id_\" with your own student ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2160a7",
   "metadata": {},
   "source": [
    "Set your group and student IDs. Do this only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16721334",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T20:09:18.763403Z",
     "start_time": "2025-12-11T20:09:18.759462Z"
    }
   },
   "outputs": [],
   "source": [
    "# group id for this project\n",
    "group_id = '43'  # Replace the digits with your group id\n",
    "\n",
    "# Students working on this notebook\n",
    "student_a = 'stud-id_12019873'  # Replace the digits after \"id_\" with student A's student ID\n",
    "student_b = 'stud-id_12120509'  # Replace the digits after \"id_\" with student B's student ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb927186",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T20:09:20.191066Z",
     "start_time": "2025-12-11T20:09:20.186159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Roles. Don't change these values.\n",
    "code_writer_role = 'code_writer'\n",
    "code_executor_role = 'code_executor'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e253f6",
   "metadata": {},
   "source": [
    "Setup the starvers API for logging your steps into our server-sided graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4195fdc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T20:09:22.132685Z",
     "start_time": "2025-12-11T20:09:22.127767Z"
    }
   },
   "outputs": [],
   "source": [
    "get_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025\"\n",
    "post_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025/statements\"\n",
    "engine = TripleStoreEngine(get_endpoint, post_endpoint, skip_connection_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043cee91",
   "metadata": {},
   "source": [
    "Use these prefixes in your notebooks. You can extend this dict with your prefixes of additional ontologies that you use in this notebook. Replace 00 with your group id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68e6f5c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T20:09:23.635172Z",
     "start_time": "2025-12-11T20:09:23.629710Z"
    }
   },
   "outputs": [],
   "source": [
    "prefixes = {\n",
    "    'xsd': 'http://www.w3.org/2001/XMLSchema#',\n",
    "    'rdfs': 'http://www.w3.org/2000/01/rdf-schema#',\n",
    "    'foaf': 'http://xmlns.com/foaf/0.1/',\n",
    "    'prov': 'http://www.w3.org/ns/prov#',\n",
    "    'sc': 'https://schema.org/',\n",
    "    'cr': 'http://mlcommons.org/croissant/',\n",
    "    'mls': 'http://www.w3.org/ns/mls#',\n",
    "    'mlso': 'http://w3id.org/mlso',\n",
    "    'siu': 'https://si-digital-framework.org/SI/units/',\n",
    "    'siq': 'https://si-digital-framework.org/SI/quantities/',\n",
    "    'qudt': 'http://qudt.org/schema/qudt/',\n",
    "    '': f'https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/',\n",
    "}\n",
    "\n",
    "prefix_header = '\\n'.join([f'PREFIX {k}: <{v}>' for k, v in prefixes.items()]) + '\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970468d",
   "metadata": {},
   "source": [
    "Ontologies to use\n",
    "* Provenance of the experiment process\n",
    "    * PROV-O: \n",
    "        * doc: https://www.w3.org/TR/prov-o/\n",
    "        * serialization: https://www.w3.org/ns/prov-o\n",
    "* Data used and created\n",
    "    * schema.org - Dataset: \n",
    "        * doc: https://schema.org/Dataset\n",
    "        * serialization: https://schema.org/version/latest/schemaorg-current-https.ttl\n",
    "    * Crossaint\n",
    "        * doc: https://docs.mlcommons.org/croissant/docs/croissant-spec.html\n",
    "        * serialization: https://github.com/mlcommons/croissant/blob/main/docs/croissant.ttl\n",
    "* ML experiments performed\n",
    "    * MLSO: \n",
    "        * doc: https://github.com/dtai-kg/MLSO\n",
    "        * doc: https://dtai-kg.github.io/MLSO/#http://w3id.org/\n",
    "        * serialization: https://dtai-kg.github.io/MLSO/ontology.ttl\n",
    "* Measurements, Metrics, Units\n",
    "    * QUDT\n",
    "        * doc:https://qudt.org/\n",
    "        * doc: https://github.com/qudt/qudt-public-repo\n",
    "        * serialization: https://github.com/qudt/qudt-public-repo/blob/main/src/main/rdf/schema/SCHEMA_QUDT.ttl\n",
    "    * SI Digital Framework\n",
    "        * doc: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/docs/README.md\n",
    "        * doc: https://si-digital-framework.org/\n",
    "        * doc: https://si-digital-framework.org/SI\n",
    "        * serialization: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/TTL/si.ttl\n",
    "    * Quantities and Units\n",
    "        * doc: https://www.omg.org/spec/Commons\n",
    "        * serialization: https://www.omg.org/spec/Commons/QuantitiesAndUnits.ttl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a62393d",
   "metadata": {},
   "source": [
    "Use this function to record execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f08ce56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T20:09:27.070748Z",
     "start_time": "2025-12-11T20:09:27.063735Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def now() -> str:\n",
    "    \"\"\"\n",
    "    Returns the current time in ISO 8601 format with UTC timezone in the following format:\n",
    "    YYYY-MM-DDTHH:MM:SS.sssZ\n",
    "    \"\"\"\n",
    "    time.tzname = ('Europe/Vienna', 'Europe/Vienna')\n",
    "    timestamp = datetime.datetime.now(datetime.timezone.utc)\n",
    "    timestamp_formated = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3] + \"Z\"\n",
    "\n",
    "    return timestamp_formated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a1605",
   "metadata": {},
   "source": [
    "Register yourself in the Knowledge Graph using ProvO. Change the given name, family name and immatriculation number to reflect your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4080a558",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T20:09:29.833739Z",
     "start_time": "2025-12-11T20:09:28.767417Z"
    }
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 504: Gateway Time-out",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     13\u001b[39m reigstration_triples_b = [\n\u001b[32m     14\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudent_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rdf:type foaf:Person .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     15\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudent_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rdf:type prov:Agent .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudent_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m <http://purl.obolibrary.org/obo/IAO_0000219> \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m76543210\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m^^xsd:string .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     22\u001b[39m ]\n\u001b[32m     24\u001b[39m role_triples = [\n\u001b[32m     25\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode_writer_role\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rdf:type prov:Role .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     26\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode_executor_role\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rdf:type prov:Role .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     27\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreigstration_triples_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefixes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m engine.insert(reigstration_triples_b, prefixes=prefixes)\n\u001b[32m     31\u001b[39m engine.insert(role_triples, prefixes=prefixes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\BI2025\\Lib\\site-packages\\starvers\\starvers.py:512\u001b[39m, in \u001b[36mTripleStoreEngine.insert\u001b[39m\u001b[34m(self, triples, prefixes, timestamp, chunk_size)\u001b[39m\n\u001b[32m    510\u001b[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001b[33m\"\u001b[39m\u001b[33mNOW()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    511\u001b[39m     \u001b[38;5;28mself\u001b[39m.sparql_post.setQuery(insert_statement)\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparql_post\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mTriples inserted.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\BI2025\\Lib\\site-packages\\SPARQLWrapper\\Wrapper.py:960\u001b[39m, in \u001b[36mSPARQLWrapper.query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mQueryResult\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    943\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[33;03m    Execute the query.\u001b[39;00m\n\u001b[32m    945\u001b[39m \u001b[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m \u001b[33;03m    :rtype: :class:`QueryResult` instance\u001b[39;00m\n\u001b[32m    959\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\BI2025\\Lib\\site-packages\\SPARQLWrapper\\Wrapper.py:940\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EndPointInternalError(e.read())\n\u001b[32m    939\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\BI2025\\Lib\\site-packages\\SPARQLWrapper\\Wrapper.py:926\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    924\u001b[39m         response = urlopener(request, timeout=\u001b[38;5;28mself\u001b[39m.timeout)\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         response = \u001b[43murlopener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m.returnFormat\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m urllib.error.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:216\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    215\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:525\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_response.get(protocol, []):\n\u001b[32m    524\u001b[39m     meth = \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m     response = \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:634\u001b[39m, in \u001b[36mHTTPErrorProcessor.http_response\u001b[39m\u001b[34m(self, request, response)\u001b[39m\n\u001b[32m    631\u001b[39m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m200\u001b[39m <= code < \u001b[32m300\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:563\u001b[39m, in \u001b[36mOpenerDirector.error\u001b[39m\u001b[34m(self, proto, *args)\u001b[39m\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[32m    562\u001b[39m     args = (\u001b[38;5;28mdict\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhttp_error_default\u001b[39m\u001b[33m'\u001b[39m) + orig_args\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:496\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    495\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    498\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:643\u001b[39m, in \u001b[36mHTTPDefaultErrorHandler.http_error_default\u001b[39m\u001b[34m(self, req, fp, code, msg, hdrs)\u001b[39m\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001b[31mHTTPError\u001b[39m: HTTP Error 504: Gateway Time-out"
     ]
    }
   ],
   "source": [
    "# Ontologies used: foaf, prov, IAO\n",
    "reigstration_triples_a = [\n",
    "    f':{student_a} rdf:type foaf:Person .',\n",
    "    f':{student_a} rdf:type prov:Agent .',\n",
    "    f':{student_a} foaf:givenName \"Johanna\" .',\n",
    "    f':{student_a} foaf:familyName \"Six\" .',\n",
    "    f':{student_a} <http://vivoweb.org/ontology/core#identifier> :{student_a} .',\n",
    "    f':{student_a} rdf:type <http://purl.obolibrary.org/obo/IAO_0000578> .',\n",
    "    f':{student_a} <http://www.w3.org/2000/01/rdf-schema#label> \"Immatriculation number\" .',\n",
    "    f':{student_a} <http://purl.obolibrary.org/obo/IAO_0000219> \"12019873\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "reigstration_triples_b = [\n",
    "    f':{student_b} rdf:type foaf:Person .',\n",
    "    f':{student_b} rdf:type prov:Agent .',\n",
    "    f':{student_b} foaf:givenName \"Bernhard\" .',\n",
    "    f':{student_b} foaf:familyName \"Siegl\" .',\n",
    "    f':{student_b} <http://vivoweb.org/ontology/core#identifier> :{student_b} .',\n",
    "    f':{student_b} rdf:type <http://purl.obolibrary.org/obo/IAO_0000578> .',\n",
    "    f':{student_b} <http://www.w3.org/2000/01/rdf-schema#label> \"Immatriculation number\" .',\n",
    "    f':{student_b} <http://purl.obolibrary.org/obo/IAO_0000219> \"76543210\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "role_triples = [\n",
    "    f':{code_writer_role} rdf:type prov:Role .',\n",
    "    f':{code_executor_role} rdf:type prov:Role .',\n",
    "]\n",
    "\n",
    "engine.insert(reigstration_triples_a, prefixes=prefixes)\n",
    "engine.insert(reigstration_triples_b, prefixes=prefixes)\n",
    "engine.insert(role_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c479ed4",
   "metadata": {},
   "source": [
    "**What not do do**\n",
    "\n",
    "Do not use [blank nodes](https://www.w3.org/wiki/BlankNodes).\n",
    "\n",
    "PROV-O uses blank nodes to connect multiple elements with each other.\n",
    "Such blank nodes (such as _:association) should not be used.\n",
    "Instead, assign a fixed node ID such as\n",
    ":5119fcd7-b571-41e0-9464-a37c7be0f574 by generating them outside of the\n",
    "notebook.\n",
    "We suggest that, for each setting where such a blank node is needed to\n",
    "connect multiple elements, you create a unique hash (using uuid.uuid4())\n",
    "and keep this as hard-coded identifier for the blank node. The template\n",
    "notebook contains examples of this. Do *not* use these provided values,\n",
    "as otherwise, your provenance documentations will all be connected via\n",
    "these identifiers!\n",
    "Also, do not generate them dynamically in every cell execution, e.g. by\n",
    "using uuid.uuid4() in a cell. This would generate many new linking nodes\n",
    "for connecting the same elements.\n",
    "Compute one for each node (cell) where you need them and make sure to\n",
    "use the same one on each re-execution of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a782d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T20:09:32.069739Z",
     "start_time": "2025-12-11T20:09:32.063839Z"
    }
   },
   "outputs": [],
   "source": [
    "airline_data_path = os.path.join(\"data\", \"airline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee069d",
   "metadata": {},
   "source": [
    "## Business Understanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee88389",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Business Understanding Phase\n",
    "\n",
    "business_understanding_phase_executor = [\n",
    "    f':business_understanding_phase rdf:type prov:Activity .',\n",
    "    f':business_understanding_phase rdfs:label \"Business Understanding Phase\" .',  ## Phase 1: Business Understanding\n",
    "]\n",
    "engine.insert(business_understanding_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dc8a3a-708a-4992-a076-038c53338e89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:10:40.843285Z",
     "start_time": "2025-12-11T19:10:40.149041Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9bd9643d1e26a8dc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "data_src_and_scenario_comment = \"\"\"\n",
    "The data source consists of 21 attributes and about 3000 rows. It contains data from December 2019 and 2020, that consists of the number of delayed, cancelled or diverted flights and the cause of delays. The data can be used by airlines to identify what causes the most flight delays / cancellations.\n",
    "\"\"\"\n",
    "\n",
    "business_objectives_comment = \"\"\"\n",
    "The airline wants to improve punctuality and therefore reduce the number of delayed, cancelled flights.\n",
    "\"\"\"\n",
    "\n",
    "business_success_criteria_comment = \"\"\"\n",
    "The success criteria are: improvement of on time performance. Delayed flights are reduced by at least 5 percent in the next year.\n",
    "Top 10 airports are identified with the most delays, cancelled flights and monitored.\n",
    "\"\"\"\n",
    "\n",
    "data_mining_goals_comment = \"\"\"\n",
    "We need to identify the main drivers of delays and cancellations by analyzing delay causes, airport, airline and temporal factors;\n",
    "Quantify the distribution of delay categories (weather, carrier, security, NAS, ..);\n",
    "Identify airports and airlines with significant higher delays/cancellations;\n",
    "Build models to estimate the likelihood of delays for a given airport, airline and time.\n",
    "\"\"\"\n",
    "\n",
    "data_mining_success_criteria_comment = \"\"\"\n",
    "Clustering results to show interpretable groupings of airports based on their delay and cancellation behaviour,\n",
    "Achieve a accuraccy of at least 70% with our classification model\n",
    "\"\"\"\n",
    "\n",
    "ai_risk_aspects_comment = \"\"\"\n",
    "The system should be low-risk, as it supports operational analysis and decision-making rather than automated enforcement or individual-level decisions\n",
    "\"\"\"\n",
    "\n",
    "bu_ass_uuid_executor = \"bb6a40f9-9d92-4f9f-bbd2-b65ef6a82da2\"  # Generate once\n",
    "business_understanding_executor = [\n",
    "    f':business_understanding rdf:type prov:Activity .',\n",
    "    f':business_understanding sc:isPartOf :business_understanding_phase .',\n",
    "    # Connect Activity to Parent Business Understanding Phase Activity\n",
    "    f':business_understanding prov:qualifiedAssociation :{bu_ass_uuid_executor} .',\n",
    "    f':{bu_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{bu_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{bu_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(business_understanding_executor, prefixes=prefixes)\n",
    "\n",
    "business_understanding_data_executor = [\n",
    "    # 1a\n",
    "    f':bu_data_source_and_scenario rdf:type prov:Entity .',\n",
    "    f':bu_data_source_and_scenario prov:wasGeneratedBy :business_understanding .',\n",
    "    f':bu_data_source_and_scenario rdfs:label \"1a Data Source and Scenario\" .',\n",
    "    f':bu_data_source_and_scenario rdfs:comment \"\"\"{data_src_and_scenario_comment}\"\"\" .',\n",
    "    # 1b\n",
    "    f':bu_business_objectives rdf:type prov:Entity .',\n",
    "    f':bu_business_objectives prov:wasGeneratedBy :business_understanding .',\n",
    "    f':bu_business_objectives rdfs:label \"1b Business Objectives\" .',\n",
    "    f':bu_business_objectives rdfs:comment \"\"\"{business_objectives_comment}\"\"\" .',\n",
    "    # 1c\n",
    "    f':bu_business_success_criteria rdf:type prov:Entity .',\n",
    "    f':bu_business_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "    f':bu_business_success_criteria rdfs:label \"1c Business Success Criteria\" .',\n",
    "    f':bu_business_success_criteria rdfs:comment \"\"\"{business_success_criteria_comment}\"\"\" .',\n",
    "    # 1d\n",
    "    f':bu_data_mining_goals rdf:type prov:Entity .',\n",
    "    f':bu_data_mining_goals prov:wasGeneratedBy :business_understanding .',\n",
    "    f':bu_data_mining_goals rdfs:label \"1d Data Mining Goals\" .',\n",
    "    f':bu_data_mining_goals rdfs:comment \"\"\"{data_mining_goals_comment}\"\"\" .',\n",
    "    # 1e\n",
    "    f':bu_data_mining_success_criteria rdf:type prov:Entity .',\n",
    "    f':bu_data_mining_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "    f':bu_data_mining_success_criteria rdfs:label \"1e Data Mining Success Criteria\" .',\n",
    "    f':bu_data_mining_success_criteria rdfs:comment \"\"\"{data_mining_success_criteria_comment}\"\"\" .',\n",
    "    # 1f\n",
    "    f':bu_ai_risk_aspects rdf:type prov:Entity .',\n",
    "    f':bu_ai_risk_aspects prov:wasGeneratedBy :business_understanding .',\n",
    "    f':bu_ai_risk_aspects rdfs:label \"1f AI risk aspects\" .',\n",
    "    f':bu_ai_risk_aspects rdfs:comment \"\"\"{ai_risk_aspects_comment}\"\"\" .',\n",
    "\n",
    "]\n",
    "engine.insert(business_understanding_data_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae9b28",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce717fb",
   "metadata": {},
   "source": [
    "The following pseudo-code & pseudo-documentation may be used as a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449cc32a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T20:09:35.557509Z",
     "start_time": "2025-12-11T20:09:35.189767Z"
    }
   },
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Data Understanding Phase\n",
    "\n",
    "business_understanding_phase_executor = [\n",
    "    f':data_understanding_phase rdf:type prov:Activity .',\n",
    "    f':data_understanding_phase rdfs:label \"Data Understanding Phase\" .',\n",
    "]\n",
    "engine.insert(business_understanding_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247a9de3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T20:09:37.706671Z",
     "start_time": "2025-12-11T20:09:36.572460Z"
    }
   },
   "outputs": [],
   "source": [
    "airline_data_path = os.path.join(\"data\", \"airline\")\n",
    "load_airline_data_code_writer = student_a\n",
    "\n",
    "\n",
    "def load_airline_data() -> pd.DataFrame:\n",
    "    ### Load your data\n",
    "    input_file = os.path.join(airline_data_path, 'airline_delay.csv')\n",
    "    raw_data = pd.read_csv(input_file, sep=',', header=0)\n",
    "\n",
    "    raw_data.sort_values(['year', 'month'], inplace=True)\n",
    "    raw_data.set_index(['year', 'month', 'carrier', 'airport'], inplace=True)\n",
    "\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "start_time_ld = now()\n",
    "data = load_airline_data()\n",
    "end_time_ld = now()\n",
    "\n",
    "display(data.head())\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "# Now document the raw data and the loaded data using appropriate ontologies.\n",
    "\n",
    "# Always add these triples for every activity to define the executor!\n",
    "ld_ass_uuid_executor = \"b8bac193-c4e6-4e31-9134-b23e001e279c\"  # Generate once\n",
    "load_airline_data_executor = [\n",
    "    f':load_airline_data prov:qualifiedAssociation :{ld_ass_uuid_executor} .',\n",
    "    f':{ld_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ld_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ld_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(load_airline_data_executor, prefixes=prefixes)\n",
    "\n",
    "ld_ass_uuid_writer = \"c600e15c-87a9-4e2a-be85-b6c2a3014210\"  # Generate once\n",
    "ld_report = \"\"\"\n",
    "Load all airline data and create a hierarchical index (year, month).\n",
    "\"\"\"\n",
    "load_airline_data_activity = [\n",
    "    ':load_airline_data rdf:type prov:Activity .',\n",
    "    ':load_airline_data sc:isPartOf :data_understanding_phase .',\n",
    "    ':load_airline_data rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':load_airline_data rdfs:comment \"\"\"{ld_report}\"\"\" .',\n",
    "    f':load_airline_data prov:startedAtTime \"{start_time_ld}\"^^xsd:dateTime .',\n",
    "    f':load_airline_data prov:endedAtTime \"{end_time_ld}\"^^xsd:dateTime .',\n",
    "    f':load_airline_data prov:qualifiedAssociation :{ld_ass_uuid_writer} .',\n",
    "    f':{ld_ass_uuid_writer} prov:agent :{load_airline_data_code_writer} .',\n",
    "    f':{ld_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{ld_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    # INPUT of activity\n",
    "    ':load_airline_data prov:used :raw_data .',\n",
    "    ':load_airline_data prov:used :raw_data_path .',\n",
    "    ':raw_data rdf:type prov:Entity .',\n",
    "    ':raw_data_path rdf:type prov:Entity .',\n",
    "    ':raw_data prov:wasDerivedFrom :raw_data_path .',\n",
    "    # OUTPUT of activity\n",
    "    ':data rdf:type prov:Entity .',\n",
    "    ':data prov:wasGeneratedBy :load_airline_data .',\n",
    "    ':data prov:wasDerivedFrom :raw_data .',\n",
    "]\n",
    "engine.insert(load_airline_data_activity, prefixes=prefixes)\n",
    "\n",
    "# Further descibe the raw data using Croissant\n",
    "raw_data_triples = [\n",
    "    ':raw_data rdf:type sc:Dataset .',\n",
    "    ':raw_data sc:name \\'Airline Delay Dataset (Dec 2019–2020)\\' .',\n",
    "    ':raw_data sc:description \\'Summary airline delay counts per carrier per US city for December 2019 and 2020.\\' .',\n",
    "\n",
    "    # Continue with futher information about the dataset...\n",
    "    ':airline_delay_csv rdf:type cr:FileObject .',\n",
    "    ':airline_delay_csv sc:name \\'airline_delay.csv\\' .',\n",
    "    ':airline_delay_csv sc:encodingFormat \\'text/csv\\' .',\n",
    "    ':raw_data sc:distribution :airline_delay .',\n",
    "    # Continue with further information about the distribution...\n",
    "    ':raw_recordset rdf:type cr:RecordSet .',\n",
    "    ':raw_recordset sc:name \\'Table of airline data for december for 2019 and 2020\\' .',\n",
    "    ':raw_recordset cr:source :airline_delay_csv .',\n",
    "    ':raw_data cr:recordSet :raw_recordset .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_year .',\n",
    "    ':field_year rdf:type cr:Field .',\n",
    "    ':field_year sc:name \\'year\\' .',\n",
    "    ':field_year sc:description \\'Year in which flight data was collected.\\' .',\n",
    "    ':field_year cr:dataType xsd:gYear .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_month .',\n",
    "    ':field_month rdf:type cr:Field .',\n",
    "    ':field_month sc:name \\'month\\' .',\n",
    "    ':field_month sc:description \\'Month (1–12) of data collection.\\' .',\n",
    "    ':field_month cr:dataType xsd:integer .',\n",
    "    ':field_month qudt:unit qudt: qudt:DateTimeStringEncodingType  .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_carrier .',\n",
    "    ':field_carrier rdf:type cr:Field .',\n",
    "    ':field_carrier sc:name \\'carrier\\' .',\n",
    "    ':field_carrier sc:description \\'Two-letter airline carrier code.\\' .',\n",
    "    ':field_carrier cr:dataType xsd:string .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_carrier_name .',\n",
    "    ':field_carrier_name rdf:type cr:Field .',\n",
    "    ':field_carrier_name sc:name \\'carrier_name\\' .',\n",
    "    ':field_carrier_name sc:description \\'Full airline carrier name.\\' .',\n",
    "    ':field_carrier_name cr:dataType xsd:string .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_airport .',\n",
    "    ':field_airport rdf:type cr:Field .',\n",
    "    ':field_airport sc:name \\'airport\\' .',\n",
    "    ':field_airport sc:description \\'Three-letter airport code for arrival airport.\\' .',\n",
    "    ':field_airport cr:dataType xsd:string .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_airport_name .',\n",
    "    ':field_airport_name rdf:type cr:Field .',\n",
    "    ':field_airport_name sc:name \\'airport_name\\' .',\n",
    "    ':field_airport_name sc:description \\'Full name of the airport.\\' .',\n",
    "    ':field_airport_name cr:dataType xsd:string .',\n",
    "\n",
    "    # Count fields\n",
    "    ':raw_recordset cr:field :field_arr_flights .',\n",
    "    ':field_arr_flights rdf:type cr:Field .',\n",
    "    ':field_arr_flights sc:name \\'arr_flights\\' .',\n",
    "    ':field_arr_flights sc:description \\'Number of flights arriving at airport.\\' .',\n",
    "    ':field_arr_flights cr:dataType xsd:integer .',\n",
    "    ':field_arr_flights qudt:unit qudt:CountingUnit .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_arr_del15 .',\n",
    "    ':field_arr_del15 rdf:type cr:Field .',\n",
    "    ':field_arr_del15 sc:name \\'arr_del15\\' .',\n",
    "    ':field_arr_del15 sc:description \\'Flights arriving more than 15 minutes late.\\' .',\n",
    "    ':field_arr_del15 cr:dataType xsd:integer .',\n",
    "    ':field_arr_del15 qudt:unit qudt:CountingUnit .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_carrier_ct .',\n",
    "    ':field_carrier_ct rdf:type cr:Field .',\n",
    "    ':field_carrier_ct sc:name \\'carrier_ct\\' .',\n",
    "    ':field_carrier_ct sc:description \\'Flights delayed due to air carrier (e.g. crew).\\' .',\n",
    "    ':field_carrier_ct cr:dataType xsd:integer .',\n",
    "    ':field_carrier_ct qudt:unit qudt:CountingUnit .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_weather_ct .',\n",
    "    ':field_weather_ct rdf:type cr:Field .',\n",
    "    ':field_weather_ct sc:name \\'weather_ct\\' .',\n",
    "    ':field_weather_ct sc:description \\'Flights delayed due to weather.\\' .',\n",
    "    ':field_weather_ct cr:dataType xsd:integer .',\n",
    "    ':field_weather_ct qudt:unit qudt:CountingUnit .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_nas_ct .',\n",
    "    ':field_nas_ct rdf:type cr:Field .',\n",
    "    ':field_nas_ct sc:name \\'nas_ct\\' .',\n",
    "    ':field_nas_ct sc:description \\'Flights delayed due to National Aviation System.\\' .',\n",
    "    ':field_nas_ct cr:dataType xsd:integer .',\n",
    "    ':field_nas_ct qudt:unit qudt:CountingUnit .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_security_ct .',\n",
    "    ':field_security_ct rdf:type cr:Field .',\n",
    "    ':field_security_ct sc:name \\'security_ct\\' .',\n",
    "    ':field_security_ct sc:description \\'Flights canceled due to security issues.\\' .',\n",
    "    ':field_security_ct cr:dataType xsd:integer .',\n",
    "    ':field_security_ct qudt:unit qudt:CountingUnit .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_late_aircraft_ct .',\n",
    "    ':field_late_aircraft_ct rdf:type cr:Field .',\n",
    "    ':field_late_aircraft_ct sc:name \\'late_aircraft_ct\\' .',\n",
    "    ':field_late_aircraft_ct sc:description \\'Flights delayed due to a previous late aircraft.\\' .',\n",
    "    ':field_late_aircraft_ct cr:dataType xsd:integer .',\n",
    "    ':field_late_aircraft_ct qudt:unit qudt:CountingUnit .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_arr_cancelled .',\n",
    "    ':field_arr_cancelled rdf:type cr:Field .',\n",
    "    ':field_arr_cancelled sc:name \\'arr_cancelled\\' .',\n",
    "    ':field_arr_cancelled sc:description \\'Number of cancelled flights.\\' .',\n",
    "    ':field_arr_cancelled cr:dataType xsd:integer .',\n",
    "    ':field_arr_cancelled qudt:unit qudt:CountingUnit .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_arr_diverted .',\n",
    "    ':field_arr_diverted rdf:type cr:Field .',\n",
    "    ':field_arr_diverted sc:name \\'arr_diverted\\' .',\n",
    "    ':field_arr_diverted sc:description \\'Number of diverted flights.\\' .',\n",
    "    ':field_arr_diverted cr:dataType xsd:integer .',\n",
    "    ':field_arr_diverted qudt:unit qudt:CountingUnit .',\n",
    "\n",
    "    # Delay time (minutes)\n",
    "    ':raw_recordset cr:field :field_arr_delay .',\n",
    "    ':field_arr_delay rdf:type cr:Field .',\n",
    "    ':field_arr_delay sc:name \\'arr_delay\\' .',\n",
    "    ':field_arr_delay sc:description \\'Total delay time in minutes.\\' .',\n",
    "    ':field_arr_delay cr:dataType xsd:double .',\n",
    "    ':field_arr_delay qudt:unit qudt:Minute .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_carrier_delay .',\n",
    "    ':field_carrier_delay rdf:type cr:Field .',\n",
    "    ':field_carrier_delay sc:name \\'carrier_delay\\' .',\n",
    "    ':field_carrier_delay sc:description \\'Delay minutes due to air carrier issues.\\' .',\n",
    "    ':field_carrier_delay cr:dataType xsd:double .',\n",
    "    ':field_carrier_delay qudt:unit qudt:Minute .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_weather_delay .',\n",
    "    ':field_weather_delay rdf:type cr:Field .',\n",
    "    ':field_weather_delay sc:name \\'weather_delay\\' .',\n",
    "    ':field_weather_delay sc:description \\'Delay minutes due to weather.\\' .',\n",
    "    ':field_weather_delay cr:dataType xsd:double .',\n",
    "    ':field_weather_delay qudt:unit qudt:Minute .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_nas_delay .',\n",
    "    ':field_nas_delay rdf:type cr:Field .',\n",
    "    ':field_nas_delay sc:name \\'nas_delay\\' .',\n",
    "    ':field_nas_delay sc:description \\'Delay minutes due to National Aviation System.\\' .',\n",
    "    ':field_nas_delay cr:dataType xsd:double .',\n",
    "    ':field_nas_delay qudt:unit qudt:Minute .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_security_delay .',\n",
    "    ':field_security_delay rdf:type cr:Field .',\n",
    "    ':field_security_delay sc:name \\'security_delay\\' .',\n",
    "    ':field_security_delay sc:description \\'Delay minutes due to security issues.\\' .',\n",
    "    ':field_security_delay cr:dataType xsd:double .',\n",
    "    ':field_security_delay qudt:unit qudt:Minute .',\n",
    "\n",
    "    ':raw_recordset cr:field :field_late_aircraft_delay .',\n",
    "    ':field_late_aircraft_delay rdf:type cr:Field .',\n",
    "    ':field_late_aircraft_delay sc:name \\'late_aircraft_delay\\' .',\n",
    "    ':field_late_aircraft_delay sc:description \\'Delay minutes due to previous late aircraft.\\' .',\n",
    "    ':field_late_aircraft_delay cr:dataType xsd:double .',\n",
    "    ':field_late_aircraft_delay qudt:unit qudt:Minute .',\n",
    "]\n",
    "engine.insert(raw_data_triples, prefixes=prefixes)\n",
    "\n",
    "# Also the output of the load activity is a dataset that can be described with Croissant\n",
    "data_triples = [\n",
    "    ':data rdf:type sc:Dataset .',\n",
    "    # ...\n",
    "    ':recordset rdf:type cr:RecordSet .',\n",
    "    # ...\n",
    "    ':data cr:recordSet :recordset .',\n",
    "    # ...\n",
    "    # Fields can also be reused\n",
    "    ':recordset cr:field :field_date .',\n",
    "    ':recordset cr:field :field_number .',\n",
    "    # The loaded data has additional fields\n",
    "    ':recordset cr:field :field_day_of_week .',\n",
    "    ':field_day_of_week rdf:type cr:Field .',\n",
    "    ':field_day_of_week sc:name \\'day_of_week\\' .',\n",
    "    ':field_day_of_week sc:description \\'Day of week description\\' .',\n",
    "    ':field_day_of_week cr:dataType xsd:string .',\n",
    "    # This is not actually a field in the dataframe but below demonstrates how units may be used\n",
    "    ':recordset cr:field :field_temp .',\n",
    "    ':field_temp rdf:type cr:Field .',\n",
    "    ':field_temp sc:name \\'temperature\\' .',\n",
    "    ':field_temp sc:description \\'Description Temperature\\' .',\n",
    "    ':field_temp cr:dataType xsd:double .',\n",
    "]\n",
    "engine.insert(data_triples, prefixes=prefixes)\n",
    "\n",
    "# Also add the units to the fields\n",
    "units_triples = [\n",
    "    ':field_number qudt:unit qudt:CountingUnit .',\n",
    "    ':field_temp qudt:unit siu:degree_Celsius .',\n",
    "\n",
    "    ':field_year qudt:unit qudt: qudt:DateTimeStringEncodingType  .',\n",
    "\n",
    "]\n",
    "engine.insert(units_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5234dde69268460",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:07:47.932780Z",
     "start_time": "2025-12-11T19:07:47.820350Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_stats = data.describe().T\n",
    "print(numeric_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaec50050d9591aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T21:08:14.316242Z",
     "start_time": "2025-12-11T21:08:14.293390Z"
    }
   },
   "outputs": [],
   "source": [
    "#check missing values\n",
    "missing = {\n",
    "    col: data.index[data[col].isna()].tolist()\n",
    "    for col in data.columns\n",
    "    if data[col].isna().any()\n",
    "}\n",
    "for col, count in missing.items():\n",
    "    print(f\"{col}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6781e016-c770-43d2-871a-f4f4ab7378b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T21:15:38.607312Z",
     "start_time": "2025-12-11T21:15:38.585270Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_skewness(df: pd.DataFrame, column: str):\n",
    "    s = df[column].dropna()\n",
    "    sk = s.skew()\n",
    "    print(f\"Skewness of '{column}': {sk:.3f}\")\n",
    "    return sk\n",
    "\n",
    "numeric_cols = ['arr_flights', 'arr_del15', 'carrier_ct', 'weather_ct', 'nas_ct', 'security_ct', 'late_aircraft_ct', 'arr_cancelled', 'arr_diverted', 'arr_delay', 'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay' ]\n",
    "for col in numeric_cols:\n",
    "    check_skewness(data, col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ffd0789618b5e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T21:23:32.564645Z",
     "start_time": "2025-12-11T21:23:32.549723Z"
    }
   },
   "outputs": [],
   "source": [
    "#check correlation\n",
    "numeric_cols = ['arr_flights', 'arr_del15', 'carrier_ct', 'weather_ct', 'nas_ct', 'security_ct', 'late_aircraft_ct', 'arr_cancelled', 'arr_diverted', 'arr_delay', 'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay' ]\n",
    "flight_corr = data[numeric_cols].corr()\n",
    "print(flight_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a728f162b50ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T21:30:33.232651Z",
     "start_time": "2025-12-11T21:30:33.225892Z"
    }
   },
   "outputs": [],
   "source": [
    "#check kurtosis\n",
    "kurtosis = data.kurtosis(numeric_only=True)\n",
    "\n",
    "print(\"\\nKurtosis:\\n\", kurtosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a7c2084ee02b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T21:35:28.476474Z",
     "start_time": "2025-12-11T21:35:25.637177Z"
    }
   },
   "outputs": [],
   "source": [
    "#visualize histogram\n",
    "import seaborn\n",
    "\n",
    "numeric_cols = ['arr_flights', 'arr_del15', 'carrier_ct', 'weather_ct', 'nas_ct', 'security_ct', 'late_aircraft_ct', 'arr_cancelled', 'arr_diverted', 'arr_delay', 'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay' ]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    plt.figure(figsize=(6,3))\n",
    "    seaborn.histplot(data[col], bins=100, kde=False)\n",
    "    plt.title(f\"Histogram of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15da5453bd86fe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T21:36:33.603470Z",
     "start_time": "2025-12-11T21:36:31.909870Z"
    }
   },
   "outputs": [],
   "source": [
    "#visualize outliers\n",
    "numeric_cols = ['arr_flights', 'arr_del15', 'carrier_ct', 'weather_ct', 'nas_ct', 'security_ct', 'late_aircraft_ct', 'arr_cancelled', 'arr_diverted', 'arr_delay', 'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay' ]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    plt.figure(figsize=(6,2))\n",
    "    seaborn.boxplot(x=data[col])\n",
    "    plt.title(f\"Boxplot of {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46bd8a5e28f15f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T00:27:09.516042Z",
     "start_time": "2025-12-12T00:27:09.318384Z"
    }
   },
   "outputs": [],
   "source": [
    "check_outliers_code_writer = student_a\n",
    "\n",
    "\n",
    "def check_outliers(data: pd.DataFrame, threshold=3.0, columns='arr_flights') -> dict:\n",
    "    results = {}\n",
    "\n",
    "    ### DIRTY HACK\n",
    "    ### REPLACE WITH YOUR ACTUAL OUTLIER CHECKING\n",
    "\n",
    "    tmp = data.copy()\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "\n",
    "    for col in columns:\n",
    "        values = tmp[col].astype(float)\n",
    "\n",
    "        mean = values.mean()\n",
    "        std = values.std()\n",
    "\n",
    "        if std == 0 or np.isnan(std):\n",
    "            results[col] = []\n",
    "            continue\n",
    "\n",
    "        z_scores = (values - mean) / std\n",
    "\n",
    "        mask = np.abs(z_scores) > threshold\n",
    "        outliers = values[mask].index\n",
    "\n",
    "        outlier_info = [\n",
    "            {\n",
    "                'index': int(idx),\n",
    "                'z_score': float(z_scores.loc[idx])\n",
    "            }\n",
    "            for idx in outliers\n",
    "        ]\n",
    "\n",
    "        results[col] = outlier_info\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "start_time_co = now()\n",
    "outlier_columns = list(data.columns[2::])\n",
    "outliers_report = check_outliers(data, outlier_columns)\n",
    "print(outliers_report)\n",
    "end_time_co = now()\n",
    "\n",
    "start_time_ho = now()\n",
    "print(outliers_report)\n",
    "end_time_ho = now()\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "# There are three steps involved in this process:\n",
    "# 1. activity creates a figure, report etc. => in this case a report\n",
    "# 2. activity inspects the outcome and derives decisions => in this case to remove the outliers that were found\n",
    "# 3. activity follows up on the decision by changing the data => will be done in the data preparation phase\n",
    "\n",
    "# 1. Activty: Checking for outliers and creating the report\n",
    "co_ass_uuid_executor = \"15085e9d-15f1-4727-9b6e-776dd07fcd08\"\n",
    "check_outliers_executor = [\n",
    "    f':check_outliers prov:qualifiedAssociation :{co_ass_uuid_executor} .',\n",
    "    f':{co_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{co_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(check_outliers_executor, prefixes=prefixes)\n",
    "\n",
    "co_ass_uuid_writer = \"cd4970df-9f40-4bb1-8fad-e4dc4fcdd284\"\n",
    "co_comment = \"\"\"\n",
    "Identifying outliers with a dirty hack that uses the z-score of each row within in column and reports all values\n",
    "with a z-score higher than 2.2 as an outlier, which is not a reasonable threshold but used here to avoid not\n",
    "finding any outliers for demonstration purposes.\n",
    "\"\"\"\n",
    "check_outliers_activity = [\n",
    "    ':check_outliers rdf:type prov:Activity .',\n",
    "    ':check_outliers sc:isPartOf :data_understanding_phase .',\n",
    "    ':check_outliers rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':check_outliers rdfs:comment \"\"\"{co_comment}\"\"\" .',\n",
    "    f':check_outliers prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':check_outliers prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':check_outliers prov:qualifiedAssociation :{co_ass_uuid_writer} .',\n",
    "    f':{co_ass_uuid_writer} prov:agent :{check_outliers_code_writer} .',\n",
    "    f':{co_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':check_outliers prov:used :data .',\n",
    "    ':outlier_report rdf:type prov:Entity .',\n",
    "    f':outlier_report rdfs:comment \"\"\"{json.dumps(outliers_report, indent=2)}\"\"\" .',\n",
    "    ':outlier_report prov:wasGeneratedBy :check_outliers .',\n",
    "    # ...\n",
    "]\n",
    "engine.insert(check_outliers_activity, prefixes=prefixes)\n",
    "\n",
    "# 2. Activity: Inspecting the report and taking a decision on what to do\n",
    "ior_ass_uuid_executor = \"6eaa2c0a-e592-4d85-b37f-d695844910cf\"\n",
    "ior_comment = \"\"\"\n",
    "After inspecting the report the decision has been made to remove all outliers that were identfied for demonstration purpose3\n",
    "\"\"\"\n",
    "inspect_outlier_report_executor = student_a\n",
    "inspect_outlier_report_activity = [\n",
    "    ':inspect_outlier_report rdf:type prov:Activity .',\n",
    "    ':inspect_outlier_report rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':inspect_outlier_report rdfs:comment \"\"\"{co_comment}\"\"\" .',\n",
    "    f':inspect_outlier_report prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_outlier_report prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_outlier_report prov:qualifiedAssociation :{ior_ass_uuid_executor} .',\n",
    "    f':{ior_ass_uuid_executor} prov:agent :{inspect_outlier_report_executor} .',\n",
    "    f':{ior_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ior_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':inspect_outlier_report prov:used :outlier_report .',\n",
    "    ':outlier_decision rdf:type prov:Entity .',\n",
    "    f':outlier_decision rdfs:comment \"\"\"Removing all outliers for demonstration purposes.\"\"\" .',\n",
    "    ':outlier_decision prov:wasGeneratedBy :inspect_outlier_report .',\n",
    "    # ...\n",
    "]\n",
    "engine.insert(inspect_outlier_report_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4cbed36d380523",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T23:55:59.842783Z",
     "start_time": "2025-12-11T23:55:59.787793Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_cols = ['arr_flights', 'arr_del15', 'carrier_ct', 'weather_ct', 'nas_ct', 'security_ct', 'late_aircraft_ct',\n",
    "                'arr_cancelled', 'arr_diverted', 'arr_delay', 'carrier_delay', 'weather_delay', 'nas_delay',\n",
    "                'security_delay', 'late_aircraft_delay']\n",
    "\n",
    "\n",
    "#check for negative values\n",
    "for col in numeric_cols:\n",
    "    if (data[col] < 0).any():\n",
    "        print(f\"Column {col} has negative values!\")\n",
    "\n",
    "#check for invalid values\n",
    "invalid_del15 = data[data['arr_del15'] > data['arr_flights']]\n",
    "print(\"Rows with arr_del15 > arr_flights:\", len(invalid_del15))\n",
    "\n",
    "invalid_cancel = data[data['arr_cancelled'] > data['arr_flights']]\n",
    "print(\"Rows with arr_cancelled > arr_flights:\", len(invalid_cancel))\n",
    "\n",
    "#check for outliers\n",
    "for col in numeric_cols:\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q3 = data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = data[(data[col] < Q1 - 1.5*IQR) | (data[col] > Q3 + 1.5*IQR)]\n",
    "    print(f\"{col}: {len(outliers)} outliers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73453d69cf136a0",
   "metadata": {},
   "source": [
    "**Continue with other tasks of the Data Understanding phase such as checking the distribution, skewness, plausibility of values, etc...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c53095855e3a3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T00:20:04.372113Z",
     "start_time": "2025-12-12T00:20:03.689607Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_distribution(df: pd.DataFrame, column: str):\n",
    "    s = df[column].dropna()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    seaborn.histplot(s, kde=True, ax=axes[0])\n",
    "    axes[0].set_title(f\"Histogram + KDE: {column}\")\n",
    "\n",
    "    seaborn.boxplot(x=s, ax=axes[1])\n",
    "    axes[1].set_title(f\"Boxplot: {column}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "check_distribution(data, \"arr_delay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235387ba2b7b7a48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T00:28:48.789251Z",
     "start_time": "2025-12-12T00:28:48.530852Z"
    }
   },
   "outputs": [],
   "source": [
    "def iqr_outliers(df: pd.DataFrame, columns, multiplier=1.5):\n",
    "    results = {}\n",
    "\n",
    "    for col in columns:\n",
    "        col_values = df[col].astype(float)\n",
    "        Q1 = col_values.quantile(0.25)\n",
    "        Q3 = col_values.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - multiplier * IQR\n",
    "        upper_bound = Q3 + multiplier * IQR\n",
    "\n",
    "        mask = (col_values < lower_bound) | (col_values > upper_bound)\n",
    "        outlier_indices = col_values[mask].index\n",
    "\n",
    "        outlier_info = []\n",
    "        for idx in outlier_indices:\n",
    "            value = float(col_values.loc[idx])\n",
    "            if IQR == 0:\n",
    "                iqr_dist = np.nan  # cannot compute distance\n",
    "            else:\n",
    "                iqr_dist = float((value - Q3) / IQR) if value > Q3 else float((Q1 - value) / IQR)\n",
    "\n",
    "            outlier_info.append({\n",
    "                'index': idx,\n",
    "                'value': value,\n",
    "                'iqr_distance': iqr_dist\n",
    "            })\n",
    "\n",
    "        results[col] = outlier_info\n",
    "\n",
    "    return results\n",
    "\n",
    "numeric_cols = ['arr_flights', 'arr_del15', 'carrier_ct', 'weather_ct', 'nas_ct', 'security_ct', 'late_aircraft_ct',\n",
    "                'arr_cancelled', 'arr_diverted', 'arr_delay', 'carrier_delay', 'weather_delay', 'nas_delay',\n",
    "                'security_delay', 'late_aircraft_delay']\n",
    "outliers = iqr_outliers(data, numeric_cols)\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16349e3",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3d290a6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 504: Gateway Time-out",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## Each Activity that follows is part of the Data Preparation Phase\u001b[39;00m\n\u001b[32m      3\u001b[39m data_preparation_phase_executor = [\n\u001b[32m      4\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:data_preparation_phase rdf:type prov:Activity .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:data_preparation_phase rdfs:label \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mData Preparation Phase\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      6\u001b[39m ]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_preparation_phase_executor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefixes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\BI2025\\Lib\\site-packages\\starvers\\starvers.py:512\u001b[39m, in \u001b[36mTripleStoreEngine.insert\u001b[39m\u001b[34m(self, triples, prefixes, timestamp, chunk_size)\u001b[39m\n\u001b[32m    510\u001b[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001b[33m\"\u001b[39m\u001b[33mNOW()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    511\u001b[39m     \u001b[38;5;28mself\u001b[39m.sparql_post.setQuery(insert_statement)\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparql_post\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mTriples inserted.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\BI2025\\Lib\\site-packages\\SPARQLWrapper\\Wrapper.py:960\u001b[39m, in \u001b[36mSPARQLWrapper.query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mQueryResult\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    943\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[33;03m    Execute the query.\u001b[39;00m\n\u001b[32m    945\u001b[39m \u001b[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m \u001b[33;03m    :rtype: :class:`QueryResult` instance\u001b[39;00m\n\u001b[32m    959\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\BI2025\\Lib\\site-packages\\SPARQLWrapper\\Wrapper.py:940\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EndPointInternalError(e.read())\n\u001b[32m    939\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\BI2025\\Lib\\site-packages\\SPARQLWrapper\\Wrapper.py:926\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    924\u001b[39m         response = urlopener(request, timeout=\u001b[38;5;28mself\u001b[39m.timeout)\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         response = \u001b[43murlopener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m.returnFormat\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m urllib.error.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:216\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    215\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:525\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_response.get(protocol, []):\n\u001b[32m    524\u001b[39m     meth = \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m     response = \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:634\u001b[39m, in \u001b[36mHTTPErrorProcessor.http_response\u001b[39m\u001b[34m(self, request, response)\u001b[39m\n\u001b[32m    631\u001b[39m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m200\u001b[39m <= code < \u001b[32m300\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:563\u001b[39m, in \u001b[36mOpenerDirector.error\u001b[39m\u001b[34m(self, proto, *args)\u001b[39m\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[32m    562\u001b[39m     args = (\u001b[38;5;28mdict\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhttp_error_default\u001b[39m\u001b[33m'\u001b[39m) + orig_args\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:496\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    495\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    498\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\BI2025\\Lib\\urllib\\request.py:643\u001b[39m, in \u001b[36mHTTPDefaultErrorHandler.http_error_default\u001b[39m\u001b[34m(self, req, fp, code, msg, hdrs)\u001b[39m\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001b[31mHTTPError\u001b[39m: HTTP Error 504: Gateway Time-out"
     ]
    }
   ],
   "source": [
    "## Each Activity that follows is part of the Data Preparation Phase\n",
    "\n",
    "data_preparation_phase_executor = [\n",
    "    f':data_preparation_phase rdf:type prov:Activity .',\n",
    "    f':data_preparation_phase rdfs:label \"Data Preparation Phase\" .',\n",
    "]\n",
    "engine.insert(data_preparation_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d076f60",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "handle_outliers_code_writer = student_b\n",
    "\n",
    "\n",
    "def handle_outliers(df: pd.DataFrame, outliers_report: dict) -> pd.DataFrame:\n",
    "    df_clean = df.copy()\n",
    "    n_start = len(df_clean)\n",
    "\n",
    "    def log(rule_name, before, after):\n",
    "        print(f\"[{rule_name}] removed {before - after} rows (remaining: {after})\")\n",
    "\n",
    "    \n",
    "    numeric_cols = [\n",
    "    'arr_flights', 'arr_del15', 'carrier_ct', 'weather_ct', 'nas_ct',\n",
    "    'security_ct', 'late_aircraft_ct', 'arr_cancelled', 'arr_diverted',\n",
    "    'arr_delay', 'carrier_delay', 'weather_delay', 'nas_delay',\n",
    "    'security_delay', 'late_aircraft_delay'\n",
    "    ]\n",
    "\n",
    "    # Drop rows where we have NA values in some columns\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean.dropna(subset=numeric_cols)\n",
    "    after = len(df_clean)\n",
    "    log(\"Drop NAs\", before, after)\n",
    "    \n",
    "    # Drop rows with negative values where not meaningful\n",
    "    before = len(df_clean)\n",
    "    for col in numeric_cols:\n",
    "        df_clean = df_clean[df_clean[col] >= 0]\n",
    "    after = len(df_clean)\n",
    "    log(\"Negative values\", before, after)\n",
    "\n",
    "    # Delayed flights cannot exceed arriving flights\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean[\"arr_del15\"] <= df_clean[\"arr_flights\"]]\n",
    "    after = len(df_clean)\n",
    "    log(\"Delayed > arriving flights\", before, after)\n",
    "\n",
    "    # Cancelled flights cannot exceed arriving flights\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[df_clean[\"arr_cancelled\"] <= df_clean[\"arr_flights\"]]\n",
    "    after = len(df_clean)\n",
    "    log(\"Cancelled > arriving flights\", before, after)\n",
    "\n",
    "    # Delay minutes must be zero if no delayed flights occurred\n",
    "    before = len(df_clean)\n",
    "    delay_min_cols = [\n",
    "        \"arr_delay\", \"carrier_delay\", \"weather_delay\",\n",
    "        \"nas_delay\", \"security_delay\", \"late_aircraft_delay\"\n",
    "    ]\n",
    "    no_delays = df_clean[\"arr_del15\"] == 0\n",
    "    for col in delay_min_cols:\n",
    "        df_clean = df_clean[~(no_delays & (df_clean[col] > 0))]\n",
    "    after = len(df_clean)\n",
    "    log(\"Delay minutes with zero delayed flights\", before, after)\n",
    "\n",
    "    # Division-by-zero cases: arrivals == 0 but delays recorded\n",
    "    before = len(df_clean)\n",
    "    df_clean = df_clean[~((df_clean[\"arr_flights\"] == 0) & (df_clean[\"arr_delay\"] > 0))]\n",
    "    after = len(df_clean)\n",
    "    log(\"Arrivals = 0 with delay minutes\", before, after)\n",
    "\n",
    "    print(f\"[TOTAL] removed {n_start - len(df_clean)} rows (final size: {len(df_clean)})\")\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "start_time_td = now()\n",
    "cleaned_data = handle_outliers(data, data)\n",
    "end_time_td = now()\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "# This is the continuation of the example from the Data Understanding phase above.\n",
    "# There are three steps involved in this process:\n",
    "# 1. activity creates a figure, report etc. => already done in data understanding phase\n",
    "# 2. activity inspects the outcome and derives decisions => already done in data understanding phase\n",
    "# 3. activity follows up on the decision by changing the data => in this case by removing the the outliers that were found\n",
    "\n",
    "ro_ass_uuid_executor = \"ec7e81e1-86ea-475a-a8d4-c7d8ee535488\"\n",
    "handle_outliers_executor = [\n",
    "    f':handle_outliers prov:qualifiedAssociation :{ro_ass_uuid_executor} .',\n",
    "    f':{ro_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ro_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ro_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(handle_outliers_executor, prefixes=prefixes)\n",
    "\n",
    "td_ass_uuid_writer = \"1405f15a-3545-4014-a962-637f3c10a137\"\n",
    "td_comment = \"\"\"\n",
    "Removing all outliers or unusual data points that cannot be used further in our model. We dropp rows with NAs, negative values (where it does not make sense) and remove also rows where we made a sanity check (e.g. delayed flights count cannot exceed total flights arrived).\n",
    "\"\"\"\n",
    "handle_outliers_activity = [\n",
    "    ':handle_outliers rdf:type prov:Activity .',\n",
    "    ':handle_outliers sc:isPartOf :data_preparation_phase .',\n",
    "    #':handle_outliers rdfs:comment \\'Data Preparation\\' .',\n",
    "    f':handle_outliers rdfs:comment \"\"\"{td_comment}\"\"\" .',\n",
    "    f':handle_outliers prov:startedAtTime \"{start_time_td}\"^^xsd:dateTime .',\n",
    "    f':handle_outliers prov:endedAtTime \"{end_time_td}\"^^xsd:dateTime .',\n",
    "    f':handle_outliers prov:qualifiedAssociation :{td_ass_uuid_writer} .',\n",
    "    f':{td_ass_uuid_writer} prov:agent :{handle_outliers_code_writer} .',\n",
    "    f':{td_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{td_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':handle_outliers prov:used :data .',\n",
    "    ':handle_outliers prov:used :outlier_decision .',\n",
    "    ':cleaned_data rdf:type prov:Entity .',\n",
    "    ':cleaned_data prov:wasGeneratedBy :handle_outliers .',\n",
    "    ':cleaned_data prov:wasDerivedFrom :data .',\n",
    "]\n",
    "engine.insert(handle_outliers_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100cff7-8fd5-4ba1-8913-b4f1ccdfda35",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8800ce26b8f3e2e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Continue with other tasks of the Data Preparation phase such as binning, scaling etc...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b563ab1-9aea-4481-bb54-14f0fcddfe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "task3b_comment = \"\"\"\n",
    "Several preprocessing steps were considered but deliberately not applied:\n",
    "\n",
    "- Outlier removal: The numerical variables are extreme right-skewed, so those values correspond to large airports, carriers or extreme delay events. If we would remove those, we would distort the business objective and underrepresent the major datapoints. We want to reserve the real world variability.\n",
    "\n",
    "- Scaling/Log transformations: We decided to not scale some columns yet, since me may do that in the modeling phase if it is required by an algorithm\n",
    "\n",
    "- Removal of attributes due to correlation: We have a strong correlation between arr_delay and carrier_delay, nas_delay, .. and also between  arr_del15 and other delay counts. We believe the different delay values encode different operational causes so we would remove some information which could be necessary for our business objective.\n",
    "\n",
    "- Encoding of categorical attributes: We will do that in the modeling phase since it depends on which algorithm we use.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "task3b_activity = [\n",
    "    ':task3b rdf:type prov:Activity .',\n",
    "    ':task3b sc:isPartOf :data_preparation_phase .',\n",
    "    ':task3b rdfs:label \"Data Preparation – Considered but Not Applied Steps\" .',\n",
    "    f':task3b rdfs:comment \"\"\"{task3b_comment}\"\"\" .'\n",
    "]\n",
    "\n",
    "engine.insert(task3b_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb2f84-58b2-44c5-a9ff-aec875bcc31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "task3c_comment = \"\"\"\n",
    "Several options for derived attributes were analyzed.\n",
    "\n",
    "- Relative delay: arr_delay/arr_flights\n",
    "with that we can easily compare delay across airports and carriers of different sizes, for small flight counts it could be unstable\n",
    "- High potential\n",
    "\n",
    "- Proportion of total delay attributable to each cause: e.g. weather_Delay/ arr_delay or carrier_delay / arr_delay\n",
    "We can capture the structure of delays rather than absolute volume\n",
    "- high potential\n",
    "\n",
    "- Aggregation statistics: e.g. average delay per airport, cancellationrate per carrier\n",
    "Capture historical performance, improve predictive performance (data leakage danger!)\n",
    "- High potential\n",
    "\"\"\"\n",
    "\n",
    "task3c_activity = [\n",
    "    ':task3c rdf:type prov:Activity .',\n",
    "    ':task3c sc:isPartOf :data_preparation_phase .',\n",
    "    ':task3c rdfs:label \"Data Preparation – Derived Attribute Analysis\" .',\n",
    "    f':task3c rdfs:comment \"\"\"{task3c_comment}\"\"\" .'\n",
    "]\n",
    "\n",
    "engine.insert(task3c_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd20601-167e-4df5-9b27-83600d51894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task3d_comment = \"\"\"\n",
    "Several external data sources were identified as potentially beneficial.\n",
    "\n",
    "- Weather data: Observations linked to airport and date\n",
    "- Airport infrastructure data: Characteristics like number of runways, passenger volume, runway utilization\n",
    "- Airline operational characteristics: fleet size and age, network complexity\n",
    "- Public holiday and event calendar: Flag major travel periods or high demand spikes caused by events\n",
    "- Air traffic control: Information on airspace restrictions or other temporary regulations\n",
    "\"\"\"\n",
    "\n",
    "task3d_activity = [\n",
    "    ':task3d rdf:type prov:Activity .',\n",
    "    ':task3d sc:isPartOf :data_preparation_phase .',\n",
    "    ':task3d rdfs:label \"Data Preparation – External Data Source Analysis\" .',\n",
    "    f':task3d rdfs:comment \"\"\"{task3d_comment}\"\"\" .'\n",
    "]\n",
    "\n",
    "engine.insert(task3d_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0036428-fcdf-4ee8-ad52-424f95024cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your final transformed dataset should also be documented appropriately using Croissant, SI, etc.\n",
    "\n",
    "prepared_data_triples = [\n",
    "    ':prepared_data rdf:type prov:Entity .',\n",
    "    ':prepared_data rdf:type sc:Dataset .',\n",
    "    ':prepared_data rdfs:label \"Prepared Airline Delay Dataset\" .',\n",
    "    ':prepared_data prov:wasDerivedFrom :cleaned_data .',\n",
    "    ':prepared_data prov:wasDerivedFrom :data .',\n",
    "    ':prepared_data rdfs:comment \"\"\"Final dataset after data preparation. Contains only valid, non-negative, and semantically consistent airline delay records. Used as input for the modeling phase.\"\"\" .'\n",
    "]\n",
    "engine.insert(prepared_data_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c19ebb",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb93dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Modeling Phase\n",
    "\n",
    "modeling_phase_executor = [\n",
    "    f':modeling_phase rdf:type prov:Activity .',\n",
    "    f':modeling rdfs:label \"Modeling Phase\" .',\n",
    "]\n",
    "engine.insert(modeling_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a80b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_code_writer = student_a\n",
    "\n",
    "#############################################\n",
    "# Documentation 4a\n",
    "#############################################\n",
    "\n",
    "dma_ass_uuid_writer = \"b3e840ab-ac23-415e-bd9c-6d00bb79c37a\"\n",
    "dma_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "identify_data_mining_algorithm_activity = [\n",
    "    f':define_algorithm rdf:type prov:Activity .',\n",
    "    f':define_algorithm sc:isPartOf :modeling_phase .',\n",
    "    f':define_algorithm rdfs:comment \"\"\"{dma_comment}\"\"\" .',\n",
    "    f':define_algorithm prov:qualifiedAssociation :{dma_ass_uuid_writer} .',\n",
    "    f':{dma_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{dma_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{dma_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # example algorithm definition\n",
    "    f':random_forest_algorithm rdf:type mls:Algorithm .',\n",
    "    f':random_forest_algorithm rdfs:label \"Random Forest Algorithm\" .',\n",
    "\n",
    "    # example implementation\n",
    "    f':random_forrest_classifier_implementation rdf:type mls:Implementation .',\n",
    "    f':random_forrest_classifier_implementation rdfs:label \"Scikit-learn RandomForestClassifier\" .',\n",
    "    f':random_forrest_classifier_implementation mls:implements :random_forest_algorithm .',\n",
    "    f':random_forrest_classifier_implementation prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "    # you can also define your Evaluation Measures here\n",
    "\n",
    "    # example evaluation \n",
    "    f':r2_score_measure rdf:type mls:EvaluationMeasure .',\n",
    "    f':r2_score_measure rdfs:label \"R-squared Score\" .',\n",
    "    f':r2_score_measure rdfs:comment \"xxx\" .',\n",
    "    f':r2_score_measure prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "]\n",
    "engine.insert(identify_data_mining_algorithm_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef613f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation 4b\n",
    "#############################################\n",
    "\n",
    "hp_ass_uuid_writer = \"fff582a8-c5cd-4030-978b-9f56b603167c\"\n",
    "hp_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "identify_hp_activity = [\n",
    "    f':identify_hyperparameters rdf:type prov:Activity .',\n",
    "    f':identify_hyperparameters sc:isPartOf :modeling_phase .',\n",
    "    f':identify_hyperparameters rdfs:comment \"\"\"{hp_comment}\"\"\" .',\n",
    "    f':identify_hyperparameters prov:qualifiedAssociation :{hp_ass_uuid_writer} .',\n",
    "    f':{hp_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{hp_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{hp_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # example parameter\n",
    "    f':hp_learning_rate rdf:type mls:HyperParameter .',\n",
    "    f':hp_learning_rate rdfs:label \"Learning Rate\" .',\n",
    "    f':hp_learning_rate rdfs:comment \"...\" .',\n",
    "    f':random_forrest_classifier_implementation mls:hasHyperParameter :hp_learning_rate .',\n",
    "    f':hp_learning_rate prov:wasGeneratedBy :identify_hyperparameters .',\n",
    "\n",
    "    # continue with your identified hyperparameters\n",
    "\n",
    "]\n",
    "engine.insert(identify_hp_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995966b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df: pd.DataFrame):\n",
    "    #do something\n",
    "    return 'train_set', 'validation_set', 'test_set'\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Documentation 4c\n",
    "#############################################\n",
    "\n",
    "### Define Train/Validation/Test splits\n",
    "split_ass_uuid_writer = \"fb58ae6c-9d58-44c9-ac7e-529111bdf7fc\"\n",
    "split_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "## Use your prepared dataset\n",
    "input_dataset = \":prepared_data\"\n",
    "\n",
    "define_split_activity = [\n",
    "    f':define_data_split rdf:type prov:Activity .',\n",
    "    f':define_data_split sc:isPartOf :modeling_phase .',\n",
    "    f':define_data_split rdfs:comment \"Train/Validation/Test Split Definition\" .',\n",
    "    f':define_data_split rdfs:comment \"\"\"{split_comment}\"\"\" .',\n",
    "    f':define_data_split prov:qualifiedAssociation :{split_ass_uuid_writer} .',\n",
    "    f':{split_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{split_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{split_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':define_data_split prov:used {input_dataset} .',\n",
    "\n",
    "    # Training Set\n",
    "    f':training_set rdf:type sc:Dataset .',\n",
    "    f':training_set rdfs:label \"Training Set\" .',\n",
    "    f':training_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':training_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':training_set rdfs:comment \"Contains xx samples\" .',\n",
    "\n",
    "    # Validation Set\n",
    "    f':validation_set rdf:type sc:Dataset .',\n",
    "    f':validation_set rdfs:label \"Validation Set\" .',\n",
    "    f':validation_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':validation_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':validation_set rdfs:comment \"Contains xx samples\" .',\n",
    "\n",
    "    # Test Set\n",
    "    f':test_set rdf:type sc:Dataset .',\n",
    "    f':test_set rdfs:label \"Test Set\" .',\n",
    "    f':test_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':test_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':test_set rdfs:comment \"Contains xx samples\" .',\n",
    "\n",
    "]\n",
    "engine.insert(define_split_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b5ed6-54d6-4c81-9adb-e295fbd5c364",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-978b274ef875c238",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_and_finetune_model(training_set, validation_set):\n",
    "    # do something here\n",
    "\n",
    "    # Try to automate as much documentation work as possible.\n",
    "    # Define your training runs with their respective hyperparameter settings, etc.\n",
    "    # Document each time a training run, model, its hp_settings, evaluations, ...  \n",
    "    # Create performance figures/graphs\n",
    "\n",
    "    return 'Find most suitable model'\n",
    "\n",
    "\n",
    "start_time_tafm = now()\n",
    "# train_and_finetune_model()\n",
    "end_time_tafm = now()\n",
    "\n",
    "#############################################\n",
    "# Documentation 4d & e & f\n",
    "#############################################\n",
    "\n",
    "tafm_ass_uuid_writer = \"21d60fe3-c9ab-4a0a-bae7-b9fe9653c755\"\n",
    "tafm_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "# EXAMPLE output from your training\n",
    "training_run1 = \"run_1\"\n",
    "model_run1 = \"model_run1\"\n",
    "hp1_setting_run1 = \"hp_setting_run1\"\n",
    "eval_train_run1 = \"metric_train_run1\"\n",
    "eval_validation_run1 = \"metric_validation_run1\"\n",
    "\n",
    "train_model_activity = [\n",
    "    # Activity \n",
    "    f':train_and_finetune_model rdf:type prov:Activity .',\n",
    "    f':train_and_finetune_model sc:isPartOf :modeling_phase .',\n",
    "    f':train_and_finetune_model rdfs:comment \"\"\"{tafm_comment}\"\"\" .',\n",
    "    f':train_and_finetune_model prov:startedAtTime \"{start_time_tafm}\"^^xsd:dateTime .',\n",
    "    f':train_and_finetune_model prov:endedAtTime \"{end_time_tafm}\"^^xsd:dateTime .',\n",
    "    f':train_and_finetune_model prov:qualifiedAssociation :{tafm_ass_uuid_writer} .',\n",
    "    f':{tafm_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{tafm_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{tafm_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    ########################################\n",
    "    # ONE model run - automate everything below!\n",
    "\n",
    "    # Parameter settings\n",
    "    f':{hp1_setting_run1} rdf:type mls:HyperParameterSetting .',\n",
    "    f':{hp1_setting_run1} mls:specifiedBy :hp_learning_rate .',\n",
    "    f':{hp1_setting_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{hp1_setting_run1} prov:wasGeneratedBy :train_and_finetune_model .',\n",
    "    # add your further parameters\n",
    "\n",
    "    # Describe your Run\n",
    "    f':{training_run1} rdf:type mls:Run .',\n",
    "    f':{training_run1} sc:isPartOf :train_and_finetune_model .',\n",
    "    f':{training_run1} mls:realizes :random_forest_algorithm .',\n",
    "    f':{training_run1} rdf:label \"Training Run 1 with...\" .',\n",
    "    f':{training_run1} mls:executes :your_implementation .',\n",
    "    f':{training_run1} mls:hasInput :training_set .',\n",
    "    f':{training_run1} mls:hasInput :validation_set .',\n",
    "    f':{training_run1} mls:hasInput :{hp1_setting_run1} .',\n",
    "    # list all your used parameters here\n",
    "    f':{training_run1} mls:hasOutput :{model_run1} .',\n",
    "    f':{training_run1} mls:hasOutput :{eval_train_run1} .',\n",
    "    f':{training_run1} mls:hasOutput :{eval_validation_run1} .',\n",
    "\n",
    "    # Describe your Model\n",
    "    f':{model_run1} rdf:type mls:Model .',\n",
    "    f':{model_run1} prov:label \"xxx\" .',\n",
    "    f':{model_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{model_run1} mlso:trainedOn :training_set .',\n",
    "    f':{model_run1} mlso:hasAlgorithmType :random_forest_algorithm .',\n",
    "\n",
    "    # Describe your evaluations\n",
    "    # You can have multiple evaluations per model \n",
    "    f':{eval_train_run1} rdf:type mls:ModelEvaluation .',\n",
    "    f':{eval_train_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{eval_train_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{eval_train_run1} mls:specifiedBy :r2_score_measure .',\n",
    "    f':{eval_train_run1} prov:used :training_set .',\n",
    "\n",
    "    f':{eval_validation_run1} rdf:type mls:ModelEvaluation .',\n",
    "    f':{eval_validation_run1} prov:wasGeneratedBy :{training_run1} .',\n",
    "    f':{eval_validation_run1} mls:hasValue \"1.23\"^^xsd:double .',\n",
    "    f':{eval_validation_run1} mls:specifiedBy :r2_score_measure .',\n",
    "    f':{eval_validation_run1} prov:used :validation_set .',\n",
    "\n",
    "    # Dont forget to document any visualizations\n",
    "\n",
    "]\n",
    "engine.insert(train_model_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b6b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_model_full_data(training_set, validation_set):\n",
    "    # create your\n",
    "    return \"Final Trained Model\"\n",
    "\n",
    "\n",
    "start_time_tafm = now()\n",
    "# train_and_finetune_model()\n",
    "end_time_tafm = now()\n",
    "\n",
    "#############################################\n",
    "# Documentation 4g\n",
    "#############################################\n",
    "\n",
    "retrain_ass_uuid_writer = \"96815ee0-524c-437b-b5fa-2e15b945c993\"  # Generate once\n",
    "\n",
    "final_training_activity = \":retrain_final_model\"\n",
    "final_model = \":final_model_entity\"\n",
    "\n",
    "# Document the retraining activity.\n",
    "# Hint: This activity is still part of the :modeling_phase\n",
    "\n",
    "retrain_documentation = [\n",
    "    # your documentation here    \n",
    "]\n",
    "engine.insert(retrain_documentation, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02059271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06583f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a88bf71f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46137067",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Evaluation Phase\n",
    "\n",
    "evaluation_phase_executor = [\n",
    "    f':evaluation_phase rdf:type prov:Activity .',\n",
    "    f':evaluation_phase rdfs:label \"Evaluation Phase\" .',\n",
    "]\n",
    "engine.insert(evaluation_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d80e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_code_writer = student_b\n",
    "\n",
    "\n",
    "def evaluate_on_test_data(final_model, test_set):\n",
    "    # Predict and evaluation on test data\n",
    "\n",
    "    return 'Performance'\n",
    "\n",
    "\n",
    "start_time_eval = now()\n",
    "#evaluate_on_test_data()\n",
    "end_time_eval = now()\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "eval_ass_uuid = \"7f1431e9-feed-429a-92ed-c131b23cbe79\"  # Generate once\n",
    "final_model = \":final_model_entity\"\n",
    "test_set = \":test_set\"\n",
    "\n",
    "eval_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "evaluate_activity = [\n",
    "    f':evaluate_final_model rdf:type prov:Activity .',\n",
    "    f':evaluate_final_model sc:isPartOf :evaluation_phase .',\n",
    "    f':evaluate_final_model rdfs:label \"Final Model Evaluation on Test Set\" .',\n",
    "    f':evaluate_final_model rdfs:comment \"\"\"{eval_comment}\"\"\" .',\n",
    "    f':evaluate_final_model prov:startedAtTime \"{start_time_eval}\"^^xsd:dateTime .',\n",
    "    f':evaluate_final_model prov:endedAtTime \"{end_time_eval}\"^^xsd:dateTime .',\n",
    "    f':evaluate_final_model prov:qualifiedAssociation :{eval_ass_uuid} .',\n",
    "\n",
    "    f':{eval_ass_uuid} prov:agent :{eval_code_writer} .',\n",
    "    f':{eval_ass_uuid} rdf:type prov:Association .',\n",
    "    f':{eval_ass_uuid} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Inputs\n",
    "    f':evaluate_final_model prov:used {final_model} .',\n",
    "    f':evaluate_final_model prov:used {test_set} .',\n",
    "\n",
    "    # Reference to Data Mining Success Criteria from Phase 1\n",
    "    f':evaluate_final_model prov:used :bu_data_mining_success_criteria .',\n",
    "\n",
    "    # Document you final model performance\n",
    "\n",
    "    # Hint: you evaluate bias in this way:\n",
    "    f':bias_evaluation_result rdf:type mls:ModelEvaluation .',\n",
    "    f':bias_evaluation_result prov:wasGeneratedBy :evaluate_final_model .',\n",
    "    f':bias_evaluation_result rdfs:label \"Bias Analysis\" .',\n",
    "    f':bias_evaluation_result rdfs:comment \"...\" .',\n",
    "\n",
    "]\n",
    "engine.insert(evaluate_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785c94b",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ad2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Deployment Phase\n",
    "\n",
    "deployment_phase_executor = [\n",
    "    f':deployment_phase rdf:type prov:Activity .',\n",
    "    f':deployment_phase rdfs:label \"Deployment Phase\" .',\n",
    "]\n",
    "engine.insert(deployment_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176313c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "comparison_and_recommendations_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "ethical_aspects_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "monitoring_plan_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "reproducibility_reflection_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "dep_ass_uuid_executor = \"72a921e0-1234-4567-89ab-cdef01234567\"  # Generate once\n",
    "deployment_executor = [\n",
    "    f':plan_deployment rdf:type prov:Activity .',\n",
    "    f':plan_deployment sc:isPartOf :deployment_phase .',  # Connect to Parent Phase\n",
    "    f':plan_deployment rdfs:label \"Plan Deployment\"@en .',\n",
    "\n",
    "    f':plan_deployment prov:qualifiedAssociation :{dep_ass_uuid_executor} .',\n",
    "    f':{dep_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{dep_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{dep_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(deployment_executor, prefixes=prefixes)\n",
    "\n",
    "deployment_data_executor = [\n",
    "    #6a\n",
    "    f':dep_recommendations rdf:type prov:Entity .',\n",
    "    f':dep_recommendations prov:wasGeneratedBy :plan_deployment .',\n",
    "    f':dep_recommendations rdfs:label \"6a Business Objectives Reflection and Deployment Recommendations\" .',\n",
    "    f':dep_recommendations rdfs:comment \"\"\"{comparison_and_recommendations_comment}\"\"\" .',\n",
    "    #6b\n",
    "    f':dep_ethical_risks rdf:type prov:Entity .',\n",
    "    f':dep_ethical_risks prov:wasGeneratedBy :plan_deployment .',\n",
    "    f':dep_ethical_risks rdfs:label \"6b Ethical Aspects and Risks\" .',\n",
    "    f':dep_ethical_risks rdfs:comment \"\"\"{ethical_aspects_comment}\"\"\" .',\n",
    "    #6c\n",
    "    f':dep_monitoring_plan rdf:type prov:Entity .',\n",
    "    f':dep_monitoring_plan prov:wasGeneratedBy :plan_deployment .',\n",
    "    f':dep_monitoring_plan rdfs:label \"6c Monitoring Plan\" .',\n",
    "    f':dep_monitoring_plan rdfs:comment \"\"\"{monitoring_plan_comment}\"\"\" .',\n",
    "    #6d\n",
    "    f':dep_reproducibility_reflection rdf:type prov:Entity .',\n",
    "    f':dep_reproducibility_reflection prov:wasGeneratedBy :plan_deployment .',\n",
    "    f':dep_reproducibility_reflection rdfs:label \"6d Reproducibility Reflection\" .',\n",
    "    f':dep_reproducibility_reflection rdfs:comment \"\"\"{reproducibility_reflection_comment}\"\"\" .',\n",
    "\n",
    "]\n",
    "engine.insert(deployment_data_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e528dac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70d410af",
   "metadata": {},
   "source": [
    "# Generate Latex Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f44e16",
   "metadata": {},
   "source": [
    "The following cells give you an example of how to automatically create a Latex Report from your provenance documentation.\n",
    "\n",
    "Feel free to use the example provided. If you use it, you should adapt and extend it with relevant sections/tables/plots/... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37046b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:10:59.412039Z",
     "start_time": "2025-12-11T19:10:59.404060Z"
    }
   },
   "outputs": [],
   "source": [
    "base_iri = f\"https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d887eabd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:11:00.488518Z",
     "start_time": "2025-12-11T19:11:00.477302Z"
    }
   },
   "outputs": [],
   "source": [
    "# This cell includes cleaning functions\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def latex_escape(text: str | None) -> str:\n",
    "    if text is None: return \"\"\n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\\\\", r\"\\textbackslash{}\")\n",
    "    pairs = [\n",
    "        (\"&\", r\"\\&\"), (\"%\", r\"\\%\"), (\"$\", r\"\\$\"), (\"#\", r\"\\#\"),\n",
    "        (\"_\", r\"\\_\"), (\"{\", r\"\\{\"), (\"}\", r\"\\}\"),\n",
    "        (\"~\", r\"\\textasciitilde{}\"), (\"^\", r\"\\textasciicircum{}\")\n",
    "    ]\n",
    "    for k, v in pairs:\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_rdf(x) -> str:\n",
    "    if hasattr(x, \"toPython\"): return str(x.toPython())\n",
    "    if x is None: return \"\"\n",
    "    s = str(x).strip()\n",
    "    s = s.strip('\"').strip(\"'\")\n",
    "    s = s.strip()\n",
    "    if \"^^\" in s:\n",
    "        s = s.split(\"^^\")[0].strip('\"')\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def fmt_iso(ts: str) -> str:\n",
    "    if not ts: return \"\"\n",
    "    try:\n",
    "        clean_ts = ts.split(\"^^\")[0].strip('\"')\n",
    "        clean_ts = clean_ts.replace(\"Z\", \"+00:00\") if clean_ts.endswith(\"Z\") else clean_ts\n",
    "        return datetime.fromisoformat(clean_ts).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    except:\n",
    "        return latex_escape(str(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948da2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:11:05.430449Z",
     "start_time": "2025-12-11T19:11:03.238191Z"
    }
   },
   "outputs": [],
   "source": [
    "# This cell includes exemplary queries for different phases\n",
    "\n",
    "\n",
    "### Author Block\n",
    "author_query = f\"\"\"\n",
    "{prefix_header}\n",
    "PREFIX iao: <http://purl.obolibrary.org/obo/>\n",
    "\n",
    "SELECT DISTINCT ?uri ?given ?family ?matr WHERE {{\n",
    "  VALUES ?uri {{ :{student_a} :{student_b} }}\n",
    "  \n",
    "  ?uri a foaf:Person .\n",
    "  ?uri foaf:givenName ?given .\n",
    "  ?uri foaf:familyName ?family .\n",
    "  ?uri iao:IAO_0000219 ?matr .\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "res_authors = engine.query(author_query)\n",
    "author_block_latex = \"\"\n",
    "\n",
    "if not res_authors.empty:  # type:ignore\n",
    "    for _, row in res_authors.iterrows():  # type:ignore\n",
    "\n",
    "        uri_str = str(row['uri'])\n",
    "        given = latex_escape(clean_rdf(row['given']))\n",
    "        family = latex_escape(clean_rdf(row['family']))\n",
    "        matr = latex_escape(clean_rdf(row['matr']))\n",
    "        if student_a in uri_str:\n",
    "            responsibility = \"Student A\"\n",
    "        elif student_b in uri_str:\n",
    "            responsibility = \"Student B\"\n",
    "        else:\n",
    "            responsibility = \"Student\"\n",
    "\n",
    "        author_block_latex += rf\"\"\"\n",
    "          \\author{{{given} {family}}}\n",
    "          \\authornote{{{responsibility}, Matr.Nr.: {matr}}}\n",
    "          \\affiliation{{\n",
    "            \\institution{{TU Wien}}\n",
    "            \\country{{Austria}}\n",
    "          }}\n",
    "          \"\"\"\n",
    "\n",
    "### Business Understanding example\n",
    "bu_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?ds_comment ?bo_comment WHERE {{\n",
    "  OPTIONAL {{ :bu_data_source_and_scenario rdfs:comment ?ds_comment . }}\n",
    "  OPTIONAL {{ :bu_business_objectives rdfs:comment ?bo_comment . }}\n",
    "}} LIMIT 1\n",
    "\"\"\"\n",
    "res_bu = engine.query(bu_query)\n",
    "row_bu = res_bu.iloc[0] if not res_bu.empty else {}  # type:ignore\n",
    "bu_data_source = latex_escape(clean_rdf(row_bu.get(\"ds_comment\", \"\")))\n",
    "bu_objectives = latex_escape(clean_rdf(row_bu.get(\"bo_comment\", \"\")))\n",
    "\n",
    "### Data Understanding examples\n",
    "# Example Dataset Description\n",
    "du_desc_query = f\"\"\"\n",
    "{prefix_header}\n",
    "SELECT ?desc WHERE {{ :raw_data sc:description ?desc . }} LIMIT 1\n",
    "\"\"\"\n",
    "res_du_desc = engine.query(du_desc_query)\n",
    "row_du_desc = res_du_desc.iloc[0] if not res_du_desc.empty else {}  # type:ignore\n",
    "du_description = latex_escape(clean_rdf(row_du_desc.get(\"desc\", \"\")))\n",
    "\n",
    "# Example Feature Columns Table\n",
    "du_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?name (SAMPLE(?dtypeRaw) as ?dtype) (SAMPLE(?descRaw) as ?desc) WHERE {{\n",
    "  :raw_data cr:recordSet ?rs .\n",
    "  ?rs cr:field ?field .\n",
    "  ?field sc:name ?name .\n",
    "  ?field sc:description ?descRaw .\n",
    "  ?field cr:dataType ?dtypeRaw .\n",
    "}} \n",
    "GROUP BY ?name\n",
    "ORDER BY ?name\n",
    "\"\"\"\n",
    "res_du = engine.query(du_query)\n",
    "du_rows = []\n",
    "if not res_du.empty:  # type:ignore\n",
    "    for _, f in res_du.iterrows():  # type:ignore\n",
    "        dtype_raw = clean_rdf(f.get(\"dtype\", \"\"))\n",
    "        if '#' in dtype_raw:\n",
    "            dtype = dtype_raw.split('#')[-1]\n",
    "        elif '/' in dtype_raw:\n",
    "            dtype = dtype_raw.split('/')[-1]\n",
    "        else:\n",
    "            dtype = dtype_raw\n",
    "\n",
    "        desc = clean_rdf(f.get(\"desc\", \"\"))\n",
    "        row_str = f\"{latex_escape(clean_rdf(f['name']))} & {latex_escape(dtype)} & {latex_escape(desc)} \\\\\\\\\"\n",
    "        du_rows.append(row_str)\n",
    "du_table_rows = \"\\n    \".join(du_rows)\n",
    "\n",
    "### Data Preparation\n",
    "dp_3outliers_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :handle_outliers rdfs:comment ?comment .\n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "res_dp3outliers = engine.query(dp_3outliers_query)\n",
    "row_dp3outliers = res_dp3outliers.iloc[0] if not res_dp3outliers.empty else {}\n",
    "dp3outliers_comment = latex_escape(clean_rdf(row_dp3outliers.get(\"comment\", \"\")))\n",
    "\n",
    "\n",
    "### Task 3b\n",
    "dp_3b_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :task3b rdfs:comment ?comment .\n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "res_dp3b = engine.query(dp_3b_query)\n",
    "row_dp3b = res_dp3b.iloc[0] if not res_dp3b.empty else {}\n",
    "dp3b_comment = latex_escape(clean_rdf(row_dp3b.get(\"comment\", \"\")))\n",
    "\n",
    "dp_3c_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :task3c rdfs:comment ?comment .\n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "res_dp3c = engine.query(dp_3c_query)\n",
    "row_dp3c = res_dp3c.iloc[0] if not res_dp3c.empty else {}\n",
    "dp3c_comment = latex_escape(clean_rdf(row_dp3c.get(\"comment\", \"\")))\n",
    "\n",
    "dp_3d_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :task3d rdfs:comment ?comment .\n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "res_dp3d = engine.query(dp_3d_query)\n",
    "row_dp3d = res_dp3d.iloc[0] if not res_dp3d.empty else {}\n",
    "dp3d_comment = latex_escape(clean_rdf(row_dp3d.get(\"comment\", \"\")))\n",
    "\n",
    "\n",
    "dp_3summary_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?comment WHERE {{\n",
    "  :prepared_data rdfs:comment ?comment .\n",
    "}}\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    "res_dp3summary = engine.query(dp_3summary_query)\n",
    "row_dp3summary = res_dp3summary.iloc[0] if not res_dp3summary.empty else {}\n",
    "dp3summary_comment = latex_escape(clean_rdf(row_dp3summary.get(\"comment\", \"\")))\n",
    "\n",
    "### Modeling example\n",
    "# Hyperparameters\n",
    "hp_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?hpName (SAMPLE(?hpValRaw) as ?hpVal) (MAX(?hpDescRaw) as ?hpDesc) WHERE {{\n",
    "  ?run sc:isPartOf :train_and_finetune_model .\n",
    "  ?run mls:hasInput ?setting .\n",
    "  ?setting a mls:HyperParameterSetting .\n",
    "  ?setting mls:hasValue ?hpValRaw .\n",
    "  ?setting mls:specifiedBy ?hpDef .\n",
    "  ?hpDef rdfs:label ?hpName .\n",
    "  OPTIONAL {{ ?hpDef rdfs:comment ?hpDescRaw . }}\n",
    "}} \n",
    "GROUP BY ?hpName\n",
    "ORDER BY ?hpName\n",
    "\"\"\"\n",
    "res_hp = engine.query(hp_query)\n",
    "hp_rows = []\n",
    "if not res_hp.empty:  #type:ignore\n",
    "    for _, row in res_hp.iterrows():  #type:ignore\n",
    "        name = latex_escape(clean_rdf(row['hpName']))\n",
    "        val = latex_escape(clean_rdf(row['hpVal']))\n",
    "        desc = latex_escape(clean_rdf(row.get('hpDesc', '')))\n",
    "        hp_rows.append(rf\"{name} & {desc} & {val} \\\\\")\n",
    "\n",
    "hp_table_rows = \"\\n    \".join(hp_rows)\n",
    "\n",
    "# Run Info\n",
    "run_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?algoLabel ?start ?end ?metricLabel ?metricVal WHERE {{\n",
    "  OPTIONAL {{ :train_and_finetune_model prov:startedAtTime ?start ; prov:endedAtTime ?end . }}\n",
    "  OPTIONAL {{\n",
    "      ?run sc:isPartOf :train_and_finetune_model .\n",
    "      ?run mls:realizes ?algo .\n",
    "      ?algo rdfs:label ?algoLabel .\n",
    "  }}\n",
    "  OPTIONAL {{\n",
    "    ?run sc:isPartOf :train_and_finetune_model .\n",
    "    ?run mls:hasOutput ?eval .\n",
    "    ?eval a mls:ModelEvaluation ; mls:hasValue ?metricVal .\n",
    "    OPTIONAL {{ ?eval mls:specifiedBy ?m . ?m rdfs:label ?metricLabel . }}\n",
    "  }}\n",
    "}} LIMIT 1\n",
    "\"\"\"\n",
    "res_run = engine.query(run_query)\n",
    "row_run = res_run.iloc[0] if not res_run.empty else {}  #type:ignore\n",
    "mod_algo = latex_escape(clean_rdf(row_run.get(\"algoLabel\", \"\")))\n",
    "mod_start = latex_escape(fmt_iso(clean_rdf(row_run.get(\"start\"))))\n",
    "mod_end = latex_escape(fmt_iso(clean_rdf(row_run.get(\"end\"))))\n",
    "mod_m_lbl = latex_escape(clean_rdf(row_run.get(\"metricLabel\", \"\")))\n",
    "raw_val = clean_rdf(row_run.get('metricVal', ''))\n",
    "mod_m_val = f\"{float(raw_val):.4f}\" if raw_val else \"\"\n",
    "\n",
    "print(\"Data extraction done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca8fa1c",
   "metadata": {},
   "source": [
    "The following includes the Latex report itself. It fills in the query-results from the cell before. The ACM Template is already filled. \n",
    "Make sure that you update Student A and B accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9ce52f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:11:44.321412Z",
     "start_time": "2025-12-11T19:11:44.314682Z"
    }
   },
   "outputs": [],
   "source": [
    "latex_content = rf\"\"\"\\documentclass[sigconf]{{acmart}}\n",
    "\n",
    "\\AtBeginDocument{{ \\providecommand\\BibTeX{{ Bib\\TeX }} }}\n",
    "\\setcopyright{{acmlicensed}}\n",
    "\\copyrightyear{{2025}}\n",
    "\\acmYear{{2025}}\n",
    "\\acmDOI{{XXXXXXX.XXXXXXX}}\n",
    "\n",
    "\\acmConference[BI 2025]{{Business Intelligence}}{{-}}{{-}}\n",
    "\n",
    "\\begin{{document}}\n",
    "\n",
    "\\title{{BI2025 Experiment Report - Group {group_id}}}\n",
    "%% ---Authors: Dynamically added ---\n",
    "{author_block_latex}\n",
    "\n",
    "\\begin{{abstract}}\n",
    "  This report documents the machine learning experiment for Group {group_id}, following the CRISP-DM process model.\n",
    "\\end{{abstract}}\n",
    "\n",
    "\\ccsdesc[500]{{Computing methodologies~Machine learning}}\n",
    "\\keywords{{CRISP-DM, Provenance, Knowledge Graph, Machine Learning}}\n",
    "\n",
    "\\maketitle\n",
    "\n",
    "%% --- 1. Business Understanding ---\n",
    "\\section{{Business Understanding}}\n",
    "\n",
    "\\subsection{{Data Source and Scenario}}\n",
    "{bu_data_source}\n",
    "\n",
    "\\subsection{{Business Objectives}}\n",
    "{bu_objectives}\n",
    "\n",
    "%% --- 2. Data Understanding ---\n",
    "\\section{{Data Understanding}}\n",
    "\\textbf{{Dataset Description:}} {du_description}\n",
    "\n",
    "The following features were identified in the dataset:\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Raw Data Features}}\n",
    "  \\label{{tab:features}}\n",
    "  \\begin{{tabular}}{{lp{{0.2\\linewidth}}p{{0.4\\linewidth}}}}\n",
    "    \\toprule\n",
    "    \\textbf{{Feature Name}} & \\textbf{{Data Type}} & \\textbf{{Description}} \\\\\n",
    "    \\midrule\n",
    "    {du_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "%% --- 3. Data Preparation ---\n",
    "\\section{{Data Preparation}}\n",
    "\\subsection{{Handling outliers}}\n",
    "{dp3outliers_comment}\n",
    "\n",
    "\\subsection{{Considered but Not Applied Pre-processing Steps}}\n",
    "{dp3b_comment}\n",
    "\\subsection{{Analysis of Potential Derived Attributes}}\n",
    "{dp3c_comment}\n",
    "\\subsection{{Analysis of Potential External Data Sources}}\n",
    "{dp3d_comment}\n",
    "\\subsection{{Outcome of the Data Preparation Phase}}\n",
    "{dp3summary_comment}\n",
    "\n",
    "\n",
    "%% --- 4. Modeling ---\n",
    "\\section{{Modeling}}\n",
    "\n",
    "\\subsection{{Hyperparameter Configuration}}\n",
    "The model was trained using the following hyperparameter settings:\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Hyperparameter Settings}}\n",
    "  \\label{{tab:hyperparams}}\n",
    "  \\begin{{tabular}}{{lp{{0.4\\linewidth}}l}}\n",
    "    \\toprule\n",
    "    \\textbf{{Parameter}} & \\textbf{{Description}} & \\textbf{{Value}} \\\\\n",
    "    \\midrule\n",
    "    {hp_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "\\subsection{{Training Run}}\n",
    "A training run was executed with the following characteristics:\n",
    "\\begin{{itemize}}\n",
    "    \\item \\textbf{{Algorithm:}} {mod_algo}\n",
    "    \\item \\textbf{{Start Time:}} {mod_start}\n",
    "    \\item \\textbf{{End Time:}} {mod_end}\n",
    "    \\item \\textbf{{Result:}} {mod_m_lbl} = {mod_m_val}\n",
    "\\end{{itemize}}\n",
    "\n",
    "%% --- 5. Evaluation ---\n",
    "\\section{{Evaluation}}\n",
    "\n",
    "%% --- 6. Deployment ---\n",
    "\\section{{Deployment}}\n",
    "\n",
    "\\section{{Conclusion}}\n",
    "\n",
    "\\end{{document}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c947b2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T19:11:46.178298Z",
     "start_time": "2025-12-11T19:11:46.167096Z"
    }
   },
   "outputs": [],
   "source": [
    "# This cell stores the Latex report to the data/report directory\n",
    "\n",
    "out_dir = os.path.join(\"data\", \"report\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, \"experiment_report.tex\")\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_content)\n",
    "\n",
    "print(f\"Report written to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3345839b-fab1-4036-907f-34869f97453b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
